{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "#n_instance = 1000\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z = 6 #3的倍數\n",
    "nodes = 4 #8\n",
    "var = 2\n",
    "use_bias = 'True'\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "export_excel(X_train, 'Dataset/X_train')\n",
    "export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            8           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4)            16          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 4)            0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 4)            16          re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           64          batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16)           64          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 16)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            64          re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 4)            16          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 4)            0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4)            0           re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            24          flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            24          flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 6)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 6)            24          lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 320\n",
      "Trainable params: 252\n",
      "Non-trainable params: 68\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30)                210       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 852\n",
      "Trainable params: 752\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n",
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 336\n",
      "Trainable params: 280\n",
      "Non-trainable params: 56\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network.build_encoder(Z, nodes, n_features)\n",
    "decoder=network.build_decoder(Z, var, n_features, use_bias)\n",
    "discriminator=network.build_discriminator(Z, nodes)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model\n",
    "\n",
    "GANorWGAN='WGAN' #GAN\n",
    "epochs = 1000 #500\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 4)            8           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 4)            16          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 4)            0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 4)            16          re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 16)           64          batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16)           64          dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 16)           0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 4)            64          re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 4)            16          dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 4)            0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4)            0           re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 6)            24          flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 6)            24          flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 6)            0           dense_20[0][0]                   \n",
      "                                                                 dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 6)            24          lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 320\n",
      "Trainable params: 252\n",
      "Non-trainable params: 68\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 30)                210       \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 852\n",
      "Trainable params: 752\n",
      "Non-trainable params: 100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "aae = AAE_Model.AAE(Z, n_features, batch_size, GANorWGAN, nodes, var, use_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (100, 2)\n",
      "data shape_2 (100, 2)\n",
      "data shape_3 (100, 2)\n",
      "data shape_4 (100, 2)\n",
      "data shape_5 (100, 2)\n",
      "data shape_6 (100, 2)\n",
      "data shape_7 (100, 2)\n",
      "data shape_8 (100, 2)\n",
      "data shape_9 (100, 2)\n",
      "data shape_10 (100, 2)\n",
      "Cycles:  10\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### latent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_21/kernel:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_21/kernel:0'] when minimizing the loss.\n",
      "1 [D real: -0.528791, D fake: 0.532890], [Enc/Dec loss: 0.691861, Enc/Dis: 0.697879]\n",
      "2 [D real: -0.529945, D fake: 0.532353], [Enc/Dec loss: 0.703153, Enc/Dis: 0.709183]\n",
      "3 [D real: -0.529313, D fake: 0.532913], [Enc/Dec loss: 0.712167, Enc/Dis: 0.718189]\n",
      "4 [D real: -0.532457, D fake: 0.534657], [Enc/Dec loss: 0.686073, Enc/Dis: 0.692084]\n",
      "5 [D real: -0.534664, D fake: 0.536155], [Enc/Dec loss: 0.678888, Enc/Dis: 0.684908]\n",
      "6 [D real: -0.534898, D fake: 0.537612], [Enc/Dec loss: 0.683532, Enc/Dis: 0.689563]\n",
      "7 [D real: -0.536682, D fake: 0.537662], [Enc/Dec loss: 0.685526, Enc/Dis: 0.691565]\n",
      "8 [D real: -0.536764, D fake: 0.539039], [Enc/Dec loss: 0.680234, Enc/Dis: 0.686282]\n",
      "9 [D real: -0.538003, D fake: 0.540531], [Enc/Dec loss: 0.671624, Enc/Dis: 0.677677]\n",
      "10 [D real: -0.539402, D fake: 0.541160], [Enc/Dec loss: 0.661441, Enc/Dis: 0.667491]\n",
      "11 [D real: -0.553074, D fake: 0.548261], [Enc/Dec loss: 0.622927, Enc/Dis: 0.629042]\n",
      "12 [D real: -0.550694, D fake: 0.551208], [Enc/Dec loss: 0.655703, Enc/Dis: 0.661879]\n",
      "13 [D real: -0.552342, D fake: 0.552215], [Enc/Dec loss: 0.642207, Enc/Dis: 0.648381]\n",
      "14 [D real: -0.552459, D fake: 0.554018], [Enc/Dec loss: 0.620618, Enc/Dis: 0.626769]\n",
      "15 [D real: -0.553733, D fake: 0.555133], [Enc/Dec loss: 0.603452, Enc/Dis: 0.609595]\n",
      "16 [D real: -0.554270, D fake: 0.555800], [Enc/Dec loss: 0.600338, Enc/Dis: 0.606492]\n",
      "17 [D real: -0.554033, D fake: 0.557133], [Enc/Dec loss: 0.595894, Enc/Dis: 0.602048]\n",
      "18 [D real: -0.555415, D fake: 0.557805], [Enc/Dec loss: 0.588181, Enc/Dis: 0.594338]\n",
      "19 [D real: -0.556029, D fake: 0.558879], [Enc/Dec loss: 0.595648, Enc/Dis: 0.601819]\n",
      "20 [D real: -0.556484, D fake: 0.560088], [Enc/Dec loss: 0.589979, Enc/Dis: 0.596154]\n",
      "21 [D real: -0.564197, D fake: 0.569855], [Enc/Dec loss: 0.549693, Enc/Dis: 0.555911]\n",
      "22 [D real: -0.564866, D fake: 0.570352], [Enc/Dec loss: 0.547056, Enc/Dis: 0.553266]\n",
      "23 [D real: -0.566999, D fake: 0.571876], [Enc/Dec loss: 0.551427, Enc/Dis: 0.557640]\n",
      "24 [D real: -0.568051, D fake: 0.571751], [Enc/Dec loss: 0.546858, Enc/Dis: 0.553090]\n",
      "25 [D real: -0.569603, D fake: 0.571950], [Enc/Dec loss: 0.562950, Enc/Dis: 0.569202]\n",
      "26 [D real: -0.570454, D fake: 0.573170], [Enc/Dec loss: 0.545510, Enc/Dis: 0.551763]\n",
      "27 [D real: -0.570833, D fake: 0.574345], [Enc/Dec loss: 0.536476, Enc/Dis: 0.542733]\n",
      "28 [D real: -0.571507, D fake: 0.575139], [Enc/Dec loss: 0.534974, Enc/Dis: 0.541235]\n",
      "29 [D real: -0.572289, D fake: 0.576499], [Enc/Dec loss: 0.531986, Enc/Dis: 0.538255]\n",
      "30 [D real: -0.573030, D fake: 0.577464], [Enc/Dec loss: 0.524673, Enc/Dis: 0.530947]\n",
      "31 [D real: -0.583162, D fake: 0.589267], [Enc/Dec loss: 0.538103, Enc/Dis: 0.544427]\n",
      "32 [D real: -0.585095, D fake: 0.586820], [Enc/Dec loss: 0.543686, Enc/Dis: 0.550043]\n",
      "33 [D real: -0.583896, D fake: 0.588049], [Enc/Dec loss: 0.525651, Enc/Dis: 0.532008]\n",
      "34 [D real: -0.586049, D fake: 0.588994], [Enc/Dec loss: 0.488570, Enc/Dis: 0.494912]\n",
      "35 [D real: -0.586742, D fake: 0.588135], [Enc/Dec loss: 0.483630, Enc/Dis: 0.489975]\n",
      "36 [D real: -0.586699, D fake: 0.589296], [Enc/Dec loss: 0.475679, Enc/Dis: 0.482015]\n",
      "37 [D real: -0.587708, D fake: 0.588536], [Enc/Dec loss: 0.473499, Enc/Dis: 0.479847]\n",
      "38 [D real: -0.588742, D fake: 0.588939], [Enc/Dec loss: 0.462710, Enc/Dis: 0.469063]\n",
      "39 [D real: -0.589710, D fake: 0.589518], [Enc/Dec loss: 0.454936, Enc/Dis: 0.461291]\n",
      "40 [D real: -0.590530, D fake: 0.590221], [Enc/Dec loss: 0.456589, Enc/Dis: 0.462962]\n",
      "41 [D real: -0.597589, D fake: 0.602416], [Enc/Dec loss: 0.381769, Enc/Dis: 0.388140]\n",
      "42 [D real: -0.596139, D fake: 0.601334], [Enc/Dec loss: 0.348650, Enc/Dis: 0.354964]\n",
      "43 [D real: -0.598569, D fake: 0.600997], [Enc/Dec loss: 0.363879, Enc/Dis: 0.370231]\n",
      "44 [D real: -0.599285, D fake: 0.603007], [Enc/Dec loss: 0.382965, Enc/Dis: 0.389357]\n",
      "45 [D real: -0.598778, D fake: 0.602583], [Enc/Dec loss: 0.390602, Enc/Dis: 0.397014]\n",
      "46 [D real: -0.600415, D fake: 0.601579], [Enc/Dec loss: 0.389826, Enc/Dis: 0.396256]\n",
      "47 [D real: -0.601655, D fake: 0.602993], [Enc/Dec loss: 0.392482, Enc/Dis: 0.398918]\n",
      "48 [D real: -0.602227, D fake: 0.603529], [Enc/Dec loss: 0.389261, Enc/Dis: 0.395698]\n",
      "49 [D real: -0.602810, D fake: 0.604019], [Enc/Dec loss: 0.390540, Enc/Dis: 0.396990]\n",
      "50 [D real: -0.604259, D fake: 0.604210], [Enc/Dec loss: 0.389624, Enc/Dis: 0.396085]\n",
      "51 [D real: -0.615378, D fake: 0.616118], [Enc/Dec loss: 0.419991, Enc/Dis: 0.426577]\n",
      "52 [D real: -0.614655, D fake: 0.615481], [Enc/Dec loss: 0.400263, Enc/Dis: 0.406814]\n",
      "53 [D real: -0.611947, D fake: 0.615943], [Enc/Dec loss: 0.378051, Enc/Dis: 0.384575]\n",
      "54 [D real: -0.614685, D fake: 0.618117], [Enc/Dec loss: 0.382983, Enc/Dis: 0.389519]\n",
      "55 [D real: -0.614772, D fake: 0.618805], [Enc/Dec loss: 0.370494, Enc/Dis: 0.377028]\n",
      "56 [D real: -0.616014, D fake: 0.619886], [Enc/Dec loss: 0.365153, Enc/Dis: 0.371677]\n",
      "57 [D real: -0.616920, D fake: 0.620235], [Enc/Dec loss: 0.358309, Enc/Dis: 0.364840]\n",
      "58 [D real: -0.617804, D fake: 0.620662], [Enc/Dec loss: 0.357134, Enc/Dis: 0.363678]\n",
      "59 [D real: -0.617813, D fake: 0.621147], [Enc/Dec loss: 0.360421, Enc/Dis: 0.366976]\n",
      "60 [D real: -0.618174, D fake: 0.621809], [Enc/Dec loss: 0.358114, Enc/Dis: 0.364672]\n",
      "61 [D real: -0.622619, D fake: 0.630385], [Enc/Dec loss: 0.363374, Enc/Dis: 0.370051]\n",
      "62 [D real: -0.626782, D fake: 0.630401], [Enc/Dec loss: 0.335417, Enc/Dis: 0.342061]\n",
      "63 [D real: -0.628511, D fake: 0.631369], [Enc/Dec loss: 0.322313, Enc/Dis: 0.328964]\n",
      "64 [D real: -0.628377, D fake: 0.633495], [Enc/Dec loss: 0.315289, Enc/Dis: 0.321926]\n",
      "65 [D real: -0.629263, D fake: 0.634778], [Enc/Dec loss: 0.313492, Enc/Dis: 0.320135]\n",
      "66 [D real: -0.630475, D fake: 0.635218], [Enc/Dec loss: 0.317306, Enc/Dis: 0.323949]\n",
      "67 [D real: -0.631034, D fake: 0.636038], [Enc/Dec loss: 0.314784, Enc/Dis: 0.321426]\n",
      "68 [D real: -0.631807, D fake: 0.636649], [Enc/Dec loss: 0.323667, Enc/Dis: 0.330336]\n",
      "69 [D real: -0.633585, D fake: 0.637179], [Enc/Dec loss: 0.326587, Enc/Dis: 0.333261]\n",
      "70 [D real: -0.633740, D fake: 0.637639], [Enc/Dec loss: 0.323261, Enc/Dis: 0.329937]\n",
      "71 [D real: -0.646592, D fake: 0.645590], [Enc/Dec loss: 0.286981, Enc/Dis: 0.293677]\n",
      "72 [D real: -0.645421, D fake: 0.639876], [Enc/Dec loss: 0.330186, Enc/Dis: 0.336952]\n",
      "73 [D real: -0.644470, D fake: 0.642039], [Enc/Dec loss: 0.303052, Enc/Dis: 0.309814]\n",
      "74 [D real: -0.645782, D fake: 0.644129], [Enc/Dec loss: 0.299083, Enc/Dis: 0.305843]\n",
      "75 [D real: -0.646118, D fake: 0.644933], [Enc/Dec loss: 0.289536, Enc/Dis: 0.296296]\n",
      "76 [D real: -0.647139, D fake: 0.646054], [Enc/Dec loss: 0.286297, Enc/Dis: 0.293057]\n",
      "77 [D real: -0.647746, D fake: 0.646957], [Enc/Dec loss: 0.291387, Enc/Dis: 0.298157]\n",
      "78 [D real: -0.649089, D fake: 0.647188], [Enc/Dec loss: 0.294544, Enc/Dis: 0.301322]\n",
      "79 [D real: -0.649049, D fake: 0.648587], [Enc/Dec loss: 0.292921, Enc/Dis: 0.299700]\n",
      "80 [D real: -0.649704, D fake: 0.650172], [Enc/Dec loss: 0.296049, Enc/Dis: 0.302843]\n",
      "81 [D real: -0.652488, D fake: 0.664500], [Enc/Dec loss: 0.277710, Enc/Dis: 0.284600]\n",
      "82 [D real: -0.654691, D fake: 0.665413], [Enc/Dec loss: 0.271525, Enc/Dis: 0.278382]\n",
      "83 [D real: -0.658125, D fake: 0.665353], [Enc/Dec loss: 0.274297, Enc/Dis: 0.281156]\n",
      "84 [D real: -0.661354, D fake: 0.666592], [Enc/Dec loss: 0.280051, Enc/Dis: 0.286924]\n",
      "85 [D real: -0.661555, D fake: 0.665928], [Enc/Dec loss: 0.290868, Enc/Dis: 0.297760]\n",
      "86 [D real: -0.662036, D fake: 0.664723], [Enc/Dec loss: 0.292286, Enc/Dis: 0.299176]\n",
      "87 [D real: -0.663325, D fake: 0.665543], [Enc/Dec loss: 0.288588, Enc/Dis: 0.295486]\n",
      "88 [D real: -0.664109, D fake: 0.666578], [Enc/Dec loss: 0.287099, Enc/Dis: 0.294005]\n",
      "89 [D real: -0.664974, D fake: 0.667575], [Enc/Dec loss: 0.284020, Enc/Dis: 0.290930]\n",
      "90 [D real: -0.665795, D fake: 0.668287], [Enc/Dec loss: 0.282854, Enc/Dis: 0.289770]\n",
      "91 [D real: -0.666102, D fake: 0.674644], [Enc/Dec loss: 0.265842, Enc/Dis: 0.272830]\n",
      "92 [D real: -0.674267, D fake: 0.674930], [Enc/Dec loss: 0.252869, Enc/Dis: 0.259878]\n",
      "93 [D real: -0.676578, D fake: 0.675251], [Enc/Dec loss: 0.246392, Enc/Dis: 0.253396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 [D real: -0.676149, D fake: 0.676044], [Enc/Dec loss: 0.257847, Enc/Dis: 0.264890]\n",
      "95 [D real: -0.677423, D fake: 0.676869], [Enc/Dec loss: 0.247803, Enc/Dis: 0.254836]\n",
      "96 [D real: -0.676587, D fake: 0.676819], [Enc/Dec loss: 0.249709, Enc/Dis: 0.256738]\n",
      "97 [D real: -0.676790, D fake: 0.678052], [Enc/Dec loss: 0.251827, Enc/Dis: 0.258874]\n",
      "98 [D real: -0.677397, D fake: 0.679763], [Enc/Dec loss: 0.248536, Enc/Dis: 0.255586]\n",
      "99 [D real: -0.678744, D fake: 0.680464], [Enc/Dec loss: 0.250593, Enc/Dis: 0.257660]\n",
      "100 [D real: -0.679049, D fake: 0.681877], [Enc/Dec loss: 0.248359, Enc/Dis: 0.255428]\n",
      "101 [D real: -0.690583, D fake: 0.690007], [Enc/Dec loss: 0.247550, Enc/Dis: 0.254675]\n",
      "102 [D real: -0.689958, D fake: 0.691703], [Enc/Dec loss: 0.240888, Enc/Dis: 0.248045]\n",
      "103 [D real: -0.690459, D fake: 0.692571], [Enc/Dec loss: 0.260047, Enc/Dis: 0.267219]\n",
      "104 [D real: -0.691593, D fake: 0.694026], [Enc/Dec loss: 0.255335, Enc/Dis: 0.262503]\n",
      "105 [D real: -0.691739, D fake: 0.694536], [Enc/Dec loss: 0.250225, Enc/Dis: 0.257393]\n",
      "106 [D real: -0.693798, D fake: 0.695222], [Enc/Dec loss: 0.247156, Enc/Dis: 0.254341]\n",
      "107 [D real: -0.694668, D fake: 0.696080], [Enc/Dec loss: 0.249360, Enc/Dis: 0.256546]\n",
      "108 [D real: -0.695572, D fake: 0.697608], [Enc/Dec loss: 0.253193, Enc/Dis: 0.260386]\n",
      "109 [D real: -0.695850, D fake: 0.698330], [Enc/Dec loss: 0.254398, Enc/Dis: 0.261605]\n",
      "110 [D real: -0.696435, D fake: 0.698494], [Enc/Dec loss: 0.253769, Enc/Dis: 0.260984]\n",
      "111 [D real: -0.706929, D fake: 0.703748], [Enc/Dec loss: 0.242501, Enc/Dis: 0.249733]\n",
      "112 [D real: -0.705046, D fake: 0.707222], [Enc/Dec loss: 0.249012, Enc/Dis: 0.256269]\n",
      "113 [D real: -0.705913, D fake: 0.708370], [Enc/Dec loss: 0.243271, Enc/Dis: 0.250554]\n",
      "114 [D real: -0.707155, D fake: 0.709852], [Enc/Dec loss: 0.241074, Enc/Dis: 0.248361]\n",
      "115 [D real: -0.709318, D fake: 0.709924], [Enc/Dec loss: 0.243626, Enc/Dis: 0.250917]\n",
      "116 [D real: -0.709997, D fake: 0.709752], [Enc/Dec loss: 0.243381, Enc/Dis: 0.250685]\n",
      "117 [D real: -0.711133, D fake: 0.710303], [Enc/Dec loss: 0.250784, Enc/Dis: 0.258106]\n",
      "118 [D real: -0.711526, D fake: 0.709949], [Enc/Dec loss: 0.247297, Enc/Dis: 0.254630]\n",
      "119 [D real: -0.711825, D fake: 0.710829], [Enc/Dec loss: 0.243823, Enc/Dis: 0.251156]\n",
      "120 [D real: -0.712615, D fake: 0.711736], [Enc/Dec loss: 0.240288, Enc/Dis: 0.247629]\n",
      "121 [D real: -0.720413, D fake: 0.716694], [Enc/Dec loss: 0.210250, Enc/Dis: 0.217606]\n",
      "122 [D real: -0.720473, D fake: 0.720150], [Enc/Dec loss: 0.228900, Enc/Dis: 0.236292]\n",
      "123 [D real: -0.719290, D fake: 0.720078], [Enc/Dec loss: 0.226046, Enc/Dis: 0.233452]\n",
      "124 [D real: -0.719981, D fake: 0.723036], [Enc/Dec loss: 0.220725, Enc/Dis: 0.228143]\n",
      "125 [D real: -0.721560, D fake: 0.724535], [Enc/Dec loss: 0.214422, Enc/Dis: 0.221852]\n",
      "126 [D real: -0.722418, D fake: 0.725483], [Enc/Dec loss: 0.220856, Enc/Dis: 0.228297]\n",
      "127 [D real: -0.722474, D fake: 0.726089], [Enc/Dec loss: 0.219746, Enc/Dis: 0.227190]\n",
      "128 [D real: -0.723489, D fake: 0.726627], [Enc/Dec loss: 0.220028, Enc/Dis: 0.227479]\n",
      "129 [D real: -0.724599, D fake: 0.726923], [Enc/Dec loss: 0.217999, Enc/Dis: 0.225455]\n",
      "130 [D real: -0.725019, D fake: 0.727789], [Enc/Dec loss: 0.218708, Enc/Dis: 0.226170]\n",
      "131 [D real: -0.734084, D fake: 0.731893], [Enc/Dec loss: 0.218871, Enc/Dis: 0.226501]\n",
      "132 [D real: -0.734898, D fake: 0.732924], [Enc/Dec loss: 0.226156, Enc/Dis: 0.233781]\n",
      "133 [D real: -0.735649, D fake: 0.733687], [Enc/Dec loss: 0.209254, Enc/Dis: 0.216860]\n",
      "134 [D real: -0.735793, D fake: 0.735219], [Enc/Dec loss: 0.207523, Enc/Dis: 0.215131]\n",
      "135 [D real: -0.735875, D fake: 0.735146], [Enc/Dec loss: 0.215201, Enc/Dis: 0.222812]\n",
      "136 [D real: -0.736294, D fake: 0.737137], [Enc/Dec loss: 0.212325, Enc/Dis: 0.219937]\n",
      "137 [D real: -0.737597, D fake: 0.738467], [Enc/Dec loss: 0.212842, Enc/Dis: 0.220457]\n",
      "138 [D real: -0.738194, D fake: 0.739204], [Enc/Dec loss: 0.215559, Enc/Dis: 0.223183]\n",
      "139 [D real: -0.739046, D fake: 0.739448], [Enc/Dec loss: 0.214103, Enc/Dis: 0.221731]\n",
      "140 [D real: -0.740025, D fake: 0.740683], [Enc/Dec loss: 0.214407, Enc/Dis: 0.222040]\n",
      "141 [D real: -0.744353, D fake: 0.747411], [Enc/Dec loss: 0.237693, Enc/Dis: 0.245471]\n",
      "142 [D real: -0.748892, D fake: 0.749138], [Enc/Dec loss: 0.209411, Enc/Dis: 0.217150]\n",
      "143 [D real: -0.748356, D fake: 0.751403], [Enc/Dec loss: 0.209238, Enc/Dis: 0.216965]\n",
      "144 [D real: -0.748434, D fake: 0.753097], [Enc/Dec loss: 0.204910, Enc/Dis: 0.212639]\n",
      "145 [D real: -0.748659, D fake: 0.752058], [Enc/Dec loss: 0.201836, Enc/Dis: 0.209569]\n",
      "146 [D real: -0.750263, D fake: 0.752502], [Enc/Dec loss: 0.208775, Enc/Dis: 0.216525]\n",
      "147 [D real: -0.751017, D fake: 0.753138], [Enc/Dec loss: 0.206072, Enc/Dis: 0.213826]\n",
      "148 [D real: -0.752014, D fake: 0.753366], [Enc/Dec loss: 0.202235, Enc/Dis: 0.209986]\n",
      "149 [D real: -0.752829, D fake: 0.753813], [Enc/Dec loss: 0.200900, Enc/Dis: 0.208652]\n",
      "150 [D real: -0.753181, D fake: 0.754102], [Enc/Dec loss: 0.198459, Enc/Dis: 0.206210]\n",
      "151 [D real: -0.758705, D fake: 0.758345], [Enc/Dec loss: 0.168079, Enc/Dis: 0.175821]\n",
      "152 [D real: -0.760623, D fake: 0.760798], [Enc/Dec loss: 0.162438, Enc/Dis: 0.170187]\n",
      "153 [D real: -0.761127, D fake: 0.762961], [Enc/Dec loss: 0.169054, Enc/Dis: 0.176836]\n",
      "154 [D real: -0.761530, D fake: 0.764261], [Enc/Dec loss: 0.180685, Enc/Dis: 0.188491]\n",
      "155 [D real: -0.762459, D fake: 0.765866], [Enc/Dec loss: 0.180690, Enc/Dis: 0.188503]\n",
      "156 [D real: -0.762350, D fake: 0.766750], [Enc/Dec loss: 0.185915, Enc/Dis: 0.193743]\n",
      "157 [D real: -0.762929, D fake: 0.767325], [Enc/Dec loss: 0.188482, Enc/Dis: 0.196320]\n",
      "158 [D real: -0.764331, D fake: 0.767821], [Enc/Dec loss: 0.189241, Enc/Dis: 0.197090]\n",
      "159 [D real: -0.764936, D fake: 0.768010], [Enc/Dec loss: 0.188814, Enc/Dis: 0.196673]\n",
      "160 [D real: -0.765990, D fake: 0.768668], [Enc/Dec loss: 0.190074, Enc/Dis: 0.197940]\n",
      "161 [D real: -0.769735, D fake: 0.773460], [Enc/Dec loss: 0.213340, Enc/Dis: 0.221289]\n",
      "162 [D real: -0.770623, D fake: 0.772350], [Enc/Dec loss: 0.191907, Enc/Dis: 0.199801]\n",
      "163 [D real: -0.772229, D fake: 0.773890], [Enc/Dec loss: 0.182189, Enc/Dis: 0.190092]\n",
      "164 [D real: -0.772732, D fake: 0.774792], [Enc/Dec loss: 0.186422, Enc/Dis: 0.194333]\n",
      "165 [D real: -0.774622, D fake: 0.775898], [Enc/Dec loss: 0.187620, Enc/Dis: 0.195548]\n",
      "166 [D real: -0.773988, D fake: 0.777040], [Enc/Dec loss: 0.186498, Enc/Dis: 0.194434]\n",
      "167 [D real: -0.775585, D fake: 0.777811], [Enc/Dec loss: 0.185970, Enc/Dis: 0.193919]\n",
      "168 [D real: -0.776592, D fake: 0.778587], [Enc/Dec loss: 0.183948, Enc/Dis: 0.191898]\n",
      "169 [D real: -0.777264, D fake: 0.779156], [Enc/Dec loss: 0.184220, Enc/Dis: 0.192184]\n",
      "170 [D real: -0.778025, D fake: 0.779416], [Enc/Dec loss: 0.188469, Enc/Dis: 0.196443]\n",
      "171 [D real: -0.785891, D fake: 0.791231], [Enc/Dec loss: 0.162810, Enc/Dis: 0.170822]\n",
      "172 [D real: -0.788593, D fake: 0.791082], [Enc/Dec loss: 0.163962, Enc/Dis: 0.171999]\n",
      "173 [D real: -0.788648, D fake: 0.791375], [Enc/Dec loss: 0.164022, Enc/Dis: 0.172074]\n",
      "174 [D real: -0.789349, D fake: 0.791301], [Enc/Dec loss: 0.165723, Enc/Dis: 0.173783]\n",
      "175 [D real: -0.789436, D fake: 0.791846], [Enc/Dec loss: 0.175823, Enc/Dis: 0.183900]\n",
      "176 [D real: -0.790306, D fake: 0.791992], [Enc/Dec loss: 0.177094, Enc/Dis: 0.185188]\n",
      "177 [D real: -0.790757, D fake: 0.793252], [Enc/Dec loss: 0.177481, Enc/Dis: 0.185582]\n",
      "178 [D real: -0.791726, D fake: 0.793683], [Enc/Dec loss: 0.177517, Enc/Dis: 0.185625]\n",
      "179 [D real: -0.792616, D fake: 0.794300], [Enc/Dec loss: 0.175768, Enc/Dis: 0.183877]\n",
      "180 [D real: -0.793141, D fake: 0.794764], [Enc/Dec loss: 0.177310, Enc/Dis: 0.185423]\n",
      "181 [D real: -0.800418, D fake: 0.800171], [Enc/Dec loss: 0.173062, Enc/Dis: 0.181213]\n",
      "182 [D real: -0.801086, D fake: 0.798937], [Enc/Dec loss: 0.187879, Enc/Dis: 0.196037]\n",
      "183 [D real: -0.801679, D fake: 0.797440], [Enc/Dec loss: 0.174647, Enc/Dis: 0.182804]\n",
      "184 [D real: -0.801374, D fake: 0.799036], [Enc/Dec loss: 0.174649, Enc/Dis: 0.182812]\n",
      "185 [D real: -0.802505, D fake: 0.800380], [Enc/Dec loss: 0.169534, Enc/Dis: 0.177697]\n",
      "186 [D real: -0.803556, D fake: 0.801527], [Enc/Dec loss: 0.166698, Enc/Dis: 0.174868]\n",
      "187 [D real: -0.803783, D fake: 0.801951], [Enc/Dec loss: 0.164496, Enc/Dis: 0.172674]\n",
      "188 [D real: -0.803976, D fake: 0.802474], [Enc/Dec loss: 0.159608, Enc/Dis: 0.167788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 [D real: -0.804053, D fake: 0.803205], [Enc/Dec loss: 0.162415, Enc/Dis: 0.170602]\n",
      "190 [D real: -0.804312, D fake: 0.804145], [Enc/Dec loss: 0.164780, Enc/Dis: 0.172976]\n",
      "191 [D real: -0.811249, D fake: 0.813490], [Enc/Dec loss: 0.155015, Enc/Dis: 0.163277]\n",
      "192 [D real: -0.812673, D fake: 0.813541], [Enc/Dec loss: 0.165492, Enc/Dis: 0.173774]\n",
      "193 [D real: -0.814359, D fake: 0.813464], [Enc/Dec loss: 0.171806, Enc/Dis: 0.180102]\n",
      "194 [D real: -0.814107, D fake: 0.814325], [Enc/Dec loss: 0.170391, Enc/Dis: 0.178695]\n",
      "195 [D real: -0.814867, D fake: 0.815297], [Enc/Dec loss: 0.168911, Enc/Dis: 0.177221]\n",
      "196 [D real: -0.815621, D fake: 0.815788], [Enc/Dec loss: 0.166745, Enc/Dis: 0.175061]\n",
      "197 [D real: -0.815926, D fake: 0.816480], [Enc/Dec loss: 0.165212, Enc/Dis: 0.173534]\n",
      "198 [D real: -0.816136, D fake: 0.817375], [Enc/Dec loss: 0.166391, Enc/Dis: 0.174717]\n",
      "199 [D real: -0.816896, D fake: 0.818171], [Enc/Dec loss: 0.165883, Enc/Dis: 0.174206]\n",
      "200 [D real: -0.817240, D fake: 0.818327], [Enc/Dec loss: 0.167238, Enc/Dis: 0.175570]\n",
      "201 [D real: -0.817034, D fake: 0.822400], [Enc/Dec loss: 0.157318, Enc/Dis: 0.165724]\n",
      "202 [D real: -0.820453, D fake: 0.823201], [Enc/Dec loss: 0.160159, Enc/Dis: 0.168578]\n",
      "203 [D real: -0.821779, D fake: 0.825107], [Enc/Dec loss: 0.148764, Enc/Dis: 0.157154]\n",
      "204 [D real: -0.823092, D fake: 0.824762], [Enc/Dec loss: 0.148419, Enc/Dis: 0.156820]\n",
      "205 [D real: -0.823076, D fake: 0.825058], [Enc/Dec loss: 0.153820, Enc/Dis: 0.162229]\n",
      "206 [D real: -0.824292, D fake: 0.826069], [Enc/Dec loss: 0.154677, Enc/Dis: 0.163088]\n",
      "207 [D real: -0.824807, D fake: 0.826400], [Enc/Dec loss: 0.153272, Enc/Dis: 0.161691]\n",
      "208 [D real: -0.825711, D fake: 0.827127], [Enc/Dec loss: 0.157717, Enc/Dis: 0.166143]\n",
      "209 [D real: -0.826633, D fake: 0.827344], [Enc/Dec loss: 0.160661, Enc/Dis: 0.169094]\n",
      "210 [D real: -0.827499, D fake: 0.827967], [Enc/Dec loss: 0.159592, Enc/Dis: 0.168026]\n",
      "211 [D real: -0.836675, D fake: 0.834984], [Enc/Dec loss: 0.143771, Enc/Dis: 0.152218]\n",
      "212 [D real: -0.835628, D fake: 0.835105], [Enc/Dec loss: 0.149316, Enc/Dis: 0.157795]\n",
      "213 [D real: -0.835904, D fake: 0.834937], [Enc/Dec loss: 0.155232, Enc/Dis: 0.163727]\n",
      "214 [D real: -0.836503, D fake: 0.834176], [Enc/Dec loss: 0.158971, Enc/Dis: 0.167466]\n",
      "215 [D real: -0.836084, D fake: 0.835363], [Enc/Dec loss: 0.154031, Enc/Dis: 0.162531]\n",
      "216 [D real: -0.836690, D fake: 0.835464], [Enc/Dec loss: 0.148846, Enc/Dis: 0.157342]\n",
      "217 [D real: -0.836663, D fake: 0.836597], [Enc/Dec loss: 0.151407, Enc/Dis: 0.159912]\n",
      "218 [D real: -0.837726, D fake: 0.837423], [Enc/Dec loss: 0.154758, Enc/Dis: 0.163271]\n",
      "219 [D real: -0.837874, D fake: 0.837716], [Enc/Dec loss: 0.153904, Enc/Dis: 0.162424]\n",
      "220 [D real: -0.838449, D fake: 0.838491], [Enc/Dec loss: 0.153437, Enc/Dis: 0.161961]\n",
      "221 [D real: -0.845441, D fake: 0.847429], [Enc/Dec loss: 0.155895, Enc/Dis: 0.164470]\n",
      "222 [D real: -0.843817, D fake: 0.844947], [Enc/Dec loss: 0.146921, Enc/Dis: 0.155510]\n",
      "223 [D real: -0.843477, D fake: 0.846129], [Enc/Dec loss: 0.147501, Enc/Dis: 0.156100]\n",
      "224 [D real: -0.844386, D fake: 0.845816], [Enc/Dec loss: 0.150500, Enc/Dis: 0.159109]\n",
      "225 [D real: -0.845060, D fake: 0.846307], [Enc/Dec loss: 0.150060, Enc/Dis: 0.158672]\n",
      "226 [D real: -0.845696, D fake: 0.846434], [Enc/Dec loss: 0.145312, Enc/Dis: 0.153923]\n",
      "227 [D real: -0.846441, D fake: 0.846699], [Enc/Dec loss: 0.143241, Enc/Dis: 0.151856]\n",
      "228 [D real: -0.846555, D fake: 0.847927], [Enc/Dec loss: 0.141552, Enc/Dis: 0.150170]\n",
      "229 [D real: -0.847451, D fake: 0.848422], [Enc/Dec loss: 0.140097, Enc/Dis: 0.148718]\n",
      "230 [D real: -0.848055, D fake: 0.849055], [Enc/Dec loss: 0.138743, Enc/Dis: 0.147368]\n",
      "231 [D real: -0.852132, D fake: 0.857780], [Enc/Dec loss: 0.155224, Enc/Dis: 0.163938]\n",
      "232 [D real: -0.854489, D fake: 0.857271], [Enc/Dec loss: 0.165515, Enc/Dis: 0.174236]\n",
      "233 [D real: -0.855499, D fake: 0.855610], [Enc/Dec loss: 0.141612, Enc/Dis: 0.150312]\n",
      "234 [D real: -0.855695, D fake: 0.855532], [Enc/Dec loss: 0.143805, Enc/Dis: 0.152508]\n",
      "235 [D real: -0.855743, D fake: 0.856449], [Enc/Dec loss: 0.147253, Enc/Dis: 0.155963]\n",
      "236 [D real: -0.856332, D fake: 0.856584], [Enc/Dec loss: 0.147212, Enc/Dis: 0.155932]\n",
      "237 [D real: -0.856236, D fake: 0.857075], [Enc/Dec loss: 0.145221, Enc/Dis: 0.153942]\n",
      "238 [D real: -0.855769, D fake: 0.857681], [Enc/Dec loss: 0.146602, Enc/Dis: 0.155332]\n",
      "239 [D real: -0.856333, D fake: 0.858380], [Enc/Dec loss: 0.143311, Enc/Dis: 0.152041]\n",
      "240 [D real: -0.857192, D fake: 0.858962], [Enc/Dec loss: 0.141480, Enc/Dis: 0.150213]\n",
      "241 [D real: -0.867255, D fake: 0.864331], [Enc/Dec loss: 0.140142, Enc/Dis: 0.148957]\n",
      "242 [D real: -0.865873, D fake: 0.864778], [Enc/Dec loss: 0.134128, Enc/Dis: 0.142884]\n",
      "243 [D real: -0.866484, D fake: 0.864929], [Enc/Dec loss: 0.140311, Enc/Dis: 0.149099]\n",
      "244 [D real: -0.866847, D fake: 0.865854], [Enc/Dec loss: 0.138932, Enc/Dis: 0.147726]\n",
      "245 [D real: -0.867017, D fake: 0.866298], [Enc/Dec loss: 0.135973, Enc/Dis: 0.144771]\n",
      "246 [D real: -0.866616, D fake: 0.866360], [Enc/Dec loss: 0.135232, Enc/Dis: 0.144040]\n",
      "247 [D real: -0.867177, D fake: 0.866542], [Enc/Dec loss: 0.135397, Enc/Dis: 0.144210]\n",
      "248 [D real: -0.867455, D fake: 0.867271], [Enc/Dec loss: 0.140732, Enc/Dis: 0.149554]\n",
      "249 [D real: -0.867997, D fake: 0.867352], [Enc/Dec loss: 0.137311, Enc/Dis: 0.146136]\n",
      "250 [D real: -0.868373, D fake: 0.867715], [Enc/Dec loss: 0.136149, Enc/Dis: 0.144976]\n",
      "251 [D real: -0.875973, D fake: 0.871863], [Enc/Dec loss: 0.147777, Enc/Dis: 0.156615]\n",
      "252 [D real: -0.875614, D fake: 0.873050], [Enc/Dec loss: 0.149125, Enc/Dis: 0.157975]\n",
      "253 [D real: -0.875124, D fake: 0.873866], [Enc/Dec loss: 0.144365, Enc/Dis: 0.153237]\n",
      "254 [D real: -0.873950, D fake: 0.874692], [Enc/Dec loss: 0.141767, Enc/Dis: 0.150645]\n",
      "255 [D real: -0.874013, D fake: 0.875216], [Enc/Dec loss: 0.138572, Enc/Dis: 0.147453]\n",
      "256 [D real: -0.874650, D fake: 0.875381], [Enc/Dec loss: 0.139173, Enc/Dis: 0.148062]\n",
      "257 [D real: -0.875181, D fake: 0.875619], [Enc/Dec loss: 0.140587, Enc/Dis: 0.149479]\n",
      "258 [D real: -0.875782, D fake: 0.876162], [Enc/Dec loss: 0.139128, Enc/Dis: 0.148019]\n",
      "259 [D real: -0.876482, D fake: 0.876874], [Enc/Dec loss: 0.137318, Enc/Dis: 0.146210]\n",
      "260 [D real: -0.877072, D fake: 0.877286], [Enc/Dec loss: 0.135964, Enc/Dis: 0.144859]\n",
      "261 [D real: -0.882042, D fake: 0.883581], [Enc/Dec loss: 0.108018, Enc/Dis: 0.116937]\n",
      "262 [D real: -0.881781, D fake: 0.882532], [Enc/Dec loss: 0.116069, Enc/Dis: 0.125024]\n",
      "263 [D real: -0.882796, D fake: 0.882781], [Enc/Dec loss: 0.122800, Enc/Dis: 0.131758]\n",
      "264 [D real: -0.882266, D fake: 0.883405], [Enc/Dec loss: 0.125011, Enc/Dis: 0.133975]\n",
      "265 [D real: -0.882642, D fake: 0.883338], [Enc/Dec loss: 0.125968, Enc/Dis: 0.134930]\n",
      "266 [D real: -0.882941, D fake: 0.883607], [Enc/Dec loss: 0.131798, Enc/Dis: 0.140770]\n",
      "267 [D real: -0.882829, D fake: 0.884048], [Enc/Dec loss: 0.137176, Enc/Dis: 0.146149]\n",
      "268 [D real: -0.883316, D fake: 0.884470], [Enc/Dec loss: 0.137505, Enc/Dis: 0.146486]\n",
      "269 [D real: -0.883718, D fake: 0.885158], [Enc/Dec loss: 0.136936, Enc/Dis: 0.145922]\n",
      "270 [D real: -0.884000, D fake: 0.884955], [Enc/Dec loss: 0.135488, Enc/Dis: 0.144478]\n",
      "271 [D real: -0.890583, D fake: 0.887339], [Enc/Dec loss: 0.131672, Enc/Dis: 0.140665]\n",
      "272 [D real: -0.890591, D fake: 0.889027], [Enc/Dec loss: 0.139225, Enc/Dis: 0.148251]\n",
      "273 [D real: -0.890760, D fake: 0.889864], [Enc/Dec loss: 0.132044, Enc/Dis: 0.141071]\n",
      "274 [D real: -0.891593, D fake: 0.891425], [Enc/Dec loss: 0.136607, Enc/Dis: 0.145643]\n",
      "275 [D real: -0.891837, D fake: 0.891885], [Enc/Dec loss: 0.139560, Enc/Dis: 0.148603]\n",
      "276 [D real: -0.892084, D fake: 0.891656], [Enc/Dec loss: 0.133994, Enc/Dis: 0.143036]\n",
      "277 [D real: -0.892577, D fake: 0.892107], [Enc/Dec loss: 0.134366, Enc/Dis: 0.143413]\n",
      "278 [D real: -0.892650, D fake: 0.892443], [Enc/Dec loss: 0.132174, Enc/Dis: 0.141221]\n",
      "279 [D real: -0.893246, D fake: 0.892843], [Enc/Dec loss: 0.133209, Enc/Dis: 0.142261]\n",
      "280 [D real: -0.893352, D fake: 0.893367], [Enc/Dec loss: 0.131812, Enc/Dis: 0.140868]\n",
      "281 [D real: -0.898017, D fake: 0.898281], [Enc/Dec loss: 0.120007, Enc/Dis: 0.129121]\n",
      "282 [D real: -0.897152, D fake: 0.897303], [Enc/Dec loss: 0.137061, Enc/Dis: 0.146187]\n",
      "283 [D real: -0.896378, D fake: 0.898274], [Enc/Dec loss: 0.130216, Enc/Dis: 0.139336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 [D real: -0.896817, D fake: 0.899097], [Enc/Dec loss: 0.137803, Enc/Dis: 0.146936]\n",
      "285 [D real: -0.897399, D fake: 0.899638], [Enc/Dec loss: 0.133304, Enc/Dis: 0.142434]\n",
      "286 [D real: -0.897934, D fake: 0.899767], [Enc/Dec loss: 0.131457, Enc/Dis: 0.140586]\n",
      "287 [D real: -0.898255, D fake: 0.900421], [Enc/Dec loss: 0.131611, Enc/Dis: 0.140742]\n",
      "288 [D real: -0.898733, D fake: 0.900823], [Enc/Dec loss: 0.129810, Enc/Dis: 0.138942]\n",
      "289 [D real: -0.899170, D fake: 0.901345], [Enc/Dec loss: 0.130216, Enc/Dis: 0.139351]\n",
      "290 [D real: -0.899523, D fake: 0.901721], [Enc/Dec loss: 0.127064, Enc/Dis: 0.136197]\n",
      "291 [D real: -0.906275, D fake: 0.905958], [Enc/Dec loss: 0.119798, Enc/Dis: 0.128947]\n",
      "292 [D real: -0.905497, D fake: 0.907026], [Enc/Dec loss: 0.122919, Enc/Dis: 0.132083]\n",
      "293 [D real: -0.906156, D fake: 0.906563], [Enc/Dec loss: 0.125114, Enc/Dis: 0.134288]\n",
      "294 [D real: -0.906524, D fake: 0.906286], [Enc/Dec loss: 0.119335, Enc/Dis: 0.128508]\n",
      "295 [D real: -0.906986, D fake: 0.906819], [Enc/Dec loss: 0.117517, Enc/Dis: 0.126694]\n",
      "296 [D real: -0.907505, D fake: 0.907249], [Enc/Dec loss: 0.122049, Enc/Dis: 0.131236]\n",
      "297 [D real: -0.907850, D fake: 0.907226], [Enc/Dec loss: 0.127545, Enc/Dis: 0.136744]\n",
      "298 [D real: -0.908103, D fake: 0.907747], [Enc/Dec loss: 0.129850, Enc/Dis: 0.139053]\n",
      "299 [D real: -0.907994, D fake: 0.908197], [Enc/Dec loss: 0.128495, Enc/Dis: 0.137702]\n",
      "300 [D real: -0.908188, D fake: 0.908677], [Enc/Dec loss: 0.130618, Enc/Dis: 0.139833]\n",
      "301 [D real: -0.912017, D fake: 0.912179], [Enc/Dec loss: 0.122654, Enc/Dis: 0.131897]\n",
      "302 [D real: -0.912524, D fake: 0.912334], [Enc/Dec loss: 0.127265, Enc/Dis: 0.136520]\n",
      "303 [D real: -0.912453, D fake: 0.913127], [Enc/Dec loss: 0.127335, Enc/Dis: 0.136583]\n",
      "304 [D real: -0.912759, D fake: 0.912853], [Enc/Dec loss: 0.133264, Enc/Dis: 0.142522]\n",
      "305 [D real: -0.912729, D fake: 0.913342], [Enc/Dec loss: 0.136826, Enc/Dis: 0.146089]\n",
      "306 [D real: -0.913013, D fake: 0.913658], [Enc/Dec loss: 0.130312, Enc/Dis: 0.139571]\n",
      "307 [D real: -0.913451, D fake: 0.913628], [Enc/Dec loss: 0.129094, Enc/Dis: 0.138355]\n",
      "308 [D real: -0.913835, D fake: 0.913904], [Enc/Dec loss: 0.127515, Enc/Dis: 0.136777]\n",
      "309 [D real: -0.914149, D fake: 0.914443], [Enc/Dec loss: 0.124247, Enc/Dis: 0.133511]\n",
      "310 [D real: -0.914660, D fake: 0.914750], [Enc/Dec loss: 0.122564, Enc/Dis: 0.131833]\n",
      "311 [D real: -0.920811, D fake: 0.915906], [Enc/Dec loss: 0.126529, Enc/Dis: 0.135835]\n",
      "312 [D real: -0.919543, D fake: 0.918644], [Enc/Dec loss: 0.119021, Enc/Dis: 0.128321]\n",
      "313 [D real: -0.919612, D fake: 0.918520], [Enc/Dec loss: 0.110818, Enc/Dis: 0.120118]\n",
      "314 [D real: -0.919858, D fake: 0.919420], [Enc/Dec loss: 0.109397, Enc/Dis: 0.118700]\n",
      "315 [D real: -0.919416, D fake: 0.919662], [Enc/Dec loss: 0.110535, Enc/Dis: 0.119840]\n",
      "316 [D real: -0.919624, D fake: 0.920348], [Enc/Dec loss: 0.113458, Enc/Dis: 0.122773]\n",
      "317 [D real: -0.920094, D fake: 0.920234], [Enc/Dec loss: 0.113854, Enc/Dis: 0.123171]\n",
      "318 [D real: -0.920403, D fake: 0.920548], [Enc/Dec loss: 0.114871, Enc/Dis: 0.124192]\n",
      "319 [D real: -0.921007, D fake: 0.920660], [Enc/Dec loss: 0.118680, Enc/Dis: 0.128006]\n",
      "320 [D real: -0.921407, D fake: 0.921058], [Enc/Dec loss: 0.118785, Enc/Dis: 0.128116]\n",
      "321 [D real: -0.923401, D fake: 0.925650], [Enc/Dec loss: 0.119052, Enc/Dis: 0.128392]\n",
      "322 [D real: -0.924435, D fake: 0.925286], [Enc/Dec loss: 0.116054, Enc/Dis: 0.125413]\n",
      "323 [D real: -0.923636, D fake: 0.925458], [Enc/Dec loss: 0.123068, Enc/Dis: 0.132438]\n",
      "324 [D real: -0.924550, D fake: 0.925383], [Enc/Dec loss: 0.124013, Enc/Dis: 0.133390]\n",
      "325 [D real: -0.924956, D fake: 0.925243], [Enc/Dec loss: 0.124354, Enc/Dis: 0.133733]\n",
      "326 [D real: -0.925439, D fake: 0.925802], [Enc/Dec loss: 0.124048, Enc/Dis: 0.133431]\n",
      "327 [D real: -0.925877, D fake: 0.926318], [Enc/Dec loss: 0.123337, Enc/Dis: 0.132723]\n",
      "328 [D real: -0.926215, D fake: 0.926717], [Enc/Dec loss: 0.122377, Enc/Dis: 0.131767]\n",
      "329 [D real: -0.926449, D fake: 0.927003], [Enc/Dec loss: 0.125940, Enc/Dis: 0.135336]\n",
      "330 [D real: -0.926668, D fake: 0.927429], [Enc/Dec loss: 0.125337, Enc/Dis: 0.134736]\n",
      "331 [D real: -0.929046, D fake: 0.930506], [Enc/Dec loss: 0.117015, Enc/Dis: 0.126437]\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z, batch_size, train_dataset, epochs, scaler, X_train_scaled, scaled, X_train, y_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('D_Loss: ')\n",
    "fig, ax = plt.subplots(1,1, figsize=[10,5])\n",
    "ax.plot(aae.c1_hist, c='red')\n",
    "ax.plot(aae.c2_hist, c='blue')\n",
    "\n",
    "ax.legend(['D real', 'D fake'])\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"AAE D_Loss per Epoch\")\n",
    "plt.savefig('AAE/Losses/D_loss'+str(epochs)+'.png')\n",
    "\n",
    "\n",
    "print('G_Loss: ')\n",
    "fig, ax = plt.subplots(1,1, figsize=[10,5])\n",
    "ax.plot(aae.g1_hist, c='orange')\n",
    "ax.plot(aae.g2_hist, c='green')\n",
    "\n",
    "ax.legend(['g_Decoder', 'g_Discriminator'])\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"AAE G_Loss per Epoch\")\n",
    "plt.savefig('AAE/Losses/G_loss'+str(epochs)+'.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#latent_values5 = X.reshape([1000,Z])\n",
    "latent_values = tf.random.normal([1000, Z])\n",
    "predicted_values = aae.decoder(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder(aae.encoder(X_train_scaled))\n",
    "predicted_values3 = aae.encoder(X_train_scaled)\n",
    "#predicted_values4 = scaler.inverse_transform(X_train_scaled)\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values5[:,0],predicted_values5[:,1],predicted_values5[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    #ab = plt.subplot(projection='3d')\n",
    "    #ab.scatter(latent_values5[:,0],latent_values5[:,1],latent_values5[:,2])\n",
    "    #ab.set_ylabel('Y')\n",
    "    #ab.set_zlabel('Z')\n",
    "    #ab.set_xlabel('X')\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "\n",
    "    axes[0].scatter(predicted_values3[:,0],predicted_values3[:,1],c='pink')#encoder(X_train_scaled)\n",
    "    #axes[0].scatter(latent_values[:,0],latent_values[:,1],c='grey')\n",
    "    axes[0].set_ylabel('Y')\n",
    "    axes[0].set_xlabel('X')\n",
    "\n",
    "    \n",
    "    \n",
    "    axes[1].scatter(predicted_values2[:,0],predicted_values2[:,1],)#encoder/decoder\n",
    "    #axes[1].scatter(predicted_values4[:,0],predicted_values4[:,1],c='grey')#X_trained_scaled\n",
    "    axes[1].set_ylabel('Y')\n",
    "    axes[1].set_xlabel('X')\n",
    "\n",
    "    \n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,1],c='red') #decoder(latent space)\n",
    "    #axes[2].scatter(predicted_values4[:,0],predicted_values4[:,1],c='grey')#X_trained_scaled\n",
    "    axes[2].set_ylabel('Y')\n",
    "    axes[2].set_xlabel('X')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('AAE/Result/'+str(epochs)+'.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "n_points = 900\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//9, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//9, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//9, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//9, n_features), x_input[3])\n",
    "predict5 = np.full((n_points//9, n_features), x_input[4])\n",
    "predict6 = np.full((n_points//9, n_features), x_input[5])\n",
    "predict7 = np.full((n_points//9, n_features), x_input[6])\n",
    "predict8 = np.full((n_points//9, n_features), x_input[7])\n",
    "predict9 = np.full((n_points//9, n_features), x_input[8])\n",
    "\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4, predict5, predict6, predict7, predict8, predict9))\n",
    "predictthis_scaled = scaler.transform(predictthis)\n",
    "input_test = predictthis_scaled.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_generated = aae.predict(input_test, scaler)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,c='orange') \n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    #plt.scatter(predicted_values4[:,0],predicted_values4[:,1],c='grey')#X_trained_scaled\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('AAE/Prediction/'+str(epochs)+'.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
