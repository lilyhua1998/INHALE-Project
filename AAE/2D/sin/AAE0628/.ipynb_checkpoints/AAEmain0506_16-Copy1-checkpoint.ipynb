{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu (ELU)                       (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         elu[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "elu_1 (ELU)                     (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 952\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Decoder:\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,242\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Discriminator:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "elu_4 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                1680      \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "elu_8 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 5,801\n",
      "Trainable params: 5,401\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network16.build_encoder(Z, nodes, n_features)\n",
    "print(\"Encoder:\\n\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network16.build_decoder(Z,nodes, n_features)\n",
    "print(\"Decoder:\\n\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator=network16.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model16\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 100001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model16.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100001\n",
      "[C1 valid: 0.479702, C2 fake: 0.000000], [G loss: 0.525831, mse: 1.041375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05019/AAE_Model16.py:197: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_40_100001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_40_100001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_40_100001/assets\n",
      "Epoch 2/100001\n",
      "[C1 valid: 0.440483, C2 fake: 0.000000], [G loss: 0.424328, mse: 0.838910]\n",
      "Epoch 3/100001\n",
      "[C1 valid: 0.413559, C2 fake: 0.000000], [G loss: 0.328327, mse: 0.647530]\n",
      "Epoch 4/100001\n",
      "[C1 valid: 0.389861, C2 fake: 0.000000], [G loss: 0.283234, mse: 0.558118]\n",
      "Epoch 5/100001\n",
      "[C1 valid: 0.368624, C2 fake: 0.000000], [G loss: 0.233795, mse: 0.459976]\n",
      "Epoch 6/100001\n",
      "[C1 valid: 0.352528, C2 fake: 0.000000], [G loss: 0.204047, mse: 0.400801]\n",
      "Epoch 7/100001\n",
      "[C1 valid: 0.333111, C2 fake: 0.000000], [G loss: 0.184716, mse: 0.362617]\n",
      "Epoch 8/100001\n",
      "[C1 valid: 0.311326, C2 fake: 0.000000], [G loss: 0.160305, mse: 0.314480]\n",
      "Epoch 9/100001\n",
      "[C1 valid: 0.307080, C2 fake: 0.000000], [G loss: 0.157299, mse: 0.309033]\n",
      "Epoch 10/100001\n",
      "[C1 valid: 0.294494, C2 fake: 0.000000], [G loss: 0.134579, mse: 0.264096]\n",
      "Epoch 11/100001\n",
      "[C1 valid: 0.279753, C2 fake: 0.000000], [G loss: 0.125990, mse: 0.247422]\n",
      "Epoch 12/100001\n",
      "[C1 valid: 0.270926, C2 fake: 0.000000], [G loss: 0.118949, mse: 0.233694]\n",
      "Epoch 13/100001\n",
      "[C1 valid: 0.264993, C2 fake: 0.000000], [G loss: 0.121031, mse: 0.237930]\n",
      "Epoch 14/100001\n",
      "[C1 valid: 0.258326, C2 fake: 0.000000], [G loss: 0.107366, mse: 0.211151]\n",
      "Epoch 15/100001\n",
      "[C1 valid: 0.244289, C2 fake: 0.000000], [G loss: 0.105802, mse: 0.207966]\n",
      "Epoch 16/100001\n",
      "[C1 valid: 0.235925, C2 fake: 0.000000], [G loss: 0.096474, mse: 0.190191]\n",
      "Epoch 17/100001\n",
      "[C1 valid: 0.225807, C2 fake: 0.000000], [G loss: 0.093178, mse: 0.183590]\n",
      "Epoch 18/100001\n",
      "[C1 valid: 0.216115, C2 fake: 0.000000], [G loss: 0.089118, mse: 0.176021]\n",
      "Epoch 19/100001\n",
      "[C1 valid: 0.209359, C2 fake: 0.000000], [G loss: 0.086746, mse: 0.171437]\n",
      "Epoch 20/100001\n",
      "[C1 valid: 0.203465, C2 fake: 0.000000], [G loss: 0.083376, mse: 0.164894]\n",
      "Epoch 21/100001\n",
      "[C1 valid: 0.193439, C2 fake: 0.000000], [G loss: 0.081519, mse: 0.161291]\n",
      "Epoch 22/100001\n",
      "[C1 valid: 0.193287, C2 fake: 0.000000], [G loss: 0.074222, mse: 0.146947]\n",
      "Epoch 23/100001\n",
      "[C1 valid: 0.181002, C2 fake: 0.000000], [G loss: 0.074905, mse: 0.148098]\n",
      "Epoch 24/100001\n",
      "[C1 valid: 0.173863, C2 fake: 0.000000], [G loss: 0.071826, mse: 0.142296]\n",
      "Epoch 25/100001\n",
      "[C1 valid: 0.166895, C2 fake: 0.000000], [G loss: 0.071110, mse: 0.140797]\n",
      "Epoch 26/100001\n",
      "[C1 valid: 0.158693, C2 fake: 0.000000], [G loss: 0.069002, mse: 0.136724]\n",
      "Epoch 27/100001\n",
      "[C1 valid: 0.159773, C2 fake: 0.000000], [G loss: 0.066441, mse: 0.131466]\n",
      "Epoch 28/100001\n",
      "[C1 valid: 0.156170, C2 fake: 0.000000], [G loss: 0.066138, mse: 0.131061]\n",
      "Epoch 29/100001\n",
      "[C1 valid: 0.144205, C2 fake: 0.000000], [G loss: 0.061546, mse: 0.122148]\n",
      "Epoch 30/100001\n",
      "[C1 valid: 0.140995, C2 fake: 0.000000], [G loss: 0.055955, mse: 0.111069]\n",
      "Epoch 31/100001\n",
      "[C1 valid: 0.134931, C2 fake: 0.000000], [G loss: 0.061221, mse: 0.121560]\n",
      "Epoch 32/100001\n",
      "[C1 valid: 0.127550, C2 fake: 0.000000], [G loss: 0.060216, mse: 0.119454]\n",
      "Epoch 33/100001\n",
      "[C1 valid: 0.123173, C2 fake: 0.000000], [G loss: 0.057764, mse: 0.114925]\n",
      "Epoch 34/100001\n",
      "[C1 valid: 0.119983, C2 fake: 0.000000], [G loss: 0.054991, mse: 0.109217]\n",
      "Epoch 35/100001\n",
      "[C1 valid: 0.114170, C2 fake: 0.000000], [G loss: 0.051831, mse: 0.103137]\n",
      "Epoch 36/100001\n",
      "[C1 valid: 0.107135, C2 fake: 0.000000], [G loss: 0.050789, mse: 0.101163]\n",
      "Epoch 37/100001\n",
      "[C1 valid: 0.105985, C2 fake: 0.000000], [G loss: 0.049493, mse: 0.098632]\n",
      "Epoch 38/100001\n",
      "[C1 valid: 0.097693, C2 fake: 0.000000], [G loss: 0.047086, mse: 0.093778]\n",
      "Epoch 39/100001\n",
      "[C1 valid: 0.094016, C2 fake: 0.000000], [G loss: 0.045725, mse: 0.090985]\n",
      "Epoch 40/100001\n",
      "[C1 valid: 0.085241, C2 fake: 0.000000], [G loss: 0.046774, mse: 0.093126]\n",
      "Epoch 41/100001\n",
      "[C1 valid: 0.087443, C2 fake: 0.000000], [G loss: 0.045734, mse: 0.091050]\n",
      "Epoch 42/100001\n",
      "[C1 valid: 0.081124, C2 fake: 0.000000], [G loss: 0.043638, mse: 0.087005]\n",
      "Epoch 43/100001\n",
      "[C1 valid: 0.074849, C2 fake: 0.000000], [G loss: 0.041210, mse: 0.082069]\n",
      "Epoch 44/100001\n",
      "[C1 valid: 0.070674, C2 fake: 0.000000], [G loss: 0.041986, mse: 0.083701]\n",
      "Epoch 45/100001\n",
      "[C1 valid: 0.066686, C2 fake: 0.000000], [G loss: 0.039163, mse: 0.077894]\n",
      "Epoch 46/100001\n",
      "[C1 valid: 0.068172, C2 fake: 0.000000], [G loss: 0.039573, mse: 0.078876]\n",
      "Epoch 47/100001\n",
      "[C1 valid: 0.062947, C2 fake: 0.000000], [G loss: 0.037686, mse: 0.075146]\n",
      "Epoch 48/100001\n",
      "[C1 valid: 0.058862, C2 fake: 0.000000], [G loss: 0.038049, mse: 0.075772]\n",
      "Epoch 49/100001\n",
      "[C1 valid: 0.054712, C2 fake: 0.000000], [G loss: 0.036279, mse: 0.072304]\n",
      "Epoch 50/100001\n",
      "[C1 valid: 0.053256, C2 fake: 0.000000], [G loss: 0.036460, mse: 0.072681]\n",
      "Epoch 51/100001\n",
      "[C1 valid: 0.047491, C2 fake: 0.000000], [G loss: 0.034544, mse: 0.068896]\n",
      "Epoch 52/100001\n",
      "[C1 valid: 0.045264, C2 fake: 0.000000], [G loss: 0.032243, mse: 0.064295]\n",
      "Epoch 53/100001\n",
      "[C1 valid: 0.045801, C2 fake: 0.000000], [G loss: 0.034018, mse: 0.067748]\n",
      "Epoch 54/100001\n",
      "[C1 valid: 0.039104, C2 fake: 0.000000], [G loss: 0.032938, mse: 0.065657]\n",
      "Epoch 55/100001\n",
      "[C1 valid: 0.037704, C2 fake: 0.000000], [G loss: 0.031876, mse: 0.063547]\n",
      "Epoch 56/100001\n",
      "[C1 valid: 0.038087, C2 fake: 0.000000], [G loss: 0.030748, mse: 0.061386]\n",
      "Epoch 57/100001\n",
      "[C1 valid: 0.033932, C2 fake: 0.000000], [G loss: 0.030824, mse: 0.061525]\n",
      "Epoch 58/100001\n",
      "[C1 valid: 0.032321, C2 fake: 0.000000], [G loss: 0.029908, mse: 0.059605]\n",
      "Epoch 59/100001\n",
      "[C1 valid: 0.028660, C2 fake: 0.000000], [G loss: 0.027981, mse: 0.055861]\n",
      "Epoch 60/100001\n",
      "[C1 valid: 0.025135, C2 fake: 0.000000], [G loss: 0.026944, mse: 0.053758]\n",
      "Epoch 61/100001\n",
      "[C1 valid: 0.030526, C2 fake: 0.000000], [G loss: 0.026796, mse: 0.053363]\n",
      "Epoch 62/100001\n",
      "[C1 valid: 0.024833, C2 fake: 0.000000], [G loss: 0.026683, mse: 0.053289]\n",
      "Epoch 63/100001\n",
      "[C1 valid: 0.026560, C2 fake: 0.000000], [G loss: 0.026751, mse: 0.053301]\n",
      "Epoch 64/100001\n",
      "[C1 valid: 0.026054, C2 fake: 0.000000], [G loss: 0.025553, mse: 0.050999]\n",
      "Epoch 65/100001\n",
      "[C1 valid: 0.019863, C2 fake: 0.000000], [G loss: 0.024842, mse: 0.049525]\n",
      "Epoch 66/100001\n",
      "[C1 valid: 0.020686, C2 fake: 0.000000], [G loss: 0.024600, mse: 0.049156]\n",
      "Epoch 67/100001\n",
      "[C1 valid: 0.018243, C2 fake: 0.000000], [G loss: 0.024115, mse: 0.048128]\n",
      "Epoch 68/100001\n",
      "[C1 valid: 0.020100, C2 fake: 0.000000], [G loss: 0.023437, mse: 0.046803]\n",
      "Epoch 69/100001\n",
      "[C1 valid: 0.016260, C2 fake: 0.000000], [G loss: 0.023248, mse: 0.046409]\n",
      "Epoch 70/100001\n",
      "[C1 valid: 0.014712, C2 fake: 0.000000], [G loss: 0.022092, mse: 0.044081]\n",
      "Epoch 71/100001\n",
      "[C1 valid: 0.012351, C2 fake: 0.000000], [G loss: 0.021938, mse: 0.043793]\n",
      "Epoch 72/100001\n",
      "[C1 valid: 0.012012, C2 fake: 0.000000], [G loss: 0.022232, mse: 0.044348]\n",
      "Epoch 73/100001\n",
      "[C1 valid: 0.010684, C2 fake: 0.000000], [G loss: 0.021171, mse: 0.042284]\n",
      "Epoch 74/100001\n",
      "[C1 valid: 0.010379, C2 fake: 0.000000], [G loss: 0.019090, mse: 0.038079]\n",
      "Epoch 75/100001\n",
      "[C1 valid: 0.012234, C2 fake: 0.000000], [G loss: 0.019060, mse: 0.037940]\n",
      "Epoch 76/100001\n",
      "[C1 valid: 0.010261, C2 fake: 0.000000], [G loss: 0.020259, mse: 0.040417]\n",
      "Epoch 77/100001\n",
      "[C1 valid: 0.008222, C2 fake: 0.000000], [G loss: 0.018382, mse: 0.036592]\n",
      "Epoch 78/100001\n",
      "[C1 valid: 0.009116, C2 fake: 0.000000], [G loss: 0.018739, mse: 0.037430]\n",
      "Epoch 79/100001\n",
      "[C1 valid: 0.007790, C2 fake: 0.000000], [G loss: 0.018438, mse: 0.036826]\n",
      "Epoch 80/100001\n",
      "[C1 valid: 0.006307, C2 fake: 0.000000], [G loss: 0.017081, mse: 0.034128]\n",
      "Epoch 81/100001\n",
      "[C1 valid: 0.010247, C2 fake: 0.000000], [G loss: 0.016487, mse: 0.032897]\n",
      "Epoch 82/100001\n",
      "[C1 valid: 0.008520, C2 fake: 0.000000], [G loss: 0.016254, mse: 0.032484]\n",
      "Epoch 83/100001\n",
      "[C1 valid: 0.006335, C2 fake: 0.000000], [G loss: 0.016093, mse: 0.032159]\n",
      "Epoch 84/100001\n",
      "[C1 valid: 0.008034, C2 fake: 0.000000], [G loss: 0.016011, mse: 0.031961]\n",
      "Epoch 85/100001\n",
      "[C1 valid: 0.006513, C2 fake: 0.000000], [G loss: 0.015982, mse: 0.031938]\n",
      "Epoch 86/100001\n",
      "[C1 valid: 0.004780, C2 fake: 0.000000], [G loss: 0.015242, mse: 0.030454]\n",
      "Epoch 87/100001\n",
      "[C1 valid: 0.015381, C2 fake: 0.000000], [G loss: 0.015671, mse: 0.031250]\n",
      "Epoch 88/100001\n",
      "[C1 valid: 0.004991, C2 fake: 0.000000], [G loss: 0.015345, mse: 0.030611]\n",
      "Epoch 89/100001\n",
      "[C1 valid: 0.003685, C2 fake: 0.000000], [G loss: 0.014672, mse: 0.029311]\n",
      "Epoch 90/100001\n",
      "[C1 valid: 0.003947, C2 fake: 0.000000], [G loss: 0.013738, mse: 0.027397]\n",
      "Epoch 91/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: 0.004542, C2 fake: 0.000000], [G loss: 0.013962, mse: 0.027895]\n",
      "Epoch 92/100001\n",
      "[C1 valid: 0.004989, C2 fake: 0.000000], [G loss: 0.013938, mse: 0.027846]\n",
      "Epoch 93/100001\n",
      "[C1 valid: 0.004472, C2 fake: 0.000000], [G loss: 0.013350, mse: 0.026658]\n",
      "Epoch 94/100001\n",
      "[C1 valid: 0.004537, C2 fake: 0.000000], [G loss: 0.012606, mse: 0.025191]\n",
      "Epoch 95/100001\n",
      "[C1 valid: 0.003501, C2 fake: 0.000000], [G loss: 0.013622, mse: 0.027186]\n",
      "Epoch 96/100001\n",
      "[C1 valid: 0.002054, C2 fake: 0.000000], [G loss: 0.013098, mse: 0.026137]\n",
      "Epoch 97/100001\n",
      "[C1 valid: 0.004083, C2 fake: 0.000000], [G loss: 0.012737, mse: 0.025449]\n",
      "Epoch 98/100001\n",
      "[C1 valid: 0.002262, C2 fake: 0.000000], [G loss: 0.012644, mse: 0.025266]\n",
      "Epoch 99/100001\n",
      "[C1 valid: 0.002946, C2 fake: 0.000000], [G loss: 0.011535, mse: 0.023047]\n",
      "Epoch 100/100001\n",
      "[C1 valid: 0.002312, C2 fake: 0.000000], [G loss: 0.012431, mse: 0.024843]\n",
      "Epoch 101/100001\n",
      "[C1 valid: 0.002533, C2 fake: 0.000000], [G loss: 0.012257, mse: 0.024496]\n",
      "Epoch 102/100001\n",
      "[C1 valid: 0.002330, C2 fake: 0.000000], [G loss: 0.011482, mse: 0.022948]\n",
      "Epoch 103/100001\n",
      "[C1 valid: 0.001768, C2 fake: 0.000000], [G loss: 0.010729, mse: 0.021429]\n",
      "Epoch 104/100001\n",
      "[C1 valid: 0.000880, C2 fake: 0.000000], [G loss: 0.010567, mse: 0.021111]\n",
      "Epoch 105/100001\n",
      "[C1 valid: 0.000785, C2 fake: 0.000000], [G loss: 0.010566, mse: 0.021112]\n",
      "Epoch 106/100001\n",
      "[C1 valid: 0.001073, C2 fake: 0.000000], [G loss: 0.011018, mse: 0.022019]\n",
      "Epoch 107/100001\n",
      "[C1 valid: 0.000713, C2 fake: 0.000000], [G loss: 0.011683, mse: 0.023351]\n",
      "Epoch 108/100001\n",
      "[C1 valid: 0.000652, C2 fake: 0.000000], [G loss: 0.010372, mse: 0.020731]\n",
      "Epoch 109/100001\n",
      "[C1 valid: 0.000623, C2 fake: 0.000000], [G loss: 0.010510, mse: 0.021011]\n",
      "Epoch 110/100001\n",
      "[C1 valid: 0.000874, C2 fake: 0.000000], [G loss: 0.009559, mse: 0.019101]\n",
      "Epoch 111/100001\n",
      "[C1 valid: 0.000261, C2 fake: 0.000000], [G loss: 0.010059, mse: 0.020105]\n",
      "Epoch 112/100001\n",
      "[C1 valid: 0.000227, C2 fake: 0.000000], [G loss: 0.010184, mse: 0.020357]\n",
      "Epoch 113/100001\n",
      "[C1 valid: 0.000196, C2 fake: 0.000000], [G loss: 0.009404, mse: 0.018798]\n",
      "Epoch 114/100001\n",
      "[C1 valid: 0.000504, C2 fake: 0.000000], [G loss: 0.009931, mse: 0.019855]\n",
      "Epoch 115/100001\n",
      "[C1 valid: 0.000484, C2 fake: 0.000000], [G loss: 0.010253, mse: 0.020501]\n",
      "Epoch 116/100001\n",
      "[C1 valid: 0.000147, C2 fake: 0.000000], [G loss: 0.009841, mse: 0.019676]\n",
      "Epoch 117/100001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder.predict(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "n_points = 900\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//9, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//9, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//9, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//9, n_features), x_input[3])\n",
    "predict5 = np.full((n_points//9, n_features), x_input[4])\n",
    "predict6 = np.full((n_points//9, n_features), x_input[5])\n",
    "predict7 = np.full((n_points//9, n_features), x_input[6])\n",
    "predict8 = np.full((n_points//9, n_features), x_input[7])\n",
    "predict9 = np.full((n_points//9, n_features), x_input[8])\n",
    "\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4, predict5, predict6, predict7, predict8, predict9))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder.predict(aae.encoder(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
