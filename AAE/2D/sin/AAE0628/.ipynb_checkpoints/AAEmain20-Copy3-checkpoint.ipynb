{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu (ELU)                       (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         elu[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "elu_1 (ELU)                     (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 952\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Decoder:\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,242\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Discriminator:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "elu_4 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                1680      \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "elu_8 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 5,801\n",
      "Trainable params: 5,401\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network20.build_encoder(Z, nodes, n_features)\n",
    "print(\"Encoder:\\n\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network20.build_decoder(Z,nodes, n_features)\n",
    "print(\"Decoder:\\n\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator=network20.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model20\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 2001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model20.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "[C1 valid: -0.499314, C2 fake: 0.499973], [G loss: 0.377129, mse: 0.763701]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05012/AAE_Model20.py:190: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_40_2001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_40_2001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_40_2001/assets\n",
      "Epoch 2/2001\n",
      "[C1 valid: -0.499668, C2 fake: 0.501426], [G loss: 0.311378, mse: 0.632546]\n",
      "Epoch 3/2001\n",
      "[C1 valid: -0.499855, C2 fake: 0.501739], [G loss: 0.254774, mse: 0.519206]\n",
      "Epoch 4/2001\n",
      "[C1 valid: -0.501068, C2 fake: 0.498937], [G loss: 0.197729, mse: 0.405000]\n",
      "Epoch 5/2001\n",
      "[C1 valid: -0.500000, C2 fake: 0.500925], [G loss: 0.153689, mse: 0.317033]\n",
      "Epoch 6/2001\n",
      "[C1 valid: -0.500080, C2 fake: 0.498926], [G loss: 0.126485, mse: 0.262633]\n",
      "Epoch 7/2001\n",
      "[C1 valid: -0.499867, C2 fake: 0.498656], [G loss: 0.111585, mse: 0.233479]\n",
      "Epoch 8/2001\n",
      "[C1 valid: -0.499742, C2 fake: 0.495296], [G loss: 0.090512, mse: 0.191374]\n",
      "Epoch 9/2001\n",
      "[C1 valid: -0.500545, C2 fake: 0.492496], [G loss: 0.079332, mse: 0.168932]\n",
      "Epoch 10/2001\n",
      "[C1 valid: -0.500173, C2 fake: 0.488656], [G loss: 0.071879, mse: 0.153716]\n",
      "Epoch 11/2001\n",
      "[C1 valid: -0.494755, C2 fake: 0.489629], [G loss: 0.066042, mse: 0.142219]\n",
      "Epoch 12/2001\n",
      "[C1 valid: -0.496622, C2 fake: 0.485064], [G loss: 0.058597, mse: 0.127206]\n",
      "Epoch 13/2001\n",
      "[C1 valid: -0.498790, C2 fake: 0.480604], [G loss: 0.056848, mse: 0.123590]\n",
      "Epoch 14/2001\n",
      "[C1 valid: -0.500258, C2 fake: 0.475404], [G loss: 0.051427, mse: 0.112530]\n",
      "Epoch 15/2001\n",
      "[C1 valid: -0.498979, C2 fake: 0.473106], [G loss: 0.046902, mse: 0.103575]\n",
      "Epoch 16/2001\n",
      "[C1 valid: -0.495743, C2 fake: 0.471407], [G loss: 0.044937, mse: 0.099541]\n",
      "Epoch 17/2001\n",
      "[C1 valid: -0.499465, C2 fake: 0.467977], [G loss: 0.040706, mse: 0.091312]\n",
      "Epoch 18/2001\n",
      "[C1 valid: -0.501868, C2 fake: 0.467904], [G loss: 0.035245, mse: 0.080010]\n",
      "Epoch 19/2001\n",
      "[C1 valid: -0.496871, C2 fake: 0.464640], [G loss: 0.030506, mse: 0.070462]\n",
      "Epoch 20/2001\n",
      "[C1 valid: -0.500706, C2 fake: 0.463269], [G loss: 0.030951, mse: 0.071343]\n",
      "Epoch 21/2001\n",
      "[C1 valid: -0.497292, C2 fake: 0.456943], [G loss: 0.027808, mse: 0.064790]\n",
      "Epoch 22/2001\n",
      "[C1 valid: -0.496406, C2 fake: 0.458725], [G loss: 0.025197, mse: 0.059256]\n",
      "Epoch 23/2001\n",
      "[C1 valid: -0.497351, C2 fake: 0.457329], [G loss: 0.023664, mse: 0.056674]\n",
      "Epoch 24/2001\n",
      "[C1 valid: -0.494493, C2 fake: 0.446785], [G loss: 0.023905, mse: 0.056963]\n",
      "Epoch 25/2001\n",
      "[C1 valid: -0.494612, C2 fake: 0.449126], [G loss: 0.021720, mse: 0.052786]\n",
      "Epoch 26/2001\n",
      "[C1 valid: -0.494886, C2 fake: 0.438213], [G loss: 0.019066, mse: 0.047522]\n",
      "Epoch 27/2001\n",
      "[C1 valid: -0.491664, C2 fake: 0.438510], [G loss: 0.019949, mse: 0.049021]\n",
      "Epoch 28/2001\n",
      "[C1 valid: -0.493052, C2 fake: 0.439225], [G loss: 0.018488, mse: 0.046112]\n",
      "Epoch 29/2001\n",
      "[C1 valid: -0.493665, C2 fake: 0.436824], [G loss: 0.018586, mse: 0.046199]\n",
      "Epoch 30/2001\n",
      "[C1 valid: -0.500200, C2 fake: 0.426734], [G loss: 0.017423, mse: 0.044076]\n",
      "Epoch 31/2001\n",
      "[C1 valid: -0.487523, C2 fake: 0.420340], [G loss: 0.016602, mse: 0.042488]\n",
      "Epoch 32/2001\n",
      "[C1 valid: -0.490418, C2 fake: 0.424912], [G loss: 0.016361, mse: 0.041518]\n",
      "Epoch 33/2001\n",
      "[C1 valid: -0.497322, C2 fake: 0.422940], [G loss: 0.015277, mse: 0.039570]\n",
      "Epoch 34/2001\n",
      "[C1 valid: -0.491118, C2 fake: 0.415089], [G loss: 0.016155, mse: 0.041256]\n",
      "Epoch 35/2001\n",
      "[C1 valid: -0.503725, C2 fake: 0.412515], [G loss: 0.016277, mse: 0.041546]\n",
      "Epoch 36/2001\n",
      "[C1 valid: -0.496083, C2 fake: 0.414166], [G loss: 0.015791, mse: 0.040449]\n",
      "Epoch 37/2001\n",
      "[C1 valid: -0.500074, C2 fake: 0.412315], [G loss: 0.015352, mse: 0.039334]\n",
      "Epoch 38/2001\n",
      "[C1 valid: -0.496837, C2 fake: 0.406961], [G loss: 0.014583, mse: 0.038571]\n",
      "Epoch 39/2001\n",
      "[C1 valid: -0.504296, C2 fake: 0.405210], [G loss: 0.014617, mse: 0.037995]\n",
      "Epoch 40/2001\n",
      "[C1 valid: -0.514868, C2 fake: 0.411445], [G loss: 0.013669, mse: 0.036058]\n",
      "Epoch 41/2001\n",
      "[C1 valid: -0.515945, C2 fake: 0.402152], [G loss: 0.014188, mse: 0.037123]\n",
      "Epoch 42/2001\n",
      "[C1 valid: -0.521089, C2 fake: 0.407993], [G loss: 0.013001, mse: 0.034861]\n",
      "Epoch 43/2001\n",
      "[C1 valid: -0.522431, C2 fake: 0.401413], [G loss: 0.013226, mse: 0.034943]\n",
      "Epoch 44/2001\n",
      "[C1 valid: -0.523972, C2 fake: 0.404767], [G loss: 0.013468, mse: 0.035781]\n",
      "Epoch 45/2001\n",
      "[C1 valid: -0.526652, C2 fake: 0.406069], [G loss: 0.012805, mse: 0.034814]\n",
      "Epoch 46/2001\n",
      "[C1 valid: -0.532790, C2 fake: 0.403331], [G loss: 0.013756, mse: 0.036642]\n",
      "Epoch 47/2001\n",
      "[C1 valid: -0.542692, C2 fake: 0.400823], [G loss: 0.012124, mse: 0.033680]\n",
      "Epoch 48/2001\n",
      "[C1 valid: -0.544085, C2 fake: 0.407011], [G loss: 0.012536, mse: 0.033975]\n",
      "Epoch 49/2001\n",
      "[C1 valid: -0.544338, C2 fake: 0.415129], [G loss: 0.012094, mse: 0.033657]\n",
      "Epoch 50/2001\n",
      "[C1 valid: -0.566719, C2 fake: 0.419712], [G loss: 0.012978, mse: 0.035605]\n",
      "Epoch 51/2001\n",
      "[C1 valid: -0.588979, C2 fake: 0.425552], [G loss: 0.011550, mse: 0.033024]\n",
      "Epoch 52/2001\n",
      "[C1 valid: -0.598873, C2 fake: 0.422961], [G loss: 0.010100, mse: 0.031542]\n",
      "Epoch 53/2001\n",
      "[C1 valid: -0.629471, C2 fake: 0.426670], [G loss: 0.010848, mse: 0.033892]\n",
      "Epoch 54/2001\n",
      "[C1 valid: -0.640132, C2 fake: 0.424584], [G loss: 0.009852, mse: 0.034146]\n",
      "Epoch 55/2001\n",
      "[C1 valid: -0.655938, C2 fake: 0.409231], [G loss: 0.008703, mse: 0.032768]\n",
      "Epoch 56/2001\n",
      "[C1 valid: -0.664437, C2 fake: 0.387135], [G loss: 0.008598, mse: 0.033753]\n",
      "Epoch 57/2001\n",
      "[C1 valid: -0.678020, C2 fake: 0.381340], [G loss: 0.007499, mse: 0.032078]\n",
      "Epoch 58/2001\n",
      "[C1 valid: -0.683785, C2 fake: 0.380262], [G loss: 0.006986, mse: 0.031269]\n",
      "Epoch 59/2001\n",
      "[C1 valid: -0.683790, C2 fake: 0.369452], [G loss: 0.006987, mse: 0.031464]\n",
      "Epoch 60/2001\n",
      "[C1 valid: -0.690209, C2 fake: 0.356250], [G loss: 0.007176, mse: 0.031792]\n",
      "Epoch 61/2001\n",
      "[C1 valid: -0.694311, C2 fake: 0.347195], [G loss: 0.008009, mse: 0.033576]\n",
      "Epoch 62/2001\n",
      "[C1 valid: -0.697071, C2 fake: 0.356008], [G loss: 0.006275, mse: 0.029892]\n",
      "Epoch 63/2001\n",
      "[C1 valid: -0.709963, C2 fake: 0.341075], [G loss: 0.007056, mse: 0.031398]\n",
      "Epoch 64/2001\n",
      "[C1 valid: -0.714782, C2 fake: 0.336145], [G loss: 0.006435, mse: 0.029938]\n",
      "Epoch 65/2001\n",
      "[C1 valid: -0.721263, C2 fake: 0.332847], [G loss: 0.007080, mse: 0.030791]\n",
      "Epoch 66/2001\n",
      "[C1 valid: -0.723024, C2 fake: 0.333535], [G loss: 0.006239, mse: 0.029047]\n",
      "Epoch 67/2001\n",
      "[C1 valid: -0.728527, C2 fake: 0.329506], [G loss: 0.007166, mse: 0.031200]\n",
      "Epoch 68/2001\n",
      "[C1 valid: -0.732571, C2 fake: 0.330599], [G loss: 0.006807, mse: 0.030077]\n",
      "Epoch 69/2001\n",
      "[C1 valid: -0.740456, C2 fake: 0.329592], [G loss: 0.007353, mse: 0.030115]\n",
      "Epoch 70/2001\n",
      "[C1 valid: -0.748784, C2 fake: 0.324759], [G loss: 0.006492, mse: 0.029572]\n",
      "Epoch 71/2001\n",
      "[C1 valid: -0.751161, C2 fake: 0.315604], [G loss: 0.007423, mse: 0.029560]\n",
      "Epoch 72/2001\n",
      "[C1 valid: -0.760838, C2 fake: 0.304656], [G loss: 0.006877, mse: 0.029384]\n",
      "Epoch 73/2001\n",
      "[C1 valid: -0.763648, C2 fake: 0.301731], [G loss: 0.006412, mse: 0.028301]\n",
      "Epoch 74/2001\n",
      "[C1 valid: -0.766832, C2 fake: 0.306686], [G loss: 0.009250, mse: 0.030189]\n",
      "Epoch 75/2001\n",
      "[C1 valid: -0.770445, C2 fake: 0.308449], [G loss: 0.007403, mse: 0.028138]\n",
      "Epoch 76/2001\n",
      "[C1 valid: -0.776846, C2 fake: 0.297742], [G loss: 0.007413, mse: 0.027854]\n",
      "Epoch 77/2001\n",
      "[C1 valid: -0.779613, C2 fake: 0.288310], [G loss: 0.008471, mse: 0.028186]\n",
      "Epoch 78/2001\n",
      "[C1 valid: -0.781952, C2 fake: 0.285853], [G loss: 0.007880, mse: 0.027848]\n",
      "Epoch 79/2001\n",
      "[C1 valid: -0.791219, C2 fake: 0.286561], [G loss: 0.006802, mse: 0.027413]\n",
      "Epoch 80/2001\n",
      "[C1 valid: -0.794838, C2 fake: 0.286681], [G loss: 0.007482, mse: 0.028780]\n",
      "Epoch 81/2001\n",
      "[C1 valid: -0.799133, C2 fake: 0.283295], [G loss: 0.010224, mse: 0.028263]\n",
      "Epoch 82/2001\n",
      "[C1 valid: -0.774223, C2 fake: 0.308391], [G loss: 0.006829, mse: 0.027722]\n",
      "Epoch 83/2001\n",
      "[C1 valid: -0.805227, C2 fake: 0.284308], [G loss: 0.007711, mse: 0.028649]\n",
      "Epoch 84/2001\n",
      "[C1 valid: -0.813746, C2 fake: 0.269285], [G loss: 0.006843, mse: 0.027331]\n",
      "Epoch 85/2001\n",
      "[C1 valid: -0.816575, C2 fake: 0.265632], [G loss: 0.007539, mse: 0.027180]\n",
      "Epoch 86/2001\n",
      "[C1 valid: -0.822698, C2 fake: 0.263845], [G loss: 0.007403, mse: 0.027574]\n",
      "Epoch 87/2001\n",
      "[C1 valid: -0.809457, C2 fake: 0.274878], [G loss: 0.005374, mse: 0.026345]\n",
      "Epoch 88/2001\n",
      "[C1 valid: -0.796608, C2 fake: 0.267033], [G loss: 0.006069, mse: 0.026565]\n",
      "Epoch 89/2001\n",
      "[C1 valid: -0.830488, C2 fake: 0.254690], [G loss: 0.007917, mse: 0.027667]\n",
      "Epoch 90/2001\n",
      "[C1 valid: -0.836268, C2 fake: 0.249544], [G loss: 0.006630, mse: 0.026530]\n",
      "Epoch 91/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.840581, C2 fake: 0.249480], [G loss: 0.009290, mse: 0.026886]\n",
      "Epoch 92/2001\n",
      "[C1 valid: -0.823290, C2 fake: 0.277871], [G loss: 0.005160, mse: 0.026800]\n",
      "Epoch 93/2001\n",
      "[C1 valid: -0.830477, C2 fake: 0.250112], [G loss: 0.006617, mse: 0.025597]\n",
      "Epoch 94/2001\n",
      "[C1 valid: -0.848369, C2 fake: 0.235311], [G loss: 0.006961, mse: 0.026065]\n",
      "Epoch 95/2001\n",
      "[C1 valid: -0.844324, C2 fake: 0.232978], [G loss: 0.006279, mse: 0.026619]\n",
      "Epoch 96/2001\n",
      "[C1 valid: -0.852709, C2 fake: 0.227329], [G loss: 0.006633, mse: 0.025320]\n",
      "Epoch 97/2001\n",
      "[C1 valid: -0.856593, C2 fake: 0.235792], [G loss: 0.009025, mse: 0.026762]\n",
      "Epoch 98/2001\n",
      "[C1 valid: -0.843760, C2 fake: 0.240361], [G loss: 0.009119, mse: 0.026100]\n",
      "Epoch 99/2001\n",
      "[C1 valid: -0.792418, C2 fake: 0.259627], [G loss: 0.005649, mse: 0.026917]\n",
      "Epoch 100/2001\n",
      "[C1 valid: -0.854511, C2 fake: 0.225725], [G loss: 0.007990, mse: 0.025801]\n",
      "Epoch 101/2001\n",
      "[C1 valid: -0.864148, C2 fake: 0.218174], [G loss: 0.007419, mse: 0.024957]\n",
      "Epoch 102/2001\n",
      "[C1 valid: -0.861471, C2 fake: 0.215199], [G loss: 0.004062, mse: 0.023779]\n",
      "Epoch 103/2001\n",
      "[C1 valid: -0.848026, C2 fake: 0.223424], [G loss: 0.005111, mse: 0.025309]\n",
      "Epoch 104/2001\n",
      "[C1 valid: -0.853273, C2 fake: 0.218252], [G loss: 0.004256, mse: 0.023153]\n",
      "Epoch 105/2001\n",
      "[C1 valid: -0.868708, C2 fake: 0.200422], [G loss: 0.008312, mse: 0.025699]\n",
      "Epoch 106/2001\n",
      "[C1 valid: -0.866359, C2 fake: 0.212294], [G loss: 0.006914, mse: 0.023758]\n",
      "Epoch 107/2001\n",
      "[C1 valid: -0.875278, C2 fake: 0.197860], [G loss: 0.005112, mse: 0.024421]\n",
      "Epoch 108/2001\n",
      "[C1 valid: -0.878529, C2 fake: 0.194745], [G loss: 0.008044, mse: 0.024108]\n",
      "Epoch 109/2001\n",
      "[C1 valid: -0.837642, C2 fake: 0.221364], [G loss: 0.005785, mse: 0.023944]\n",
      "Epoch 110/2001\n",
      "[C1 valid: -0.871968, C2 fake: 0.203883], [G loss: 0.005773, mse: 0.024173]\n",
      "Epoch 111/2001\n",
      "[C1 valid: -0.867779, C2 fake: 0.195705], [G loss: 0.005780, mse: 0.023624]\n",
      "Epoch 112/2001\n",
      "[C1 valid: -0.876858, C2 fake: 0.195618], [G loss: 0.007828, mse: 0.025133]\n",
      "Epoch 113/2001\n",
      "[C1 valid: -0.877436, C2 fake: 0.195407], [G loss: 0.005925, mse: 0.023202]\n",
      "Epoch 114/2001\n",
      "[C1 valid: -0.883201, C2 fake: 0.185261], [G loss: 0.004795, mse: 0.023548]\n",
      "Epoch 115/2001\n",
      "[C1 valid: -0.884063, C2 fake: 0.182006], [G loss: 0.006854, mse: 0.025642]\n",
      "Epoch 116/2001\n",
      "[C1 valid: -0.890817, C2 fake: 0.176653], [G loss: 0.006935, mse: 0.024317]\n",
      "Epoch 117/2001\n",
      "[C1 valid: -0.890231, C2 fake: 0.173231], [G loss: 0.007054, mse: 0.022525]\n",
      "Epoch 118/2001\n",
      "[C1 valid: -0.864801, C2 fake: 0.192106], [G loss: 0.006943, mse: 0.022120]\n",
      "Epoch 119/2001\n",
      "[C1 valid: -0.846434, C2 fake: 0.215633], [G loss: 0.006217, mse: 0.023315]\n",
      "Epoch 120/2001\n",
      "[C1 valid: -0.890024, C2 fake: 0.177154], [G loss: 0.007268, mse: 0.023903]\n",
      "Epoch 121/2001\n",
      "[C1 valid: -0.886059, C2 fake: 0.170978], [G loss: 0.005353, mse: 0.023066]\n",
      "Epoch 122/2001\n",
      "[C1 valid: -0.890981, C2 fake: 0.173270], [G loss: 0.006713, mse: 0.021171]\n",
      "Epoch 123/2001\n",
      "[C1 valid: -0.890959, C2 fake: 0.167464], [G loss: 0.007157, mse: 0.021378]\n",
      "Epoch 124/2001\n",
      "[C1 valid: -0.889211, C2 fake: 0.171069], [G loss: 0.006922, mse: 0.022263]\n",
      "Epoch 125/2001\n",
      "[C1 valid: -0.884969, C2 fake: 0.168240], [G loss: 0.005914, mse: 0.022835]\n",
      "Epoch 126/2001\n",
      "[C1 valid: -0.807442, C2 fake: 0.252024], [G loss: 0.005349, mse: 0.021317]\n",
      "Epoch 127/2001\n",
      "[C1 valid: -0.856509, C2 fake: 0.209122], [G loss: 0.004993, mse: 0.021729]\n",
      "Epoch 128/2001\n",
      "[C1 valid: -0.881093, C2 fake: 0.162431], [G loss: 0.005728, mse: 0.021135]\n",
      "Epoch 129/2001\n",
      "[C1 valid: -0.893547, C2 fake: 0.167527], [G loss: 0.005495, mse: 0.020558]\n",
      "Epoch 130/2001\n",
      "[C1 valid: -0.896226, C2 fake: 0.163847], [G loss: 0.006423, mse: 0.020716]\n",
      "Epoch 131/2001\n",
      "[C1 valid: -0.896081, C2 fake: 0.157873], [G loss: 0.005020, mse: 0.022307]\n",
      "Epoch 132/2001\n",
      "[C1 valid: -0.895329, C2 fake: 0.158083], [G loss: 0.006801, mse: 0.021482]\n",
      "Epoch 133/2001\n",
      "[C1 valid: -0.901414, C2 fake: 0.148312], [G loss: 0.005632, mse: 0.020564]\n",
      "Epoch 134/2001\n",
      "[C1 valid: -0.898615, C2 fake: 0.146902], [G loss: 0.004037, mse: 0.021587]\n",
      "Epoch 135/2001\n",
      "[C1 valid: -0.893234, C2 fake: 0.153609], [G loss: 0.006149, mse: 0.021352]\n",
      "Epoch 136/2001\n",
      "[C1 valid: -0.902305, C2 fake: 0.151556], [G loss: 0.006644, mse: 0.020940]\n",
      "Epoch 137/2001\n",
      "[C1 valid: -0.890647, C2 fake: 0.169418], [G loss: 0.005785, mse: 0.019896]\n",
      "Epoch 138/2001\n",
      "[C1 valid: -0.838189, C2 fake: 0.182964], [G loss: 0.002544, mse: 0.020203]\n",
      "Epoch 139/2001\n",
      "[C1 valid: -0.893728, C2 fake: 0.155669], [G loss: 0.006036, mse: 0.021457]\n",
      "Epoch 140/2001\n",
      "[C1 valid: -0.906012, C2 fake: 0.147941], [G loss: 0.003657, mse: 0.019928]\n",
      "Epoch 141/2001\n",
      "[C1 valid: -0.905563, C2 fake: 0.148770], [G loss: 0.002033, mse: 0.019084]\n",
      "Epoch 142/2001\n",
      "[C1 valid: -0.895260, C2 fake: 0.144421], [G loss: 0.002361, mse: 0.019455]\n",
      "Epoch 143/2001\n",
      "[C1 valid: -0.863713, C2 fake: 0.177416], [G loss: 0.002414, mse: 0.019690]\n",
      "Epoch 144/2001\n",
      "[C1 valid: -0.875602, C2 fake: 0.147200], [G loss: 0.005250, mse: 0.020806]\n",
      "Epoch 145/2001\n",
      "[C1 valid: -0.908572, C2 fake: 0.142531], [G loss: 0.003545, mse: 0.019075]\n",
      "Epoch 146/2001\n",
      "[C1 valid: -0.902170, C2 fake: 0.149686], [G loss: 0.004670, mse: 0.019664]\n",
      "Epoch 147/2001\n",
      "[C1 valid: -0.908409, C2 fake: 0.140638], [G loss: 0.002592, mse: 0.018753]\n",
      "Epoch 148/2001\n",
      "[C1 valid: -0.906741, C2 fake: 0.139709], [G loss: 0.004823, mse: 0.018156]\n",
      "Epoch 149/2001\n",
      "[C1 valid: -0.911742, C2 fake: 0.131131], [G loss: 0.002299, mse: 0.019005]\n",
      "Epoch 150/2001\n",
      "[C1 valid: -0.911721, C2 fake: 0.134542], [G loss: 0.004898, mse: 0.018616]\n",
      "Epoch 151/2001\n",
      "[C1 valid: -0.917310, C2 fake: 0.125834], [G loss: 0.001610, mse: 0.018301]\n",
      "Epoch 152/2001\n",
      "[C1 valid: -0.908599, C2 fake: 0.124055], [G loss: 0.003587, mse: 0.017542]\n",
      "Epoch 153/2001\n",
      "[C1 valid: -0.916919, C2 fake: 0.130674], [G loss: 0.001373, mse: 0.018080]\n",
      "Epoch 154/2001\n",
      "[C1 valid: -0.815419, C2 fake: 0.222202], [G loss: 0.002173, mse: 0.017599]\n",
      "Epoch 155/2001\n",
      "[C1 valid: -0.913599, C2 fake: 0.138854], [G loss: 0.003806, mse: 0.017828]\n",
      "Epoch 156/2001\n",
      "[C1 valid: -0.910097, C2 fake: 0.132730], [G loss: 0.004219, mse: 0.018263]\n",
      "Epoch 157/2001\n",
      "[C1 valid: -0.915306, C2 fake: 0.138643], [G loss: 0.004254, mse: 0.018420]\n",
      "Epoch 158/2001\n",
      "[C1 valid: -0.921032, C2 fake: 0.123728], [G loss: 0.002441, mse: 0.017890]\n",
      "Epoch 159/2001\n",
      "[C1 valid: -0.917120, C2 fake: 0.129690], [G loss: 0.005032, mse: 0.018391]\n",
      "Epoch 160/2001\n",
      "[C1 valid: -0.890088, C2 fake: 0.132120], [G loss: 0.000827, mse: 0.017092]\n",
      "Epoch 161/2001\n",
      "[C1 valid: -0.872893, C2 fake: 0.166016], [G loss: 0.001510, mse: 0.017585]\n",
      "Epoch 162/2001\n",
      "[C1 valid: -0.882992, C2 fake: 0.127035], [G loss: 0.003344, mse: 0.018405]\n",
      "Epoch 163/2001\n",
      "[C1 valid: -0.912510, C2 fake: 0.122091], [G loss: 0.003009, mse: 0.017079]\n",
      "Epoch 164/2001\n",
      "[C1 valid: -0.917860, C2 fake: 0.117876], [G loss: 0.003600, mse: 0.017507]\n",
      "Epoch 165/2001\n",
      "[C1 valid: -0.920602, C2 fake: 0.119113], [G loss: 0.002142, mse: 0.017202]\n",
      "Epoch 166/2001\n",
      "[C1 valid: -0.920351, C2 fake: 0.114119], [G loss: 0.003011, mse: 0.017415]\n",
      "Epoch 167/2001\n",
      "[C1 valid: -0.922921, C2 fake: 0.112198], [G loss: 0.003331, mse: 0.016502]\n",
      "Epoch 168/2001\n",
      "[C1 valid: -0.924088, C2 fake: 0.119848], [G loss: 0.004217, mse: 0.017046]\n",
      "Epoch 169/2001\n",
      "[C1 valid: -0.910586, C2 fake: 0.118989], [G loss: 0.000954, mse: 0.016350]\n",
      "Epoch 170/2001\n",
      "[C1 valid: -0.919888, C2 fake: 0.108568], [G loss: 0.001039, mse: 0.016252]\n",
      "Epoch 171/2001\n",
      "[C1 valid: -0.918562, C2 fake: 0.126563], [G loss: 0.003422, mse: 0.015741]\n",
      "Epoch 172/2001\n",
      "[C1 valid: -0.837315, C2 fake: 0.179882], [G loss: 0.000435, mse: 0.016520]\n",
      "Epoch 173/2001\n",
      "[C1 valid: -0.895081, C2 fake: 0.122770], [G loss: 0.003387, mse: 0.014807]\n",
      "Epoch 174/2001\n",
      "[C1 valid: -0.914455, C2 fake: 0.108540], [G loss: 0.004336, mse: 0.016539]\n",
      "Epoch 175/2001\n",
      "[C1 valid: -0.913955, C2 fake: 0.107755], [G loss: 0.003331, mse: 0.015933]\n",
      "Epoch 176/2001\n",
      "[C1 valid: -0.922478, C2 fake: 0.107694], [G loss: 0.003511, mse: 0.015629]\n",
      "Epoch 177/2001\n",
      "[C1 valid: -0.919329, C2 fake: 0.112776], [G loss: 0.003415, mse: 0.015517]\n",
      "Epoch 178/2001\n",
      "[C1 valid: -0.918995, C2 fake: 0.107429], [G loss: 0.002218, mse: 0.015168]\n",
      "Epoch 179/2001\n",
      "[C1 valid: -0.921796, C2 fake: 0.108640], [G loss: 0.000282, mse: 0.015669]\n",
      "Epoch 180/2001\n",
      "[C1 valid: -0.927674, C2 fake: 0.102486], [G loss: 0.000922, mse: 0.015468]\n",
      "Epoch 181/2001\n",
      "[C1 valid: -0.926141, C2 fake: 0.103651], [G loss: 0.003501, mse: 0.015538]\n",
      "Epoch 182/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.813272, C2 fake: 0.195608], [G loss: 0.000958, mse: 0.015758]\n",
      "Epoch 183/2001\n",
      "[C1 valid: -0.912427, C2 fake: 0.112953], [G loss: 0.001283, mse: 0.014548]\n",
      "Epoch 184/2001\n",
      "[C1 valid: -0.919297, C2 fake: 0.100714], [G loss: 0.003449, mse: 0.015601]\n",
      "Epoch 185/2001\n",
      "[C1 valid: -0.917062, C2 fake: 0.109444], [G loss: 0.002261, mse: 0.014676]\n",
      "Epoch 186/2001\n",
      "[C1 valid: -0.922042, C2 fake: 0.099277], [G loss: 0.001794, mse: 0.014646]\n",
      "Epoch 187/2001\n",
      "[C1 valid: -0.924964, C2 fake: 0.097577], [G loss: 0.001187, mse: 0.014627]\n",
      "Epoch 188/2001\n",
      "[C1 valid: -0.909819, C2 fake: 0.125693], [G loss: -0.000012, mse: 0.014362]\n",
      "Epoch 189/2001\n",
      "[C1 valid: -0.901513, C2 fake: 0.119394], [G loss: 0.000799, mse: 0.014267]\n",
      "Epoch 190/2001\n",
      "[C1 valid: -0.926125, C2 fake: 0.098608], [G loss: 0.001851, mse: 0.015086]\n",
      "Epoch 191/2001\n",
      "[C1 valid: -0.875473, C2 fake: 0.202953], [G loss: -0.000455, mse: 0.013979]\n",
      "Epoch 192/2001\n",
      "[C1 valid: -0.887689, C2 fake: 0.159023], [G loss: 0.001903, mse: 0.014668]\n",
      "Epoch 193/2001\n",
      "[C1 valid: -0.923766, C2 fake: 0.095049], [G loss: 0.001779, mse: 0.013349]\n",
      "Epoch 194/2001\n",
      "[C1 valid: -0.919763, C2 fake: 0.091767], [G loss: 0.002776, mse: 0.014623]\n",
      "Epoch 195/2001\n",
      "[C1 valid: -0.925132, C2 fake: 0.092497], [G loss: 0.001911, mse: 0.013706]\n",
      "Epoch 196/2001\n",
      "[C1 valid: -0.925388, C2 fake: 0.093428], [G loss: 0.001539, mse: 0.013630]\n",
      "Epoch 197/2001\n",
      "[C1 valid: -0.928115, C2 fake: 0.090476], [G loss: -0.000668, mse: 0.013314]\n",
      "Epoch 198/2001\n",
      "[C1 valid: -0.923557, C2 fake: 0.094240], [G loss: 0.002077, mse: 0.013456]\n",
      "Epoch 199/2001\n",
      "[C1 valid: -0.911570, C2 fake: 0.117989], [G loss: 0.002032, mse: 0.013268]\n",
      "Epoch 200/2001\n",
      "[C1 valid: -0.837306, C2 fake: 0.155782], [G loss: 0.000451, mse: 0.012771]\n",
      "Epoch 201/2001\n",
      "[C1 valid: -0.925115, C2 fake: 0.097722], [G loss: 0.001891, mse: 0.013565]\n",
      "Epoch 202/2001\n",
      "[C1 valid: -0.922612, C2 fake: 0.095326], [G loss: 0.001849, mse: 0.013512]\n",
      "Epoch 203/2001\n",
      "[C1 valid: -0.931346, C2 fake: 0.095454], [G loss: 0.001224, mse: 0.013509]\n",
      "Epoch 204/2001\n",
      "[C1 valid: -0.930488, C2 fake: 0.087493], [G loss: 0.002030, mse: 0.013950]\n",
      "Epoch 205/2001\n",
      "[C1 valid: -0.931641, C2 fake: 0.088782], [G loss: 0.000043, mse: 0.012867]\n",
      "Epoch 206/2001\n",
      "[C1 valid: -0.934581, C2 fake: 0.081546], [G loss: 0.001496, mse: 0.013706]\n",
      "Epoch 207/2001\n",
      "[C1 valid: -0.932594, C2 fake: 0.083361], [G loss: 0.000452, mse: 0.012771]\n",
      "Epoch 208/2001\n",
      "[C1 valid: -0.935937, C2 fake: 0.081343], [G loss: 0.000476, mse: 0.013884]\n",
      "Epoch 209/2001\n",
      "[C1 valid: -0.934692, C2 fake: 0.082838], [G loss: -0.000703, mse: 0.012594]\n",
      "Epoch 210/2001\n",
      "[C1 valid: -0.936830, C2 fake: 0.080959], [G loss: -0.000941, mse: 0.012712]\n",
      "Epoch 211/2001\n",
      "[C1 valid: -0.936559, C2 fake: 0.085637], [G loss: -0.001712, mse: 0.012424]\n",
      "Epoch 212/2001\n",
      "[C1 valid: -0.930939, C2 fake: 0.086717], [G loss: -0.002092, mse: 0.012347]\n",
      "Epoch 213/2001\n",
      "[C1 valid: -0.739902, C2 fake: 0.365905], [G loss: 0.001302, mse: 0.012208]\n",
      "Epoch 214/2001\n",
      "[C1 valid: -0.845993, C2 fake: 0.230613], [G loss: -0.000397, mse: 0.012500]\n",
      "Epoch 215/2001\n",
      "[C1 valid: -0.916608, C2 fake: 0.101688], [G loss: 0.000931, mse: 0.012360]\n",
      "Epoch 216/2001\n",
      "[C1 valid: -0.924754, C2 fake: 0.089053], [G loss: 0.001684, mse: 0.011752]\n",
      "Epoch 217/2001\n",
      "[C1 valid: -0.925455, C2 fake: 0.083538], [G loss: 0.001900, mse: 0.012689]\n",
      "Epoch 218/2001\n",
      "[C1 valid: -0.933450, C2 fake: 0.086440], [G loss: 0.001595, mse: 0.011612]\n",
      "Epoch 219/2001\n",
      "[C1 valid: -0.931156, C2 fake: 0.079372], [G loss: 0.001225, mse: 0.012097]\n",
      "Epoch 220/2001\n",
      "[C1 valid: -0.931351, C2 fake: 0.076306], [G loss: 0.001257, mse: 0.011911]\n",
      "Epoch 221/2001\n",
      "[C1 valid: -0.938536, C2 fake: 0.081218], [G loss: 0.001130, mse: 0.012225]\n",
      "Epoch 222/2001\n",
      "[C1 valid: -0.936109, C2 fake: 0.079252], [G loss: 0.000190, mse: 0.011274]\n",
      "Epoch 223/2001\n",
      "[C1 valid: -0.934830, C2 fake: 0.075175], [G loss: 0.000988, mse: 0.012285]\n",
      "Epoch 224/2001\n",
      "[C1 valid: -0.937108, C2 fake: 0.077639], [G loss: -0.000384, mse: 0.011432]\n",
      "Epoch 225/2001\n",
      "[C1 valid: -0.931504, C2 fake: 0.074141], [G loss: -0.001130, mse: 0.011192]\n",
      "Epoch 226/2001\n",
      "[C1 valid: -0.937506, C2 fake: 0.071241], [G loss: -0.000139, mse: 0.011655]\n",
      "Epoch 227/2001\n",
      "[C1 valid: -0.915232, C2 fake: 0.086046], [G loss: -0.002772, mse: 0.011239]\n",
      "Epoch 228/2001\n",
      "[C1 valid: -0.921863, C2 fake: 0.076219], [G loss: -0.002376, mse: 0.011391]\n",
      "Epoch 229/2001\n",
      "[C1 valid: -0.937511, C2 fake: 0.073066], [G loss: -0.002008, mse: 0.011000]\n",
      "Epoch 230/2001\n",
      "[C1 valid: -0.942738, C2 fake: 0.071827], [G loss: -0.000942, mse: 0.011684]\n",
      "Epoch 231/2001\n",
      "[C1 valid: -0.942464, C2 fake: 0.071118], [G loss: -0.001877, mse: 0.011831]\n",
      "Epoch 232/2001\n",
      "[C1 valid: -0.941106, C2 fake: 0.070332], [G loss: -0.001195, mse: 0.010812]\n",
      "Epoch 233/2001\n",
      "[C1 valid: -0.941382, C2 fake: 0.073701], [G loss: -0.001652, mse: 0.010935]\n",
      "Epoch 234/2001\n",
      "[C1 valid: -0.944880, C2 fake: 0.073134], [G loss: -0.001329, mse: 0.011150]\n",
      "Epoch 235/2001\n",
      "[C1 valid: -0.941968, C2 fake: 0.068429], [G loss: -0.002040, mse: 0.010360]\n",
      "Epoch 236/2001\n",
      "[C1 valid: -0.943977, C2 fake: 0.068137], [G loss: -0.001667, mse: 0.010572]\n",
      "Epoch 237/2001\n",
      "[C1 valid: -0.942373, C2 fake: 0.068578], [G loss: -0.001064, mse: 0.010823]\n",
      "Epoch 238/2001\n",
      "[C1 valid: -0.941434, C2 fake: 0.069958], [G loss: -0.002167, mse: 0.010534]\n",
      "Epoch 239/2001\n",
      "[C1 valid: -0.940010, C2 fake: 0.069370], [G loss: -0.002727, mse: 0.011094]\n",
      "Epoch 240/2001\n",
      "[C1 valid: -0.850330, C2 fake: 0.098004], [G loss: -0.002686, mse: 0.010107]\n",
      "Epoch 241/2001\n",
      "[C1 valid: -0.935110, C2 fake: 0.075132], [G loss: -0.001430, mse: 0.010698]\n",
      "Epoch 242/2001\n",
      "[C1 valid: -0.942810, C2 fake: 0.069072], [G loss: -0.001348, mse: 0.010725]\n",
      "Epoch 243/2001\n",
      "[C1 valid: -0.945840, C2 fake: 0.072976], [G loss: -0.000725, mse: 0.010480]\n",
      "Epoch 244/2001\n",
      "[C1 valid: -0.935027, C2 fake: 0.076830], [G loss: -0.000714, mse: 0.011456]\n",
      "Epoch 245/2001\n",
      "[C1 valid: -0.936193, C2 fake: 0.076020], [G loss: -0.000881, mse: 0.010796]\n",
      "Epoch 246/2001\n",
      "[C1 valid: -0.945286, C2 fake: 0.067845], [G loss: -0.001551, mse: 0.010743]\n",
      "Epoch 247/2001\n",
      "[C1 valid: -0.948978, C2 fake: 0.071496], [G loss: -0.001751, mse: 0.010567]\n",
      "Epoch 248/2001\n",
      "[C1 valid: -0.948116, C2 fake: 0.069525], [G loss: -0.000726, mse: 0.010762]\n",
      "Epoch 249/2001\n",
      "[C1 valid: -0.945103, C2 fake: 0.068501], [G loss: -0.003064, mse: 0.010664]\n",
      "Epoch 250/2001\n",
      "[C1 valid: -0.941646, C2 fake: 0.068544], [G loss: -0.001248, mse: 0.011010]\n",
      "Epoch 251/2001\n",
      "[C1 valid: -0.945373, C2 fake: 0.064997], [G loss: -0.001896, mse: 0.011776]\n",
      "Epoch 252/2001\n",
      "[C1 valid: -0.949266, C2 fake: 0.065885], [G loss: -0.002606, mse: 0.010766]\n",
      "Epoch 253/2001\n",
      "[C1 valid: -0.949234, C2 fake: 0.061720], [G loss: -0.001666, mse: 0.010301]\n",
      "Epoch 254/2001\n",
      "[C1 valid: -0.950484, C2 fake: 0.064395], [G loss: -0.003047, mse: 0.010360]\n",
      "Epoch 255/2001\n",
      "[C1 valid: -0.944424, C2 fake: 0.065015], [G loss: -0.003082, mse: 0.009904]\n",
      "Epoch 256/2001\n",
      "[C1 valid: -0.941343, C2 fake: 0.070353], [G loss: -0.003233, mse: 0.009852]\n",
      "Epoch 257/2001\n",
      "[C1 valid: -0.951146, C2 fake: 0.064576], [G loss: -0.000337, mse: 0.010348]\n",
      "Epoch 258/2001\n",
      "[C1 valid: -0.881598, C2 fake: 0.158821], [G loss: -0.002469, mse: 0.010512]\n",
      "Epoch 259/2001\n",
      "[C1 valid: -0.811340, C2 fake: 0.219990], [G loss: -0.002924, mse: 0.010357]\n",
      "Epoch 260/2001\n",
      "[C1 valid: -0.911279, C2 fake: 0.107877], [G loss: -0.001105, mse: 0.009311]\n",
      "Epoch 261/2001\n",
      "[C1 valid: -0.944537, C2 fake: 0.089771], [G loss: -0.000626, mse: 0.009869]\n",
      "Epoch 262/2001\n",
      "[C1 valid: -0.943293, C2 fake: 0.084684], [G loss: -0.001411, mse: 0.009321]\n",
      "Epoch 263/2001\n",
      "[C1 valid: -0.941602, C2 fake: 0.073673], [G loss: -0.000622, mse: 0.009066]\n",
      "Epoch 264/2001\n",
      "[C1 valid: -0.941875, C2 fake: 0.070722], [G loss: -0.000434, mse: 0.009626]\n",
      "Epoch 265/2001\n",
      "[C1 valid: -0.947844, C2 fake: 0.072020], [G loss: -0.001966, mse: 0.008875]\n",
      "Epoch 266/2001\n",
      "[C1 valid: -0.949054, C2 fake: 0.070353], [G loss: -0.001564, mse: 0.009517]\n",
      "Epoch 267/2001\n",
      "[C1 valid: -0.945617, C2 fake: 0.066016], [G loss: -0.001341, mse: 0.009141]\n",
      "Epoch 268/2001\n",
      "[C1 valid: -0.947271, C2 fake: 0.064568], [G loss: -0.001461, mse: 0.009247]\n",
      "Epoch 269/2001\n",
      "[C1 valid: -0.946246, C2 fake: 0.065987], [G loss: -0.000912, mse: 0.009615]\n",
      "Epoch 270/2001\n",
      "[C1 valid: -0.947026, C2 fake: 0.064608], [G loss: -0.001619, mse: 0.009225]\n",
      "Epoch 271/2001\n",
      "[C1 valid: -0.949099, C2 fake: 0.064981], [G loss: -0.002211, mse: 0.009708]\n",
      "Epoch 272/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.946681, C2 fake: 0.061951], [G loss: -0.001247, mse: 0.009022]\n",
      "Epoch 273/2001\n",
      "[C1 valid: -0.945872, C2 fake: 0.064623], [G loss: -0.002426, mse: 0.008518]\n",
      "Epoch 274/2001\n",
      "[C1 valid: -0.950268, C2 fake: 0.056450], [G loss: -0.002094, mse: 0.009028]\n",
      "Epoch 275/2001\n",
      "[C1 valid: -0.953304, C2 fake: 0.057065], [G loss: -0.002770, mse: 0.008981]\n",
      "Epoch 276/2001\n",
      "[C1 valid: -0.950679, C2 fake: 0.058175], [G loss: -0.002261, mse: 0.008945]\n",
      "Epoch 277/2001\n",
      "[C1 valid: -0.951114, C2 fake: 0.057953], [G loss: -0.002763, mse: 0.009117]\n",
      "Epoch 278/2001\n",
      "[C1 valid: -0.951756, C2 fake: 0.060589], [G loss: -0.001288, mse: 0.009266]\n",
      "Epoch 279/2001\n",
      "[C1 valid: -0.934401, C2 fake: 0.067847], [G loss: -0.002556, mse: 0.008882]\n",
      "Epoch 280/2001\n",
      "[C1 valid: -0.865856, C2 fake: 0.140257], [G loss: -0.003242, mse: 0.009182]\n",
      "Epoch 281/2001\n",
      "[C1 valid: -0.857158, C2 fake: 0.190982], [G loss: -0.001257, mse: 0.009078]\n",
      "Epoch 282/2001\n",
      "[C1 valid: -0.921854, C2 fake: 0.112323], [G loss: -0.000514, mse: 0.008961]\n",
      "Epoch 283/2001\n",
      "[C1 valid: -0.942759, C2 fake: 0.087689], [G loss: -0.000636, mse: 0.008662]\n",
      "Epoch 284/2001\n",
      "[C1 valid: -0.948451, C2 fake: 0.080224], [G loss: -0.001768, mse: 0.008281]\n",
      "Epoch 285/2001\n",
      "[C1 valid: -0.943337, C2 fake: 0.078628], [G loss: -0.001867, mse: 0.009146]\n",
      "Epoch 286/2001\n",
      "[C1 valid: -0.942755, C2 fake: 0.077533], [G loss: -0.001558, mse: 0.008796]\n",
      "Epoch 287/2001\n",
      "[C1 valid: -0.946460, C2 fake: 0.075876], [G loss: -0.001774, mse: 0.008484]\n",
      "Epoch 288/2001\n",
      "[C1 valid: -0.944945, C2 fake: 0.068501], [G loss: -0.000821, mse: 0.009096]\n",
      "Epoch 289/2001\n",
      "[C1 valid: -0.947751, C2 fake: 0.063106], [G loss: -0.000923, mse: 0.008792]\n",
      "Epoch 290/2001\n",
      "[C1 valid: -0.947323, C2 fake: 0.062854], [G loss: -0.001387, mse: 0.008854]\n",
      "Epoch 291/2001\n",
      "[C1 valid: -0.948676, C2 fake: 0.062083], [G loss: -0.001412, mse: 0.008571]\n",
      "Epoch 292/2001\n",
      "[C1 valid: -0.945996, C2 fake: 0.056074], [G loss: -0.001308, mse: 0.007933]\n",
      "Epoch 293/2001\n",
      "[C1 valid: -0.948457, C2 fake: 0.057732], [G loss: -0.001987, mse: 0.007955]\n",
      "Epoch 294/2001\n",
      "[C1 valid: -0.953039, C2 fake: 0.056591], [G loss: -0.001165, mse: 0.008687]\n",
      "Epoch 295/2001\n",
      "[C1 valid: -0.950416, C2 fake: 0.060590], [G loss: -0.001600, mse: 0.008624]\n",
      "Epoch 296/2001\n",
      "[C1 valid: -0.952348, C2 fake: 0.056215], [G loss: -0.001499, mse: 0.008612]\n",
      "Epoch 297/2001\n",
      "[C1 valid: -0.954253, C2 fake: 0.056327], [G loss: -0.002440, mse: 0.007919]\n",
      "Epoch 298/2001\n",
      "[C1 valid: -0.951169, C2 fake: 0.057043], [G loss: -0.002235, mse: 0.008478]\n",
      "Epoch 299/2001\n",
      "[C1 valid: -0.955212, C2 fake: 0.058101], [G loss: -0.002333, mse: 0.008386]\n",
      "Epoch 300/2001\n",
      "[C1 valid: -0.952522, C2 fake: 0.056067], [G loss: -0.001802, mse: 0.007861]\n",
      "Epoch 301/2001\n",
      "[C1 valid: -0.952124, C2 fake: 0.054870], [G loss: -0.001908, mse: 0.008448]\n",
      "Epoch 302/2001\n",
      "[C1 valid: -0.953067, C2 fake: 0.056999], [G loss: -0.002612, mse: 0.008173]\n",
      "Epoch 303/2001\n",
      "[C1 valid: -0.953603, C2 fake: 0.057367], [G loss: -0.003412, mse: 0.007743]\n",
      "Epoch 304/2001\n",
      "[C1 valid: -0.947547, C2 fake: 0.064955], [G loss: -0.001330, mse: 0.007984]\n",
      "Epoch 305/2001\n",
      "[C1 valid: -0.951443, C2 fake: 0.055560], [G loss: -0.001902, mse: 0.008422]\n",
      "Epoch 306/2001\n",
      "[C1 valid: -0.956292, C2 fake: 0.056204], [G loss: -0.002981, mse: 0.007715]\n",
      "Epoch 307/2001\n",
      "[C1 valid: -0.952937, C2 fake: 0.052070], [G loss: -0.001809, mse: 0.007997]\n",
      "Epoch 308/2001\n",
      "[C1 valid: -0.956379, C2 fake: 0.051525], [G loss: -0.003045, mse: 0.007962]\n",
      "Epoch 309/2001\n",
      "[C1 valid: -0.952712, C2 fake: 0.055096], [G loss: -0.002550, mse: 0.007819]\n",
      "Epoch 310/2001\n",
      "[C1 valid: -0.958340, C2 fake: 0.049418], [G loss: -0.002471, mse: 0.007612]\n",
      "Epoch 311/2001\n",
      "[C1 valid: -0.955713, C2 fake: 0.054543], [G loss: -0.002222, mse: 0.007359]\n",
      "Epoch 312/2001\n",
      "[C1 valid: -0.957690, C2 fake: 0.051274], [G loss: -0.002089, mse: 0.007533]\n",
      "Epoch 313/2001\n",
      "[C1 valid: -0.956724, C2 fake: 0.054732], [G loss: -0.000745, mse: 0.008333]\n",
      "Epoch 314/2001\n",
      "[C1 valid: -0.955657, C2 fake: 0.050728], [G loss: -0.001400, mse: 0.007913]\n",
      "Epoch 315/2001\n",
      "[C1 valid: -0.951630, C2 fake: 0.055127], [G loss: -0.000453, mse: 0.008625]\n",
      "Epoch 316/2001\n",
      "[C1 valid: -0.951602, C2 fake: 0.057466], [G loss: -0.002275, mse: 0.007885]\n",
      "Epoch 317/2001\n",
      "[C1 valid: -0.958982, C2 fake: 0.054071], [G loss: -0.003633, mse: 0.007803]\n",
      "Epoch 318/2001\n",
      "[C1 valid: -0.954898, C2 fake: 0.057324], [G loss: -0.002827, mse: 0.007909]\n",
      "Epoch 319/2001\n",
      "[C1 valid: -0.956556, C2 fake: 0.052710], [G loss: -0.002169, mse: 0.007233]\n",
      "Epoch 320/2001\n",
      "[C1 valid: -0.951965, C2 fake: 0.061147], [G loss: -0.003534, mse: 0.007801]\n",
      "Epoch 321/2001\n",
      "[C1 valid: -0.944838, C2 fake: 0.058302], [G loss: -0.001866, mse: 0.007463]\n",
      "Epoch 322/2001\n",
      "[C1 valid: -0.959442, C2 fake: 0.051053], [G loss: -0.002475, mse: 0.007523]\n",
      "Epoch 323/2001\n",
      "[C1 valid: -0.958884, C2 fake: 0.049512], [G loss: -0.001926, mse: 0.007862]\n",
      "Epoch 324/2001\n",
      "[C1 valid: -0.953610, C2 fake: 0.074841], [G loss: -0.000934, mse: 0.007395]\n",
      "Epoch 325/2001\n",
      "[C1 valid: -0.852745, C2 fake: 0.206614], [G loss: -0.000224, mse: 0.007534]\n",
      "Epoch 326/2001\n",
      "[C1 valid: -0.900019, C2 fake: 0.084153], [G loss: -0.000354, mse: 0.007643]\n",
      "Epoch 327/2001\n",
      "[C1 valid: -0.950217, C2 fake: 0.076436], [G loss: -0.000599, mse: 0.007138]\n",
      "Epoch 328/2001\n",
      "[C1 valid: -0.951224, C2 fake: 0.069555], [G loss: -0.000334, mse: 0.007387]\n",
      "Epoch 329/2001\n",
      "[C1 valid: -0.952336, C2 fake: 0.066950], [G loss: -0.000534, mse: 0.007021]\n",
      "Epoch 330/2001\n",
      "[C1 valid: -0.953404, C2 fake: 0.065106], [G loss: -0.000517, mse: 0.007365]\n",
      "Epoch 331/2001\n",
      "[C1 valid: -0.952519, C2 fake: 0.061060], [G loss: -0.000359, mse: 0.007265]\n",
      "Epoch 332/2001\n",
      "[C1 valid: -0.952035, C2 fake: 0.060506], [G loss: -0.000491, mse: 0.007062]\n",
      "Epoch 333/2001\n",
      "[C1 valid: -0.950578, C2 fake: 0.057046], [G loss: -0.000278, mse: 0.007206]\n",
      "Epoch 334/2001\n",
      "[C1 valid: -0.954751, C2 fake: 0.054497], [G loss: -0.000473, mse: 0.007010]\n",
      "Epoch 335/2001\n",
      "[C1 valid: -0.957640, C2 fake: 0.051701], [G loss: -0.000616, mse: 0.007285]\n",
      "Epoch 336/2001\n",
      "[C1 valid: -0.956758, C2 fake: 0.052517], [G loss: -0.001071, mse: 0.006827]\n",
      "Epoch 337/2001\n",
      "[C1 valid: -0.956811, C2 fake: 0.049190], [G loss: -0.000571, mse: 0.007228]\n",
      "Epoch 338/2001\n",
      "[C1 valid: -0.957302, C2 fake: 0.052435], [G loss: -0.000501, mse: 0.006921]\n",
      "Epoch 339/2001\n",
      "[C1 valid: -0.866451, C2 fake: 0.197457], [G loss: -0.000034, mse: 0.007110]\n",
      "Epoch 340/2001\n",
      "[C1 valid: -0.951634, C2 fake: 0.066623], [G loss: -0.000307, mse: 0.007223]\n",
      "Epoch 341/2001\n",
      "[C1 valid: -0.950393, C2 fake: 0.067005], [G loss: -0.000626, mse: 0.006915]\n",
      "Epoch 342/2001\n",
      "[C1 valid: -0.955104, C2 fake: 0.059530], [G loss: -0.001257, mse: 0.006478]\n",
      "Epoch 343/2001\n",
      "[C1 valid: -0.953997, C2 fake: 0.061058], [G loss: -0.001068, mse: 0.007000]\n",
      "Epoch 344/2001\n",
      "[C1 valid: -0.956011, C2 fake: 0.058770], [G loss: -0.000992, mse: 0.007251]\n",
      "Epoch 345/2001\n",
      "[C1 valid: -0.953465, C2 fake: 0.055559], [G loss: -0.000956, mse: 0.007106]\n",
      "Epoch 346/2001\n",
      "[C1 valid: -0.955725, C2 fake: 0.053764], [G loss: -0.001182, mse: 0.007047]\n",
      "Epoch 347/2001\n",
      "[C1 valid: -0.959221, C2 fake: 0.049603], [G loss: -0.001237, mse: 0.007047]\n",
      "Epoch 348/2001\n",
      "[C1 valid: -0.956125, C2 fake: 0.052320], [G loss: -0.001347, mse: 0.006896]\n",
      "Epoch 349/2001\n",
      "[C1 valid: -0.956195, C2 fake: 0.052332], [G loss: -0.001448, mse: 0.006999]\n",
      "Epoch 350/2001\n",
      "[C1 valid: -0.959603, C2 fake: 0.051890], [G loss: -0.001672, mse: 0.006438]\n",
      "Epoch 351/2001\n",
      "[C1 valid: -0.956952, C2 fake: 0.048820], [G loss: -0.001466, mse: 0.006874]\n",
      "Epoch 352/2001\n",
      "[C1 valid: -0.958377, C2 fake: 0.050450], [G loss: -0.002513, mse: 0.006976]\n",
      "Epoch 353/2001\n",
      "[C1 valid: -0.956628, C2 fake: 0.046661], [G loss: -0.002339, mse: 0.006630]\n",
      "Epoch 354/2001\n",
      "[C1 valid: -0.960174, C2 fake: 0.049465], [G loss: -0.002607, mse: 0.006436]\n",
      "Epoch 355/2001\n",
      "[C1 valid: -0.960684, C2 fake: 0.048189], [G loss: -0.002047, mse: 0.006690]\n",
      "Epoch 356/2001\n",
      "[C1 valid: -0.961482, C2 fake: 0.049448], [G loss: -0.002544, mse: 0.006655]\n",
      "Epoch 357/2001\n",
      "[C1 valid: -0.963365, C2 fake: 0.049984], [G loss: -0.001285, mse: 0.006833]\n",
      "Epoch 358/2001\n",
      "[C1 valid: -0.960608, C2 fake: 0.054713], [G loss: -0.003219, mse: 0.006480]\n",
      "Epoch 359/2001\n",
      "[C1 valid: -0.952496, C2 fake: 0.048885], [G loss: -0.003139, mse: 0.006657]\n",
      "Epoch 360/2001\n",
      "[C1 valid: -0.959933, C2 fake: 0.045982], [G loss: -0.001761, mse: 0.006367]\n",
      "Epoch 361/2001\n",
      "[C1 valid: -0.956989, C2 fake: 0.048489], [G loss: -0.002705, mse: 0.006086]\n",
      "Epoch 362/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.955699, C2 fake: 0.045020], [G loss: -0.002091, mse: 0.006362]\n",
      "Epoch 363/2001\n",
      "[C1 valid: -0.958269, C2 fake: 0.043616], [G loss: -0.001645, mse: 0.006443]\n",
      "Epoch 364/2001\n",
      "[C1 valid: -0.960810, C2 fake: 0.044671], [G loss: -0.002269, mse: 0.006656]\n",
      "Epoch 365/2001\n",
      "[C1 valid: -0.958894, C2 fake: 0.043779], [G loss: -0.002391, mse: 0.006729]\n",
      "Epoch 366/2001\n",
      "[C1 valid: -0.961302, C2 fake: 0.044473], [G loss: -0.003531, mse: 0.006492]\n",
      "Epoch 367/2001\n",
      "[C1 valid: -0.957573, C2 fake: 0.043503], [G loss: -0.002738, mse: 0.006381]\n",
      "Epoch 368/2001\n",
      "[C1 valid: -0.961876, C2 fake: 0.047410], [G loss: -0.003345, mse: 0.006286]\n",
      "Epoch 369/2001\n",
      "[C1 valid: -0.959808, C2 fake: 0.042840], [G loss: -0.001778, mse: 0.006457]\n",
      "Epoch 370/2001\n",
      "[C1 valid: -0.961471, C2 fake: 0.060630], [G loss: -0.003549, mse: 0.006438]\n",
      "Epoch 371/2001\n",
      "[C1 valid: -0.825563, C2 fake: 0.332164], [G loss: -0.001454, mse: 0.006911]\n",
      "Epoch 372/2001\n",
      "[C1 valid: -0.869668, C2 fake: 0.193041], [G loss: -0.001349, mse: 0.006609]\n",
      "Epoch 373/2001\n",
      "[C1 valid: -0.921542, C2 fake: 0.097869], [G loss: -0.000944, mse: 0.006158]\n",
      "Epoch 374/2001\n",
      "[C1 valid: -0.951853, C2 fake: 0.080019], [G loss: -0.000846, mse: 0.006027]\n",
      "Epoch 375/2001\n",
      "[C1 valid: -0.956564, C2 fake: 0.080483], [G loss: -0.000717, mse: 0.006457]\n",
      "Epoch 376/2001\n",
      "[C1 valid: -0.954797, C2 fake: 0.075121], [G loss: -0.000931, mse: 0.006055]\n",
      "Epoch 377/2001\n",
      "[C1 valid: -0.957745, C2 fake: 0.066913], [G loss: -0.000915, mse: 0.006243]\n",
      "Epoch 378/2001\n",
      "[C1 valid: -0.957122, C2 fake: 0.069277], [G loss: -0.000777, mse: 0.006302]\n",
      "Epoch 379/2001\n",
      "[C1 valid: -0.956160, C2 fake: 0.061787], [G loss: -0.000750, mse: 0.006409]\n",
      "Epoch 380/2001\n",
      "[C1 valid: -0.958988, C2 fake: 0.056968], [G loss: -0.000939, mse: 0.006182]\n",
      "Epoch 381/2001\n",
      "[C1 valid: -0.956986, C2 fake: 0.061642], [G loss: -0.000704, mse: 0.006385]\n",
      "Epoch 382/2001\n",
      "[C1 valid: -0.959448, C2 fake: 0.061066], [G loss: -0.000530, mse: 0.006314]\n",
      "Epoch 383/2001\n",
      "[C1 valid: -0.958951, C2 fake: 0.053524], [G loss: -0.000573, mse: 0.006387]\n",
      "Epoch 384/2001\n",
      "[C1 valid: -0.960177, C2 fake: 0.054720], [G loss: -0.000838, mse: 0.006114]\n",
      "Epoch 385/2001\n",
      "[C1 valid: -0.957994, C2 fake: 0.050704], [G loss: -0.000597, mse: 0.006036]\n",
      "Epoch 386/2001\n",
      "[C1 valid: -0.963126, C2 fake: 0.057116], [G loss: -0.000725, mse: 0.006017]\n",
      "Epoch 387/2001\n",
      "[C1 valid: -0.961398, C2 fake: 0.051784], [G loss: -0.000956, mse: 0.005876]\n",
      "Epoch 388/2001\n",
      "[C1 valid: -0.960322, C2 fake: 0.051451], [G loss: -0.000804, mse: 0.005842]\n",
      "Epoch 389/2001\n",
      "[C1 valid: -0.958998, C2 fake: 0.051180], [G loss: -0.000603, mse: 0.006310]\n",
      "Epoch 390/2001\n",
      "[C1 valid: -0.963528, C2 fake: 0.050441], [G loss: -0.000852, mse: 0.006068]\n",
      "Epoch 391/2001\n",
      "[C1 valid: -0.958532, C2 fake: 0.051037], [G loss: -0.000965, mse: 0.005950]\n",
      "Epoch 392/2001\n",
      "[C1 valid: -0.958723, C2 fake: 0.052924], [G loss: -0.000919, mse: 0.006263]\n",
      "Epoch 393/2001\n",
      "[C1 valid: -0.961310, C2 fake: 0.047127], [G loss: -0.001387, mse: 0.005922]\n",
      "Epoch 394/2001\n",
      "[C1 valid: -0.963509, C2 fake: 0.045230], [G loss: -0.001073, mse: 0.005882]\n",
      "Epoch 395/2001\n",
      "[C1 valid: -0.961646, C2 fake: 0.046946], [G loss: -0.001386, mse: 0.006077]\n",
      "Epoch 396/2001\n",
      "[C1 valid: -0.962741, C2 fake: 0.050615], [G loss: -0.001552, mse: 0.006223]\n",
      "Epoch 397/2001\n",
      "[C1 valid: -0.963859, C2 fake: 0.050171], [G loss: -0.002080, mse: 0.005995]\n",
      "Epoch 398/2001\n",
      "[C1 valid: -0.965178, C2 fake: 0.047843], [G loss: -0.001283, mse: 0.005974]\n",
      "Epoch 399/2001\n",
      "[C1 valid: -0.963706, C2 fake: 0.050256], [G loss: -0.004012, mse: 0.005837]\n",
      "Epoch 400/2001\n",
      "[C1 valid: -0.924724, C2 fake: 0.064800], [G loss: -0.002929, mse: 0.005796]\n",
      "Epoch 401/2001\n",
      "[C1 valid: -0.961365, C2 fake: 0.050922], [G loss: -0.001554, mse: 0.005940]\n",
      "Epoch 402/2001\n",
      "[C1 valid: -0.962397, C2 fake: 0.049635], [G loss: -0.003200, mse: 0.005785]\n",
      "Epoch 403/2001\n",
      "[C1 valid: -0.935447, C2 fake: 0.053121], [G loss: -0.001988, mse: 0.005875]\n",
      "Epoch 404/2001\n",
      "[C1 valid: -0.953170, C2 fake: 0.053446], [G loss: -0.003288, mse: 0.005722]\n",
      "Epoch 405/2001\n",
      "[C1 valid: -0.956169, C2 fake: 0.086806], [G loss: -0.001850, mse: 0.005675]\n",
      "Epoch 406/2001\n",
      "[C1 valid: -0.866490, C2 fake: 0.308257], [G loss: -0.001264, mse: 0.005809]\n",
      "Epoch 407/2001\n",
      "[C1 valid: -0.949345, C2 fake: 0.119487], [G loss: -0.002388, mse: 0.005514]\n",
      "Epoch 408/2001\n",
      "[C1 valid: -0.951344, C2 fake: 0.061814], [G loss: -0.001769, mse: 0.005505]\n",
      "Epoch 409/2001\n",
      "[C1 valid: -0.958270, C2 fake: 0.058580], [G loss: -0.001921, mse: 0.005818]\n",
      "Epoch 410/2001\n",
      "[C1 valid: -0.959724, C2 fake: 0.053910], [G loss: -0.001709, mse: 0.005560]\n",
      "Epoch 411/2001\n",
      "[C1 valid: -0.957553, C2 fake: 0.053690], [G loss: -0.001566, mse: 0.005663]\n",
      "Epoch 412/2001\n",
      "[C1 valid: -0.961736, C2 fake: 0.051224], [G loss: -0.001637, mse: 0.005845]\n",
      "Epoch 413/2001\n",
      "[C1 valid: -0.958117, C2 fake: 0.050362], [G loss: -0.002434, mse: 0.005751]\n",
      "Epoch 414/2001\n",
      "[C1 valid: -0.962402, C2 fake: 0.049587], [G loss: -0.002470, mse: 0.005499]\n",
      "Epoch 415/2001\n",
      "[C1 valid: -0.959723, C2 fake: 0.053557], [G loss: -0.002245, mse: 0.005708]\n",
      "Epoch 416/2001\n",
      "[C1 valid: -0.958452, C2 fake: 0.046695], [G loss: -0.002930, mse: 0.006343]\n",
      "Epoch 417/2001\n",
      "[C1 valid: -0.957105, C2 fake: 0.049275], [G loss: -0.002034, mse: 0.006000]\n",
      "Epoch 418/2001\n",
      "[C1 valid: -0.959514, C2 fake: 0.047932], [G loss: -0.002198, mse: 0.005689]\n",
      "Epoch 419/2001\n",
      "[C1 valid: -0.961044, C2 fake: 0.045847], [G loss: -0.002762, mse: 0.005445]\n",
      "Epoch 420/2001\n",
      "[C1 valid: -0.959658, C2 fake: 0.044122], [G loss: -0.002654, mse: 0.005786]\n",
      "Epoch 421/2001\n",
      "[C1 valid: -0.961195, C2 fake: 0.044030], [G loss: -0.001878, mse: 0.006083]\n",
      "Epoch 422/2001\n",
      "[C1 valid: -0.961796, C2 fake: 0.041491], [G loss: -0.003047, mse: 0.005692]\n",
      "Epoch 423/2001\n",
      "[C1 valid: -0.964757, C2 fake: 0.051525], [G loss: -0.002920, mse: 0.005515]\n",
      "Epoch 424/2001\n",
      "[C1 valid: -0.963775, C2 fake: 0.047869], [G loss: -0.002598, mse: 0.005801]\n",
      "Epoch 425/2001\n",
      "[C1 valid: -0.962191, C2 fake: 0.045093], [G loss: -0.003373, mse: 0.005745]\n",
      "Epoch 426/2001\n",
      "[C1 valid: -0.962360, C2 fake: 0.045692], [G loss: -0.003035, mse: 0.005504]\n",
      "Epoch 427/2001\n",
      "[C1 valid: -0.950377, C2 fake: 0.062335], [G loss: -0.004159, mse: 0.005575]\n",
      "Epoch 428/2001\n",
      "[C1 valid: -0.952196, C2 fake: 0.046724], [G loss: -0.003436, mse: 0.005324]\n",
      "Epoch 429/2001\n",
      "[C1 valid: -0.963063, C2 fake: 0.049010], [G loss: -0.002317, mse: 0.005375]\n",
      "Epoch 430/2001\n",
      "[C1 valid: -0.964978, C2 fake: 0.045768], [G loss: -0.003094, mse: 0.005500]\n",
      "Epoch 431/2001\n",
      "[C1 valid: -0.967013, C2 fake: 0.045872], [G loss: -0.003096, mse: 0.005518]\n",
      "Epoch 432/2001\n",
      "[C1 valid: -0.961379, C2 fake: 0.050136], [G loss: -0.003822, mse: 0.005879]\n",
      "Epoch 433/2001\n",
      "[C1 valid: -0.963629, C2 fake: 0.044426], [G loss: -0.003089, mse: 0.005233]\n",
      "Epoch 434/2001\n",
      "[C1 valid: -0.964594, C2 fake: 0.046346], [G loss: -0.003504, mse: 0.005185]\n",
      "Epoch 435/2001\n",
      "[C1 valid: -0.967804, C2 fake: 0.044385], [G loss: -0.003119, mse: 0.005520]\n",
      "Epoch 436/2001\n",
      "[C1 valid: -0.966754, C2 fake: 0.050784], [G loss: -0.004081, mse: 0.005159]\n",
      "Epoch 437/2001\n",
      "[C1 valid: -0.961003, C2 fake: 0.044271], [G loss: -0.002061, mse: 0.005609]\n",
      "Epoch 438/2001\n",
      "[C1 valid: -0.963572, C2 fake: 0.043659], [G loss: -0.002984, mse: 0.005375]\n",
      "Epoch 439/2001\n",
      "[C1 valid: -0.963039, C2 fake: 0.055609], [G loss: -0.004510, mse: 0.005156]\n",
      "Epoch 440/2001\n",
      "[C1 valid: -0.907493, C2 fake: 0.192555], [G loss: -0.002430, mse: 0.005183]\n",
      "Epoch 441/2001\n",
      "[C1 valid: -0.947394, C2 fake: 0.058809], [G loss: -0.002023, mse: 0.005441]\n",
      "Epoch 442/2001\n",
      "[C1 valid: -0.959544, C2 fake: 0.058782], [G loss: -0.002525, mse: 0.005054]\n",
      "Epoch 443/2001\n",
      "[C1 valid: -0.961744, C2 fake: 0.052590], [G loss: -0.002271, mse: 0.005204]\n",
      "Epoch 444/2001\n",
      "[C1 valid: -0.964409, C2 fake: 0.059267], [G loss: -0.002198, mse: 0.005012]\n",
      "Epoch 445/2001\n",
      "[C1 valid: -0.962134, C2 fake: 0.051986], [G loss: -0.002002, mse: 0.005158]\n",
      "Epoch 446/2001\n",
      "[C1 valid: -0.964421, C2 fake: 0.052746], [G loss: -0.002077, mse: 0.005198]\n",
      "Epoch 447/2001\n",
      "[C1 valid: -0.962650, C2 fake: 0.047341], [G loss: -0.002045, mse: 0.005115]\n",
      "Epoch 448/2001\n",
      "[C1 valid: -0.964834, C2 fake: 0.048319], [G loss: -0.002549, mse: 0.005000]\n",
      "Epoch 449/2001\n",
      "[C1 valid: -0.960904, C2 fake: 0.049343], [G loss: -0.001770, mse: 0.005132]\n",
      "Epoch 450/2001\n",
      "[C1 valid: -0.964638, C2 fake: 0.058833], [G loss: -0.001899, mse: 0.005340]\n",
      "Epoch 451/2001\n",
      "[C1 valid: -0.916260, C2 fake: 0.153585], [G loss: -0.001976, mse: 0.005064]\n",
      "Epoch 452/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.905681, C2 fake: 0.071024], [G loss: -0.001158, mse: 0.005245]\n",
      "Epoch 453/2001\n",
      "[C1 valid: -0.957562, C2 fake: 0.063905], [G loss: -0.001153, mse: 0.005678]\n",
      "Epoch 454/2001\n",
      "[C1 valid: -0.962662, C2 fake: 0.059600], [G loss: -0.001007, mse: 0.006044]\n",
      "Epoch 455/2001\n",
      "[C1 valid: -0.964608, C2 fake: 0.056744], [G loss: -0.001019, mse: 0.006018]\n",
      "Epoch 456/2001\n",
      "[C1 valid: -0.967110, C2 fake: 0.054542], [G loss: -0.001142, mse: 0.005414]\n",
      "Epoch 457/2001\n",
      "[C1 valid: -0.959814, C2 fake: 0.056399], [G loss: -0.000940, mse: 0.005478]\n",
      "Epoch 458/2001\n",
      "[C1 valid: -0.965217, C2 fake: 0.052672], [G loss: -0.000955, mse: 0.005439]\n",
      "Epoch 459/2001\n",
      "[C1 valid: -0.962262, C2 fake: 0.046630], [G loss: -0.001012, mse: 0.005427]\n",
      "Epoch 460/2001\n",
      "[C1 valid: -0.962032, C2 fake: 0.046373], [G loss: -0.001322, mse: 0.005178]\n",
      "Epoch 461/2001\n",
      "[C1 valid: -0.964005, C2 fake: 0.045637], [G loss: -0.001049, mse: 0.005181]\n",
      "Epoch 462/2001\n",
      "[C1 valid: -0.963921, C2 fake: 0.046236], [G loss: -0.001067, mse: 0.005413]\n",
      "Epoch 463/2001\n",
      "[C1 valid: -0.965715, C2 fake: 0.041908], [G loss: -0.001183, mse: 0.005066]\n",
      "Epoch 464/2001\n",
      "[C1 valid: -0.963885, C2 fake: 0.043893], [G loss: -0.001190, mse: 0.005002]\n",
      "Epoch 465/2001\n",
      "[C1 valid: -0.963596, C2 fake: 0.046082], [G loss: -0.001250, mse: 0.005144]\n",
      "Epoch 466/2001\n",
      "[C1 valid: -0.964779, C2 fake: 0.038761], [G loss: -0.001233, mse: 0.005447]\n",
      "Epoch 467/2001\n",
      "[C1 valid: -0.968324, C2 fake: 0.046897], [G loss: -0.001835, mse: 0.005050]\n",
      "Epoch 468/2001\n",
      "[C1 valid: -0.965728, C2 fake: 0.043905], [G loss: -0.001449, mse: 0.005212]\n",
      "Epoch 469/2001\n",
      "[C1 valid: -0.966126, C2 fake: 0.055198], [G loss: -0.000372, mse: 0.005170]\n",
      "Epoch 470/2001\n",
      "[C1 valid: -0.846962, C2 fake: 0.148741], [G loss: -0.000435, mse: 0.005004]\n",
      "Epoch 471/2001\n",
      "[C1 valid: -0.938236, C2 fake: 0.053665], [G loss: -0.001537, mse: 0.004831]\n",
      "Epoch 472/2001\n",
      "[C1 valid: -0.960066, C2 fake: 0.057588], [G loss: -0.001370, mse: 0.004988]\n",
      "Epoch 473/2001\n",
      "[C1 valid: -0.960013, C2 fake: 0.055579], [G loss: -0.001506, mse: 0.004907]\n",
      "Epoch 474/2001\n",
      "[C1 valid: -0.965157, C2 fake: 0.055969], [G loss: -0.001560, mse: 0.005038]\n",
      "Epoch 475/2001\n",
      "[C1 valid: -0.963467, C2 fake: 0.050009], [G loss: -0.001782, mse: 0.004821]\n",
      "Epoch 476/2001\n",
      "[C1 valid: -0.965010, C2 fake: 0.045326], [G loss: -0.001653, mse: 0.005062]\n",
      "Epoch 477/2001\n",
      "[C1 valid: -0.964288, C2 fake: 0.049626], [G loss: -0.001596, mse: 0.005172]\n",
      "Epoch 478/2001\n",
      "[C1 valid: -0.965333, C2 fake: 0.044029], [G loss: -0.001644, mse: 0.004952]\n",
      "Epoch 479/2001\n",
      "[C1 valid: -0.961006, C2 fake: 0.041618], [G loss: -0.001569, mse: 0.004925]\n",
      "Epoch 480/2001\n",
      "[C1 valid: -0.965822, C2 fake: 0.055126], [G loss: -0.003302, mse: 0.005023]\n",
      "Epoch 481/2001\n",
      "[C1 valid: -0.956304, C2 fake: 0.047992], [G loss: -0.001809, mse: 0.005070]\n",
      "Epoch 482/2001\n",
      "[C1 valid: -0.958901, C2 fake: 0.052387], [G loss: -0.001854, mse: 0.004805]\n",
      "Epoch 483/2001\n",
      "[C1 valid: -0.958285, C2 fake: 0.056912], [G loss: -0.001818, mse: 0.004761]\n",
      "Epoch 484/2001\n",
      "[C1 valid: -0.965955, C2 fake: 0.042103], [G loss: -0.001736, mse: 0.004915]\n",
      "Epoch 485/2001\n",
      "[C1 valid: -0.963594, C2 fake: 0.046300], [G loss: -0.001887, mse: 0.004648]\n",
      "Epoch 486/2001\n",
      "[C1 valid: -0.962701, C2 fake: 0.042502], [G loss: -0.002062, mse: 0.004510]\n",
      "Epoch 487/2001\n",
      "[C1 valid: -0.968329, C2 fake: 0.042993], [G loss: -0.002318, mse: 0.004811]\n",
      "Epoch 488/2001\n",
      "[C1 valid: -0.965298, C2 fake: 0.042931], [G loss: -0.002273, mse: 0.004701]\n",
      "Epoch 489/2001\n",
      "[C1 valid: -0.968017, C2 fake: 0.044407], [G loss: -0.002450, mse: 0.004606]\n",
      "Epoch 490/2001\n",
      "[C1 valid: -0.965609, C2 fake: 0.042557], [G loss: -0.002464, mse: 0.004714]\n",
      "Epoch 491/2001\n",
      "[C1 valid: -0.964623, C2 fake: 0.044653], [G loss: -0.002212, mse: 0.004871]\n",
      "Epoch 492/2001\n",
      "[C1 valid: -0.965349, C2 fake: 0.047392], [G loss: -0.002814, mse: 0.004362]\n",
      "Epoch 493/2001\n",
      "[C1 valid: -0.968021, C2 fake: 0.037337], [G loss: -0.003342, mse: 0.004582]\n",
      "Epoch 494/2001\n",
      "[C1 valid: -0.967118, C2 fake: 0.040131], [G loss: -0.002466, mse: 0.004714]\n",
      "Epoch 495/2001\n",
      "[C1 valid: -0.965838, C2 fake: 0.038630], [G loss: -0.003487, mse: 0.004774]\n",
      "Epoch 496/2001\n",
      "[C1 valid: -0.966872, C2 fake: 0.040409], [G loss: -0.004049, mse: 0.004583]\n",
      "Epoch 497/2001\n",
      "[C1 valid: -0.916669, C2 fake: 0.148847], [G loss: -0.002054, mse: 0.004900]\n",
      "Epoch 498/2001\n",
      "[C1 valid: -0.842425, C2 fake: 0.221428], [G loss: -0.001313, mse: 0.004821]\n",
      "Epoch 499/2001\n",
      "[C1 valid: -0.909559, C2 fake: 0.093256], [G loss: -0.002135, mse: 0.004792]\n",
      "Epoch 500/2001\n",
      "[C1 valid: -0.955384, C2 fake: 0.058242], [G loss: -0.002377, mse: 0.004615]\n",
      "Epoch 501/2001\n",
      "[C1 valid: -0.966576, C2 fake: 0.050564], [G loss: -0.002673, mse: 0.004222]\n",
      "Epoch 502/2001\n",
      "[C1 valid: -0.963370, C2 fake: 0.050401], [G loss: -0.002351, mse: 0.004547]\n",
      "Epoch 503/2001\n",
      "[C1 valid: -0.963931, C2 fake: 0.051798], [G loss: -0.002362, mse: 0.004555]\n",
      "Epoch 504/2001\n",
      "[C1 valid: -0.962890, C2 fake: 0.046818], [G loss: -0.001900, mse: 0.004716]\n",
      "Epoch 505/2001\n",
      "[C1 valid: -0.959483, C2 fake: 0.049750], [G loss: -0.002011, mse: 0.004610]\n",
      "Epoch 506/2001\n",
      "[C1 valid: -0.962851, C2 fake: 0.049569], [G loss: -0.002048, mse: 0.004783]\n",
      "Epoch 507/2001\n",
      "[C1 valid: -0.967479, C2 fake: 0.043568], [G loss: -0.002402, mse: 0.004272]\n",
      "Epoch 508/2001\n",
      "[C1 valid: -0.963894, C2 fake: 0.041803], [G loss: -0.002044, mse: 0.004547]\n",
      "Epoch 509/2001\n",
      "[C1 valid: -0.963590, C2 fake: 0.041505], [G loss: -0.002199, mse: 0.004437]\n",
      "Epoch 510/2001\n",
      "[C1 valid: -0.965716, C2 fake: 0.040914], [G loss: -0.002122, mse: 0.004735]\n",
      "Epoch 511/2001\n",
      "[C1 valid: -0.964614, C2 fake: 0.040385], [G loss: -0.002351, mse: 0.004620]\n",
      "Epoch 512/2001\n",
      "[C1 valid: -0.963823, C2 fake: 0.039066], [G loss: -0.001962, mse: 0.004612]\n",
      "Epoch 513/2001\n",
      "[C1 valid: -0.967476, C2 fake: 0.043115], [G loss: -0.003484, mse: 0.004567]\n",
      "Epoch 514/2001\n",
      "[C1 valid: -0.962211, C2 fake: 0.043587], [G loss: -0.002351, mse: 0.004388]\n",
      "Epoch 515/2001\n",
      "[C1 valid: -0.966486, C2 fake: 0.042526], [G loss: -0.003217, mse: 0.004158]\n",
      "Epoch 516/2001\n",
      "[C1 valid: -0.964295, C2 fake: 0.037529], [G loss: -0.002321, mse: 0.004550]\n",
      "Epoch 517/2001\n",
      "[C1 valid: -0.966859, C2 fake: 0.039667], [G loss: -0.002545, mse: 0.004406]\n",
      "Epoch 518/2001\n",
      "[C1 valid: -0.967070, C2 fake: 0.039102], [G loss: -0.002521, mse: 0.004593]\n",
      "Epoch 519/2001\n",
      "[C1 valid: -0.968391, C2 fake: 0.037331], [G loss: -0.003561, mse: 0.004493]\n",
      "Epoch 520/2001\n",
      "[C1 valid: -0.964757, C2 fake: 0.036658], [G loss: -0.002258, mse: 0.004732]\n",
      "Epoch 521/2001\n",
      "[C1 valid: -0.970000, C2 fake: 0.035981], [G loss: -0.002703, mse: 0.004338]\n",
      "Epoch 522/2001\n",
      "[C1 valid: -0.967957, C2 fake: 0.038093], [G loss: -0.004493, mse: 0.004356]\n",
      "Epoch 523/2001\n",
      "[C1 valid: -0.965338, C2 fake: 0.039156], [G loss: -0.002844, mse: 0.004400]\n",
      "Epoch 524/2001\n",
      "[C1 valid: -0.966690, C2 fake: 0.042633], [G loss: -0.002220, mse: 0.004331]\n",
      "Epoch 525/2001\n",
      "[C1 valid: -0.967349, C2 fake: 0.035467], [G loss: -0.002125, mse: 0.004445]\n",
      "Epoch 526/2001\n",
      "[C1 valid: -0.965022, C2 fake: 0.040242], [G loss: -0.002920, mse: 0.004623]\n",
      "Epoch 527/2001\n",
      "[C1 valid: -0.964334, C2 fake: 0.041352], [G loss: -0.004286, mse: 0.004462]\n",
      "Epoch 528/2001\n",
      "[C1 valid: -0.963013, C2 fake: 0.041853], [G loss: -0.004354, mse: 0.004303]\n",
      "Epoch 529/2001\n",
      "[C1 valid: -0.965408, C2 fake: 0.038353], [G loss: -0.004338, mse: 0.004522]\n",
      "Epoch 530/2001\n",
      "[C1 valid: -0.969173, C2 fake: 0.039549], [G loss: -0.003856, mse: 0.004591]\n",
      "Epoch 531/2001\n",
      "[C1 valid: -0.968428, C2 fake: 0.039111], [G loss: -0.003831, mse: 0.004567]\n",
      "Epoch 532/2001\n",
      "[C1 valid: -0.968763, C2 fake: 0.036848], [G loss: -0.003865, mse: 0.004540]\n",
      "Epoch 533/2001\n",
      "[C1 valid: -0.963584, C2 fake: 0.061705], [G loss: -0.003885, mse: 0.004347]\n",
      "Epoch 534/2001\n",
      "[C1 valid: -0.967607, C2 fake: 0.036986], [G loss: -0.002646, mse: 0.004368]\n",
      "Epoch 535/2001\n",
      "[C1 valid: -0.965636, C2 fake: 0.041095], [G loss: -0.002739, mse: 0.004501]\n",
      "Epoch 536/2001\n",
      "[C1 valid: -0.968125, C2 fake: 0.036763], [G loss: -0.003133, mse: 0.004211]\n",
      "Epoch 537/2001\n",
      "[C1 valid: -0.964521, C2 fake: 0.034825], [G loss: -0.003893, mse: 0.004459]\n",
      "Epoch 538/2001\n",
      "[C1 valid: -0.966536, C2 fake: 0.040409], [G loss: -0.003949, mse: 0.004252]\n",
      "Epoch 539/2001\n",
      "[C1 valid: -0.966581, C2 fake: 0.040338], [G loss: -0.004862, mse: 0.004311]\n",
      "Epoch 540/2001\n",
      "[C1 valid: -0.960808, C2 fake: 0.070765], [G loss: -0.002464, mse: 0.004411]\n",
      "Epoch 541/2001\n",
      "[C1 valid: -0.869534, C2 fake: 0.246553], [G loss: -0.003620, mse: 0.004305]\n",
      "Epoch 542/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.909505, C2 fake: 0.050588], [G loss: -0.002401, mse: 0.004300]\n",
      "Epoch 543/2001\n",
      "[C1 valid: -0.962515, C2 fake: 0.046801], [G loss: -0.002502, mse: 0.004321]\n",
      "Epoch 544/2001\n",
      "[C1 valid: -0.966141, C2 fake: 0.043029], [G loss: -0.002597, mse: 0.004422]\n",
      "Epoch 545/2001\n",
      "[C1 valid: -0.964763, C2 fake: 0.042193], [G loss: -0.002403, mse: 0.004435]\n",
      "Epoch 546/2001\n",
      "[C1 valid: -0.963392, C2 fake: 0.040090], [G loss: -0.002640, mse: 0.004192]\n",
      "Epoch 547/2001\n",
      "[C1 valid: -0.966004, C2 fake: 0.045405], [G loss: -0.003091, mse: 0.004273]\n",
      "Epoch 548/2001\n",
      "[C1 valid: -0.967006, C2 fake: 0.043335], [G loss: -0.003171, mse: 0.004336]\n",
      "Epoch 549/2001\n",
      "[C1 valid: -0.963975, C2 fake: 0.036898], [G loss: -0.002268, mse: 0.004592]\n",
      "Epoch 550/2001\n",
      "[C1 valid: -0.966845, C2 fake: 0.042487], [G loss: -0.002722, mse: 0.004615]\n",
      "Epoch 551/2001\n",
      "[C1 valid: -0.966888, C2 fake: 0.041277], [G loss: -0.003012, mse: 0.004867]\n",
      "Epoch 552/2001\n",
      "[C1 valid: -0.969434, C2 fake: 0.037418], [G loss: -0.002739, mse: 0.004680]\n",
      "Epoch 553/2001\n",
      "[C1 valid: -0.968460, C2 fake: 0.042727], [G loss: -0.002558, mse: 0.004502]\n",
      "Epoch 554/2001\n",
      "[C1 valid: -0.881358, C2 fake: 0.156495], [G loss: -0.003527, mse: 0.004521]\n",
      "Epoch 555/2001\n",
      "[C1 valid: -0.935411, C2 fake: 0.043961], [G loss: -0.001775, mse: 0.004347]\n",
      "Epoch 556/2001\n",
      "[C1 valid: -0.963844, C2 fake: 0.044006], [G loss: -0.002222, mse: 0.004339]\n",
      "Epoch 557/2001\n",
      "[C1 valid: -0.962553, C2 fake: 0.041081], [G loss: -0.002266, mse: 0.004251]\n",
      "Epoch 558/2001\n",
      "[C1 valid: -0.967253, C2 fake: 0.044204], [G loss: -0.002415, mse: 0.004100]\n",
      "Epoch 559/2001\n",
      "[C1 valid: -0.967183, C2 fake: 0.040337], [G loss: -0.002754, mse: 0.004319]\n",
      "Epoch 560/2001\n",
      "[C1 valid: -0.965747, C2 fake: 0.036472], [G loss: -0.002895, mse: 0.004015]\n",
      "Epoch 561/2001\n",
      "[C1 valid: -0.967035, C2 fake: 0.034886], [G loss: -0.003218, mse: 0.004119]\n",
      "Epoch 562/2001\n",
      "[C1 valid: -0.966879, C2 fake: 0.033640], [G loss: -0.003540, mse: 0.003978]\n",
      "Epoch 563/2001\n",
      "[C1 valid: -0.964092, C2 fake: 0.032721], [G loss: -0.002962, mse: 0.004025]\n",
      "Epoch 564/2001\n",
      "[C1 valid: -0.968945, C2 fake: 0.032011], [G loss: -0.004089, mse: 0.004097]\n",
      "Epoch 565/2001\n",
      "[C1 valid: -0.967875, C2 fake: 0.037533], [G loss: -0.003593, mse: 0.004072]\n",
      "Epoch 566/2001\n",
      "[C1 valid: -0.965616, C2 fake: 0.037285], [G loss: -0.002987, mse: 0.004218]\n",
      "Epoch 567/2001\n",
      "[C1 valid: -0.964230, C2 fake: 0.037278], [G loss: -0.002920, mse: 0.004247]\n",
      "Epoch 568/2001\n",
      "[C1 valid: -0.966780, C2 fake: 0.037003], [G loss: -0.003826, mse: 0.004256]\n",
      "Epoch 569/2001\n",
      "[C1 valid: -0.965383, C2 fake: 0.040594], [G loss: -0.002449, mse: 0.003988]\n",
      "Epoch 570/2001\n",
      "[C1 valid: -0.953605, C2 fake: 0.050336], [G loss: -0.002212, mse: 0.004102]\n",
      "Epoch 571/2001\n",
      "[C1 valid: -0.964672, C2 fake: 0.032641], [G loss: -0.001990, mse: 0.004019]\n",
      "Epoch 572/2001\n",
      "[C1 valid: -0.967039, C2 fake: 0.031498], [G loss: -0.002229, mse: 0.004095]\n",
      "Epoch 573/2001\n",
      "[C1 valid: -0.968938, C2 fake: 0.032413], [G loss: -0.002404, mse: 0.003946]\n",
      "Epoch 574/2001\n",
      "[C1 valid: -0.971003, C2 fake: 0.034800], [G loss: -0.002421, mse: 0.003907]\n",
      "Epoch 575/2001\n",
      "[C1 valid: -0.967115, C2 fake: 0.031598], [G loss: -0.002537, mse: 0.003945]\n",
      "Epoch 576/2001\n",
      "[C1 valid: -0.967481, C2 fake: 0.030384], [G loss: -0.002720, mse: 0.004010]\n",
      "Epoch 577/2001\n",
      "[C1 valid: -0.966633, C2 fake: 0.032462], [G loss: -0.001986, mse: 0.004142]\n",
      "Epoch 578/2001\n",
      "[C1 valid: -0.896294, C2 fake: 0.175358], [G loss: -0.002527, mse: 0.003738]\n",
      "Epoch 579/2001\n",
      "[C1 valid: -0.948907, C2 fake: 0.091501], [G loss: -0.002059, mse: 0.004032]\n",
      "Epoch 580/2001\n",
      "[C1 valid: -0.961435, C2 fake: 0.042381], [G loss: -0.002343, mse: 0.004042]\n",
      "Epoch 581/2001\n",
      "[C1 valid: -0.961761, C2 fake: 0.043867], [G loss: -0.001957, mse: 0.004599]\n",
      "Epoch 582/2001\n",
      "[C1 valid: -0.963146, C2 fake: 0.039652], [G loss: -0.001972, mse: 0.004607]\n",
      "Epoch 583/2001\n",
      "[C1 valid: -0.966286, C2 fake: 0.042698], [G loss: -0.002512, mse: 0.003862]\n",
      "Epoch 584/2001\n",
      "[C1 valid: -0.964063, C2 fake: 0.039277], [G loss: -0.002154, mse: 0.004316]\n",
      "Epoch 585/2001\n",
      "[C1 valid: -0.963140, C2 fake: 0.034113], [G loss: -0.002193, mse: 0.003854]\n",
      "Epoch 586/2001\n",
      "[C1 valid: -0.964180, C2 fake: 0.037894], [G loss: -0.002389, mse: 0.003942]\n",
      "Epoch 587/2001\n",
      "[C1 valid: -0.966900, C2 fake: 0.034883], [G loss: -0.002333, mse: 0.003847]\n",
      "Epoch 588/2001\n",
      "[C1 valid: -0.964634, C2 fake: 0.041033], [G loss: -0.002045, mse: 0.004197]\n",
      "Epoch 589/2001\n",
      "[C1 valid: -0.968080, C2 fake: 0.035153], [G loss: -0.001728, mse: 0.004508]\n",
      "Epoch 590/2001\n",
      "[C1 valid: -0.968402, C2 fake: 0.034807], [G loss: -0.001881, mse: 0.004345]\n",
      "Epoch 591/2001\n",
      "[C1 valid: -0.967445, C2 fake: 0.039096], [G loss: -0.002013, mse: 0.004141]\n",
      "Epoch 592/2001\n",
      "[C1 valid: -0.968338, C2 fake: 0.038869], [G loss: -0.002848, mse: 0.003895]\n",
      "Epoch 593/2001\n",
      "[C1 valid: -0.958538, C2 fake: 0.035997], [G loss: -0.002492, mse: 0.003931]\n",
      "Epoch 594/2001\n",
      "[C1 valid: -0.967698, C2 fake: 0.040200], [G loss: -0.002700, mse: 0.003914]\n",
      "Epoch 595/2001\n",
      "[C1 valid: -0.937841, C2 fake: 0.045806], [G loss: -0.004282, mse: 0.003709]\n",
      "Epoch 596/2001\n",
      "[C1 valid: -0.969018, C2 fake: 0.036495], [G loss: -0.002614, mse: 0.003934]\n",
      "Epoch 597/2001\n",
      "[C1 valid: -0.959326, C2 fake: 0.036742], [G loss: -0.002923, mse: 0.003868]\n",
      "Epoch 598/2001\n",
      "[C1 valid: -0.969443, C2 fake: 0.032042], [G loss: -0.003319, mse: 0.003845]\n",
      "Epoch 599/2001\n",
      "[C1 valid: -0.964756, C2 fake: 0.035467], [G loss: -0.004676, mse: 0.003933]\n",
      "Epoch 600/2001\n",
      "[C1 valid: -0.967190, C2 fake: 0.040181], [G loss: -0.003142, mse: 0.004008]\n",
      "Epoch 601/2001\n",
      "[C1 valid: -0.968745, C2 fake: 0.033457], [G loss: -0.003916, mse: 0.003956]\n",
      "Epoch 602/2001\n",
      "[C1 valid: -0.969749, C2 fake: 0.034968], [G loss: -0.003168, mse: 0.003829]\n",
      "Epoch 603/2001\n",
      "[C1 valid: -0.956387, C2 fake: 0.032544], [G loss: -0.004591, mse: 0.003990]\n",
      "Epoch 604/2001\n",
      "[C1 valid: -0.967829, C2 fake: 0.031568], [G loss: -0.004200, mse: 0.004060]\n",
      "Epoch 605/2001\n",
      "[C1 valid: -0.968400, C2 fake: 0.031050], [G loss: -0.003962, mse: 0.003666]\n",
      "Epoch 606/2001\n",
      "[C1 valid: -0.971729, C2 fake: 0.033902], [G loss: -0.003732, mse: 0.003773]\n",
      "Epoch 607/2001\n",
      "[C1 valid: -0.968981, C2 fake: 0.033329], [G loss: -0.003590, mse: 0.003693]\n",
      "Epoch 608/2001\n",
      "[C1 valid: -0.971022, C2 fake: 0.032674], [G loss: -0.003824, mse: 0.003790]\n",
      "Epoch 609/2001\n",
      "[C1 valid: -0.959776, C2 fake: 0.071558], [G loss: -0.005372, mse: 0.003937]\n",
      "Epoch 610/2001\n",
      "[C1 valid: -0.899328, C2 fake: 0.163692], [G loss: -0.002612, mse: 0.004006]\n",
      "Epoch 611/2001\n",
      "[C1 valid: -0.962735, C2 fake: 0.045163], [G loss: -0.002788, mse: 0.004013]\n",
      "Epoch 612/2001\n",
      "[C1 valid: -0.960765, C2 fake: 0.043485], [G loss: -0.002683, mse: 0.004057]\n",
      "Epoch 613/2001\n",
      "[C1 valid: -0.963560, C2 fake: 0.043178], [G loss: -0.002552, mse: 0.004133]\n",
      "Epoch 614/2001\n",
      "[C1 valid: -0.964792, C2 fake: 0.039192], [G loss: -0.002483, mse: 0.003897]\n",
      "Epoch 615/2001\n",
      "[C1 valid: -0.922692, C2 fake: 0.074366], [G loss: -0.002844, mse: 0.004009]\n",
      "Epoch 616/2001\n",
      "[C1 valid: -0.960043, C2 fake: 0.036819], [G loss: -0.002392, mse: 0.004086]\n",
      "Epoch 617/2001\n",
      "[C1 valid: -0.961807, C2 fake: 0.040585], [G loss: -0.002688, mse: 0.003724]\n",
      "Epoch 618/2001\n",
      "[C1 valid: -0.965089, C2 fake: 0.041379], [G loss: -0.002644, mse: 0.003785]\n",
      "Epoch 619/2001\n",
      "[C1 valid: -0.967857, C2 fake: 0.042711], [G loss: -0.002763, mse: 0.003737]\n",
      "Epoch 620/2001\n",
      "[C1 valid: -0.965222, C2 fake: 0.038563], [G loss: -0.002624, mse: 0.003978]\n",
      "Epoch 621/2001\n",
      "[C1 valid: -0.965493, C2 fake: 0.033953], [G loss: -0.002898, mse: 0.003754]\n",
      "Epoch 622/2001\n",
      "[C1 valid: -0.968466, C2 fake: 0.036437], [G loss: -0.003075, mse: 0.003571]\n",
      "Epoch 623/2001\n",
      "[C1 valid: -0.968799, C2 fake: 0.040483], [G loss: -0.002646, mse: 0.003650]\n",
      "Epoch 624/2001\n",
      "[C1 valid: -0.967788, C2 fake: 0.033426], [G loss: -0.002566, mse: 0.003809]\n",
      "Epoch 625/2001\n",
      "[C1 valid: -0.966146, C2 fake: 0.031161], [G loss: -0.002823, mse: 0.003766]\n",
      "Epoch 626/2001\n",
      "[C1 valid: -0.969363, C2 fake: 0.033870], [G loss: -0.003665, mse: 0.003566]\n",
      "Epoch 627/2001\n",
      "[C1 valid: -0.968272, C2 fake: 0.029756], [G loss: -0.002592, mse: 0.003991]\n",
      "Epoch 628/2001\n",
      "[C1 valid: -0.966007, C2 fake: 0.035313], [G loss: -0.003218, mse: 0.003688]\n",
      "Epoch 629/2001\n",
      "[C1 valid: -0.968027, C2 fake: 0.035036], [G loss: -0.002606, mse: 0.003736]\n",
      "Epoch 630/2001\n",
      "[C1 valid: -0.968858, C2 fake: 0.034211], [G loss: -0.002497, mse: 0.003914]\n",
      "Epoch 631/2001\n",
      "[C1 valid: -0.928870, C2 fake: 0.111757], [G loss: -0.002044, mse: 0.003764]\n",
      "Epoch 632/2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.940067, C2 fake: 0.050532], [G loss: -0.002519, mse: 0.003704]\n",
      "Epoch 633/2001\n",
      "[C1 valid: -0.960846, C2 fake: 0.038316], [G loss: -0.002903, mse: 0.003700]\n",
      "Epoch 634/2001\n",
      "[C1 valid: -0.964356, C2 fake: 0.040704], [G loss: -0.002851, mse: 0.003719]\n",
      "Epoch 635/2001\n",
      "[C1 valid: -0.967186, C2 fake: 0.037342], [G loss: -0.002925, mse: 0.003856]\n",
      "Epoch 636/2001\n",
      "[C1 valid: -0.967140, C2 fake: 0.040587], [G loss: -0.003122, mse: 0.003822]\n",
      "Epoch 637/2001\n",
      "[C1 valid: -0.968617, C2 fake: 0.034677], [G loss: -0.003020, mse: 0.003758]\n",
      "Epoch 638/2001\n",
      "[C1 valid: -0.967511, C2 fake: 0.034092], [G loss: -0.002907, mse: 0.003880]\n",
      "Epoch 639/2001\n",
      "[C1 valid: -0.969976, C2 fake: 0.035676], [G loss: -0.003117, mse: 0.003904]\n",
      "Epoch 640/2001\n",
      "[C1 valid: -0.968728, C2 fake: 0.036416], [G loss: -0.003088, mse: 0.003746]\n",
      "Epoch 641/2001\n",
      "[C1 valid: -0.969374, C2 fake: 0.035518], [G loss: -0.003265, mse: 0.003716]\n",
      "Epoch 642/2001\n",
      "[C1 valid: -0.969266, C2 fake: 0.034132], [G loss: -0.003173, mse: 0.003705]\n",
      "Epoch 643/2001\n",
      "[C1 valid: -0.968576, C2 fake: 0.038683], [G loss: -0.002757, mse: 0.003766]\n",
      "Epoch 644/2001\n",
      "[C1 valid: -0.970355, C2 fake: 0.036638], [G loss: -0.003108, mse: 0.003514]\n",
      "Epoch 645/2001\n",
      "[C1 valid: -0.971965, C2 fake: 0.030402], [G loss: -0.003042, mse: 0.003821]\n",
      "Epoch 646/2001\n",
      "[C1 valid: -0.967826, C2 fake: 0.033399], [G loss: -0.002610, mse: 0.003594]\n",
      "Epoch 647/2001\n",
      "[C1 valid: -0.970621, C2 fake: 0.041010], [G loss: -0.002970, mse: 0.003885]\n",
      "Epoch 648/2001\n",
      "[C1 valid: -0.970978, C2 fake: 0.031949], [G loss: -0.002698, mse: 0.003824]\n",
      "Epoch 649/2001\n",
      "[C1 valid: -0.965882, C2 fake: 0.038756], [G loss: -0.003152, mse: 0.003405]\n",
      "Epoch 650/2001\n",
      "[C1 valid: -0.970667, C2 fake: 0.033213], [G loss: -0.003088, mse: 0.003557]\n",
      "Epoch 651/2001\n",
      "[C1 valid: -0.969607, C2 fake: 0.031867], [G loss: -0.003014, mse: 0.003830]\n",
      "Epoch 652/2001\n",
      "[C1 valid: -0.971669, C2 fake: 0.032412], [G loss: -0.004212, mse: 0.003512]\n",
      "Epoch 653/2001\n",
      "[C1 valid: -0.968044, C2 fake: 0.036789], [G loss: -0.003315, mse: 0.003804]\n",
      "Epoch 654/2001\n",
      "[C1 valid: -0.971697, C2 fake: 0.031415], [G loss: -0.003670, mse: 0.003807]\n",
      "Epoch 655/2001\n",
      "[C1 valid: -0.970959, C2 fake: 0.029998], [G loss: -0.003349, mse: 0.003584]\n",
      "Epoch 656/2001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder.predict(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "n_points = 900\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//9, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//9, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//9, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//9, n_features), x_input[3])\n",
    "predict5 = np.full((n_points//9, n_features), x_input[4])\n",
    "predict6 = np.full((n_points//9, n_features), x_input[5])\n",
    "predict7 = np.full((n_points//9, n_features), x_input[6])\n",
    "predict8 = np.full((n_points//9, n_features), x_input[7])\n",
    "predict9 = np.full((n_points//9, n_features), x_input[8])\n",
    "\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4, predict5, predict6, predict7, predict8, predict9))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder.predict(aae.encoder(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
