{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=2\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder=network16.build_encoder(Z, nodes, n_features)\n",
    "#print(\"Encoder:\\n\")\n",
    "#encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network16.build_decoder(Z,nodes, n_features)\n",
    "#print(\"Decoder:\\n\")\n",
    "#decoder.summary()\n",
    "\n",
    "discriminator=network16.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "#discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model16\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 10001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model16.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10001\n",
      "[D valid loss: 0.499931],[D fake loss: 0.000000], [G loss(mse): 1.269605, G loss(w): 0.770053]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05019/AAE_Model16.py:200: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_2_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_2_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_2_10001/assets\n",
      "Epoch 2/10001\n",
      "[D valid loss: 0.499738],[D fake loss: 0.000000], [G loss(mse): 1.245900, G loss(w): 0.746880]\n",
      "Epoch 3/10001\n",
      "[D valid loss: 0.499510],[D fake loss: 0.000000], [G loss(mse): 1.284030, G loss(w): 0.785594]\n",
      "Epoch 4/10001\n",
      "[D valid loss: 0.499267],[D fake loss: 0.000000], [G loss(mse): 1.242356, G loss(w): 0.744560]\n",
      "Epoch 5/10001\n",
      "[D valid loss: 0.499020],[D fake loss: 0.000000], [G loss(mse): 1.227640, G loss(w): 0.730501]\n",
      "Epoch 6/10001\n",
      "[D valid loss: 0.498769],[D fake loss: 0.000000], [G loss(mse): 1.243997, G loss(w): 0.747543]\n",
      "Epoch 7/10001\n",
      "[D valid loss: 0.498516],[D fake loss: 0.000000], [G loss(mse): 1.209877, G loss(w): 0.714140]\n",
      "Epoch 8/10001\n",
      "[D valid loss: 0.498262],[D fake loss: 0.000000], [G loss(mse): 1.234865, G loss(w): 0.739863]\n",
      "Epoch 9/10001\n",
      "[D valid loss: 0.498007],[D fake loss: 0.000000], [G loss(mse): 1.196781, G loss(w): 0.702591]\n",
      "Epoch 10/10001\n",
      "[D valid loss: 0.497750],[D fake loss: 0.000000], [G loss(mse): 1.178889, G loss(w): 0.685476]\n",
      "Epoch 11/10001\n",
      "[D valid loss: 0.497490],[D fake loss: 0.000000], [G loss(mse): 1.204680, G loss(w): 0.712148]\n",
      "Epoch 12/10001\n",
      "[D valid loss: 0.497222],[D fake loss: 0.000000], [G loss(mse): 1.203177, G loss(w): 0.711644]\n",
      "Epoch 13/10001\n",
      "[D valid loss: 0.496954],[D fake loss: 0.000000], [G loss(mse): 1.184158, G loss(w): 0.693578]\n",
      "Epoch 14/10001\n",
      "[D valid loss: 0.496685],[D fake loss: 0.000000], [G loss(mse): 1.138536, G loss(w): 0.648857]\n",
      "Epoch 15/10001\n",
      "[D valid loss: 0.496420],[D fake loss: 0.000000], [G loss(mse): 1.155812, G loss(w): 0.666968]\n",
      "Epoch 16/10001\n",
      "[D valid loss: 0.496153],[D fake loss: 0.000000], [G loss(mse): 1.136318, G loss(w): 0.648245]\n",
      "Epoch 17/10001\n",
      "[D valid loss: 0.495887],[D fake loss: 0.000000], [G loss(mse): 1.157454, G loss(w): 0.670161]\n",
      "Epoch 18/10001\n",
      "[D valid loss: 0.495620],[D fake loss: 0.000000], [G loss(mse): 1.123500, G loss(w): 0.636968]\n",
      "Epoch 19/10001\n",
      "[D valid loss: 0.495351],[D fake loss: 0.000000], [G loss(mse): 1.135540, G loss(w): 0.649781]\n",
      "Epoch 20/10001\n",
      "[D valid loss: 0.495082],[D fake loss: 0.000000], [G loss(mse): 1.106447, G loss(w): 0.621435]\n",
      "Epoch 21/10001\n",
      "[D valid loss: 0.494810],[D fake loss: 0.000000], [G loss(mse): 1.126356, G loss(w): 0.642087]\n",
      "Epoch 22/10001\n",
      "[D valid loss: 0.494540],[D fake loss: 0.000000], [G loss(mse): 1.105145, G loss(w): 0.621631]\n",
      "Epoch 23/10001\n",
      "[D valid loss: 0.494266],[D fake loss: 0.000000], [G loss(mse): 1.084388, G loss(w): 0.601662]\n",
      "Epoch 24/10001\n",
      "[D valid loss: 0.493991],[D fake loss: 0.000000], [G loss(mse): 1.090542, G loss(w): 0.608618]\n",
      "Epoch 25/10001\n",
      "[D valid loss: 0.493717],[D fake loss: 0.000000], [G loss(mse): 1.078753, G loss(w): 0.597595]\n",
      "Epoch 26/10001\n",
      "[D valid loss: 0.493439],[D fake loss: 0.000000], [G loss(mse): 1.049073, G loss(w): 0.568677]\n",
      "Epoch 27/10001\n",
      "[D valid loss: 0.493161],[D fake loss: 0.000000], [G loss(mse): 1.080439, G loss(w): 0.600920]\n",
      "Epoch 28/10001\n",
      "[D valid loss: 0.492881],[D fake loss: 0.000000], [G loss(mse): 1.074533, G loss(w): 0.595849]\n",
      "Epoch 29/10001\n",
      "[D valid loss: 0.492599],[D fake loss: 0.000000], [G loss(mse): 1.072447, G loss(w): 0.594696]\n",
      "Epoch 30/10001\n",
      "[D valid loss: 0.492316],[D fake loss: 0.000000], [G loss(mse): 1.053655, G loss(w): 0.576894]\n",
      "Epoch 31/10001\n",
      "[D valid loss: 0.492032],[D fake loss: 0.000000], [G loss(mse): 1.042230, G loss(w): 0.566423]\n",
      "Epoch 32/10001\n",
      "[D valid loss: 0.491743],[D fake loss: 0.000000], [G loss(mse): 1.041564, G loss(w): 0.566726]\n",
      "Epoch 33/10001\n",
      "[D valid loss: 0.491457],[D fake loss: 0.000000], [G loss(mse): 1.031526, G loss(w): 0.557854]\n",
      "Epoch 34/10001\n",
      "[D valid loss: 0.491164],[D fake loss: 0.000000], [G loss(mse): 1.022292, G loss(w): 0.549909]\n",
      "Epoch 35/10001\n",
      "[D valid loss: 0.490870],[D fake loss: 0.000000], [G loss(mse): 1.008026, G loss(w): 0.536703]\n",
      "Epoch 36/10001\n",
      "[D valid loss: 0.490573],[D fake loss: 0.000000], [G loss(mse): 0.983253, G loss(w): 0.513233]\n",
      "Epoch 37/10001\n",
      "[D valid loss: 0.490266],[D fake loss: 0.000000], [G loss(mse): 1.014901, G loss(w): 0.546455]\n",
      "Epoch 38/10001\n",
      "[D valid loss: 0.489965],[D fake loss: 0.000000], [G loss(mse): 1.002013, G loss(w): 0.534903]\n",
      "Epoch 39/10001\n",
      "[D valid loss: 0.489646],[D fake loss: 0.000000], [G loss(mse): 1.008638, G loss(w): 0.543243]\n",
      "Epoch 40/10001\n",
      "[D valid loss: 0.489312],[D fake loss: 0.000000], [G loss(mse): 0.982033, G loss(w): 0.518323]\n",
      "Epoch 41/10001\n",
      "[D valid loss: 0.488954],[D fake loss: 0.000000], [G loss(mse): 0.972401, G loss(w): 0.510901]\n",
      "Epoch 42/10001\n",
      "[D valid loss: 0.488597],[D fake loss: 0.000000], [G loss(mse): 0.968888, G loss(w): 0.509291]\n",
      "Epoch 43/10001\n",
      "[D valid loss: 0.488188],[D fake loss: 0.000000], [G loss(mse): 0.951090, G loss(w): 0.493524]\n",
      "Epoch 44/10001\n",
      "[D valid loss: 0.487829],[D fake loss: 0.000000], [G loss(mse): 0.963036, G loss(w): 0.507316]\n",
      "Epoch 45/10001\n",
      "[D valid loss: 0.487485],[D fake loss: 0.000000], [G loss(mse): 0.938346, G loss(w): 0.483937]\n",
      "Epoch 46/10001\n",
      "[D valid loss: 0.487143],[D fake loss: 0.000000], [G loss(mse): 0.965974, G loss(w): 0.512789]\n",
      "Epoch 47/10001\n",
      "[D valid loss: 0.486787],[D fake loss: 0.000000], [G loss(mse): 0.954235, G loss(w): 0.502267]\n",
      "Epoch 48/10001\n",
      "[D valid loss: 0.486433],[D fake loss: 0.000000], [G loss(mse): 0.928025, G loss(w): 0.477476]\n",
      "Epoch 49/10001\n",
      "[D valid loss: 0.486086],[D fake loss: 0.000000], [G loss(mse): 0.935204, G loss(w): 0.485422]\n",
      "Epoch 50/10001\n",
      "[D valid loss: 0.485765],[D fake loss: 0.000000], [G loss(mse): 0.947045, G loss(w): 0.499090]\n",
      "Epoch 51/10001\n",
      "[D valid loss: 0.485416],[D fake loss: 0.000000], [G loss(mse): 0.906132, G loss(w): 0.458123]\n",
      "Epoch 52/10001\n",
      "[D valid loss: 0.485055],[D fake loss: 0.000000], [G loss(mse): 0.937959, G loss(w): 0.490495]\n",
      "Epoch 53/10001\n",
      "[D valid loss: 0.484726],[D fake loss: 0.000000], [G loss(mse): 0.895445, G loss(w): 0.448533]\n",
      "Epoch 54/10001\n",
      "[D valid loss: 0.484376],[D fake loss: 0.000000], [G loss(mse): 0.873965, G loss(w): 0.427773]\n",
      "Epoch 55/10001\n",
      "[D valid loss: 0.484023],[D fake loss: 0.000000], [G loss(mse): 0.921034, G loss(w): 0.476128]\n",
      "Epoch 56/10001\n",
      "[D valid loss: 0.483703],[D fake loss: 0.000000], [G loss(mse): 0.898951, G loss(w): 0.454899]\n",
      "Epoch 57/10001\n",
      "[D valid loss: 0.483357],[D fake loss: 0.000000], [G loss(mse): 0.901302, G loss(w): 0.458262]\n",
      "Epoch 58/10001\n",
      "[D valid loss: 0.483001],[D fake loss: 0.000000], [G loss(mse): 0.897105, G loss(w): 0.454449]\n",
      "Epoch 59/10001\n",
      "[D valid loss: 0.482654],[D fake loss: 0.000000], [G loss(mse): 0.880890, G loss(w): 0.439259]\n",
      "Epoch 60/10001\n",
      "[D valid loss: 0.482329],[D fake loss: 0.000000], [G loss(mse): 0.883768, G loss(w): 0.442473]\n",
      "Epoch 61/10001\n",
      "[D valid loss: 0.482016],[D fake loss: 0.000000], [G loss(mse): 0.887608, G loss(w): 0.445203]\n",
      "Epoch 62/10001\n",
      "[D valid loss: 0.481632],[D fake loss: 0.000000], [G loss(mse): 0.889177, G loss(w): 0.450135]\n",
      "Epoch 63/10001\n",
      "[D valid loss: 0.481280],[D fake loss: 0.000000], [G loss(mse): 0.845474, G loss(w): 0.405509]\n",
      "Epoch 64/10001\n",
      "[D valid loss: 0.480987],[D fake loss: 0.000000], [G loss(mse): 0.870222, G loss(w): 0.428495]\n",
      "Epoch 65/10001\n",
      "[D valid loss: 0.480588],[D fake loss: 0.000000], [G loss(mse): 0.848578, G loss(w): 0.405901]\n",
      "Epoch 66/10001\n",
      "[D valid loss: 0.480223],[D fake loss: 0.000000], [G loss(mse): 0.847183, G loss(w): 0.406435]\n",
      "Epoch 67/10001\n",
      "[D valid loss: 0.479908],[D fake loss: 0.000000], [G loss(mse): 0.859066, G loss(w): 0.420589]\n",
      "Epoch 68/10001\n",
      "[D valid loss: 0.479544],[D fake loss: 0.000000], [G loss(mse): 0.853544, G loss(w): 0.413925]\n",
      "Epoch 69/10001\n",
      "[D valid loss: 0.479179],[D fake loss: 0.000000], [G loss(mse): 0.862173, G loss(w): 0.422605]\n",
      "Epoch 70/10001\n",
      "[D valid loss: 0.478861],[D fake loss: 0.000000], [G loss(mse): 0.828946, G loss(w): 0.390470]\n",
      "Epoch 71/10001\n",
      "[D valid loss: 0.478520],[D fake loss: 0.000000], [G loss(mse): 0.831446, G loss(w): 0.389547]\n",
      "Epoch 72/10001\n",
      "[D valid loss: 0.478151],[D fake loss: 0.000000], [G loss(mse): 0.853606, G loss(w): 0.417239]\n",
      "Epoch 73/10001\n",
      "[D valid loss: 0.477783],[D fake loss: 0.000000], [G loss(mse): 0.812957, G loss(w): 0.375407]\n",
      "Epoch 74/10001\n",
      "[D valid loss: 0.477419],[D fake loss: 0.000000], [G loss(mse): 0.808949, G loss(w): 0.369864]\n",
      "Epoch 75/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.477049],[D fake loss: 0.000000], [G loss(mse): 0.840207, G loss(w): 0.403515]\n",
      "Epoch 76/10001\n",
      "[D valid loss: 0.476696],[D fake loss: 0.000000], [G loss(mse): 0.833588, G loss(w): 0.392977]\n",
      "Epoch 77/10001\n",
      "[D valid loss: 0.476328],[D fake loss: 0.000000], [G loss(mse): 0.817432, G loss(w): 0.382224]\n",
      "Epoch 78/10001\n",
      "[D valid loss: 0.476036],[D fake loss: 0.000000], [G loss(mse): 0.804777, G loss(w): 0.369567]\n",
      "Epoch 79/10001\n",
      "[D valid loss: 0.475624],[D fake loss: 0.000000], [G loss(mse): 0.818016, G loss(w): 0.381425]\n",
      "Epoch 80/10001\n",
      "[D valid loss: 0.475246],[D fake loss: 0.000000], [G loss(mse): 0.807470, G loss(w): 0.372753]\n",
      "Epoch 81/10001\n",
      "[D valid loss: 0.474915],[D fake loss: 0.000000], [G loss(mse): 0.805508, G loss(w): 0.371047]\n",
      "Epoch 82/10001\n",
      "[D valid loss: 0.474551],[D fake loss: 0.000000], [G loss(mse): 0.795999, G loss(w): 0.357828]\n",
      "Epoch 83/10001\n",
      "[D valid loss: 0.474156],[D fake loss: 0.000000], [G loss(mse): 0.771299, G loss(w): 0.336086]\n",
      "Epoch 84/10001\n",
      "[D valid loss: 0.473828],[D fake loss: 0.000000], [G loss(mse): 0.791767, G loss(w): 0.355469]\n",
      "Epoch 85/10001\n",
      "[D valid loss: 0.473441],[D fake loss: 0.000000], [G loss(mse): 0.801406, G loss(w): 0.364691]\n",
      "Epoch 86/10001\n",
      "[D valid loss: 0.473102],[D fake loss: 0.000000], [G loss(mse): 0.806034, G loss(w): 0.373363]\n",
      "Epoch 87/10001\n",
      "[D valid loss: 0.472673],[D fake loss: 0.000000], [G loss(mse): 0.780937, G loss(w): 0.344708]\n",
      "Epoch 88/10001\n",
      "[D valid loss: 0.472307],[D fake loss: 0.000000], [G loss(mse): 0.779276, G loss(w): 0.345091]\n",
      "Epoch 89/10001\n",
      "[D valid loss: 0.471970],[D fake loss: 0.000000], [G loss(mse): 0.794340, G loss(w): 0.361143]\n",
      "Epoch 90/10001\n",
      "[D valid loss: 0.471601],[D fake loss: 0.000000], [G loss(mse): 0.776031, G loss(w): 0.344213]\n",
      "Epoch 91/10001\n",
      "[D valid loss: 0.471186],[D fake loss: 0.000000], [G loss(mse): 0.776485, G loss(w): 0.342191]\n",
      "Epoch 92/10001\n",
      "[D valid loss: 0.470810],[D fake loss: 0.000000], [G loss(mse): 0.765462, G loss(w): 0.335541]\n",
      "Epoch 93/10001\n",
      "[D valid loss: 0.470439],[D fake loss: 0.000000], [G loss(mse): 0.784992, G loss(w): 0.349368]\n",
      "Epoch 94/10001\n",
      "[D valid loss: 0.470096],[D fake loss: 0.000000], [G loss(mse): 0.772007, G loss(w): 0.342085]\n",
      "Epoch 95/10001\n",
      "[D valid loss: 0.469700],[D fake loss: 0.000000], [G loss(mse): 0.762474, G loss(w): 0.332742]\n",
      "Epoch 96/10001\n",
      "[D valid loss: 0.469336],[D fake loss: 0.000000], [G loss(mse): 0.756853, G loss(w): 0.327639]\n",
      "Epoch 97/10001\n",
      "[D valid loss: 0.468930],[D fake loss: 0.000000], [G loss(mse): 0.755072, G loss(w): 0.325803]\n",
      "Epoch 98/10001\n",
      "[D valid loss: 0.468546],[D fake loss: 0.000000], [G loss(mse): 0.750938, G loss(w): 0.317809]\n",
      "Epoch 99/10001\n",
      "[D valid loss: 0.468197],[D fake loss: 0.000000], [G loss(mse): 0.744884, G loss(w): 0.316872]\n",
      "Epoch 100/10001\n",
      "[D valid loss: 0.467781],[D fake loss: 0.000000], [G loss(mse): 0.759343, G loss(w): 0.332738]\n",
      "Epoch 101/10001\n",
      "[D valid loss: 0.467400],[D fake loss: 0.000000], [G loss(mse): 0.757850, G loss(w): 0.329414]\n",
      "Epoch 102/10001\n",
      "[D valid loss: 0.467014],[D fake loss: 0.000000], [G loss(mse): 0.770892, G loss(w): 0.345763]\n",
      "Epoch 103/10001\n",
      "[D valid loss: 0.466606],[D fake loss: 0.000000], [G loss(mse): 0.744536, G loss(w): 0.317229]\n",
      "Epoch 104/10001\n",
      "[D valid loss: 0.466229],[D fake loss: 0.000000], [G loss(mse): 0.758844, G loss(w): 0.329928]\n",
      "Epoch 105/10001\n",
      "[D valid loss: 0.465842],[D fake loss: 0.000000], [G loss(mse): 0.741999, G loss(w): 0.313323]\n",
      "Epoch 106/10001\n",
      "[D valid loss: 0.465456],[D fake loss: 0.000000], [G loss(mse): 0.727713, G loss(w): 0.301681]\n",
      "Epoch 107/10001\n",
      "[D valid loss: 0.465030],[D fake loss: 0.000000], [G loss(mse): 0.731041, G loss(w): 0.306508]\n",
      "Epoch 108/10001\n",
      "[D valid loss: 0.464691],[D fake loss: 0.000000], [G loss(mse): 0.741301, G loss(w): 0.314889]\n",
      "Epoch 109/10001\n",
      "[D valid loss: 0.464251],[D fake loss: 0.000000], [G loss(mse): 0.738203, G loss(w): 0.313301]\n",
      "Epoch 110/10001\n",
      "[D valid loss: 0.463854],[D fake loss: 0.000000], [G loss(mse): 0.732980, G loss(w): 0.309846]\n",
      "Epoch 111/10001\n",
      "[D valid loss: 0.463476],[D fake loss: 0.000000], [G loss(mse): 0.743839, G loss(w): 0.314577]\n",
      "Epoch 112/10001\n",
      "[D valid loss: 0.463074],[D fake loss: 0.000000], [G loss(mse): 0.720637, G loss(w): 0.294386]\n",
      "Epoch 113/10001\n",
      "[D valid loss: 0.462646],[D fake loss: 0.000000], [G loss(mse): 0.716740, G loss(w): 0.291762]\n",
      "Epoch 114/10001\n",
      "[D valid loss: 0.462328],[D fake loss: 0.000000], [G loss(mse): 0.725265, G loss(w): 0.305405]\n",
      "Epoch 115/10001\n",
      "[D valid loss: 0.461918],[D fake loss: 0.000000], [G loss(mse): 0.731655, G loss(w): 0.310808]\n",
      "Epoch 116/10001\n",
      "[D valid loss: 0.461467],[D fake loss: 0.000000], [G loss(mse): 0.723751, G loss(w): 0.301948]\n",
      "Epoch 117/10001\n",
      "[D valid loss: 0.461137],[D fake loss: 0.000000], [G loss(mse): 0.733252, G loss(w): 0.312115]\n",
      "Epoch 118/10001\n",
      "[D valid loss: 0.460646],[D fake loss: 0.000000], [G loss(mse): 0.713643, G loss(w): 0.292327]\n",
      "Epoch 119/10001\n",
      "[D valid loss: 0.460314],[D fake loss: 0.000000], [G loss(mse): 0.719321, G loss(w): 0.296793]\n",
      "Epoch 120/10001\n",
      "[D valid loss: 0.459874],[D fake loss: 0.000000], [G loss(mse): 0.717868, G loss(w): 0.297772]\n",
      "Epoch 121/10001\n",
      "[D valid loss: 0.459466],[D fake loss: 0.000000], [G loss(mse): 0.707638, G loss(w): 0.288502]\n",
      "Epoch 122/10001\n",
      "[D valid loss: 0.459087],[D fake loss: 0.000000], [G loss(mse): 0.706248, G loss(w): 0.287821]\n",
      "Epoch 123/10001\n",
      "[D valid loss: 0.458690],[D fake loss: 0.000000], [G loss(mse): 0.705584, G loss(w): 0.284336]\n",
      "Epoch 124/10001\n",
      "[D valid loss: 0.458211],[D fake loss: 0.000000], [G loss(mse): 0.714643, G loss(w): 0.297106]\n",
      "Epoch 125/10001\n",
      "[D valid loss: 0.457837],[D fake loss: 0.000000], [G loss(mse): 0.703262, G loss(w): 0.287433]\n",
      "Epoch 126/10001\n",
      "[D valid loss: 0.457365],[D fake loss: 0.000000], [G loss(mse): 0.719939, G loss(w): 0.302546]\n",
      "Epoch 127/10001\n",
      "[D valid loss: 0.457013],[D fake loss: 0.000000], [G loss(mse): 0.693910, G loss(w): 0.276903]\n",
      "Epoch 128/10001\n",
      "[D valid loss: 0.456585],[D fake loss: 0.000000], [G loss(mse): 0.696695, G loss(w): 0.284241]\n",
      "Epoch 129/10001\n",
      "[D valid loss: 0.456201],[D fake loss: 0.000000], [G loss(mse): 0.683511, G loss(w): 0.266414]\n",
      "Epoch 130/10001\n",
      "[D valid loss: 0.455777],[D fake loss: 0.000000], [G loss(mse): 0.693031, G loss(w): 0.276020]\n",
      "Epoch 131/10001\n",
      "[D valid loss: 0.455334],[D fake loss: 0.000000], [G loss(mse): 0.690397, G loss(w): 0.276418]\n",
      "Epoch 132/10001\n",
      "[D valid loss: 0.454956],[D fake loss: 0.000000], [G loss(mse): 0.699389, G loss(w): 0.285714]\n",
      "Epoch 133/10001\n",
      "[D valid loss: 0.454503],[D fake loss: 0.000000], [G loss(mse): 0.699007, G loss(w): 0.286004]\n",
      "Epoch 134/10001\n",
      "[D valid loss: 0.454090],[D fake loss: 0.000000], [G loss(mse): 0.692843, G loss(w): 0.280140]\n",
      "Epoch 135/10001\n",
      "[D valid loss: 0.453667],[D fake loss: 0.000000], [G loss(mse): 0.684142, G loss(w): 0.272461]\n",
      "Epoch 136/10001\n",
      "[D valid loss: 0.453227],[D fake loss: 0.000000], [G loss(mse): 0.682708, G loss(w): 0.266795]\n",
      "Epoch 137/10001\n",
      "[D valid loss: 0.452817],[D fake loss: 0.000000], [G loss(mse): 0.697389, G loss(w): 0.286456]\n",
      "Epoch 138/10001\n",
      "[D valid loss: 0.452471],[D fake loss: 0.000000], [G loss(mse): 0.682560, G loss(w): 0.273200]\n",
      "Epoch 139/10001\n",
      "[D valid loss: 0.451985],[D fake loss: 0.000000], [G loss(mse): 0.682842, G loss(w): 0.272397]\n",
      "Epoch 140/10001\n",
      "[D valid loss: 0.451547],[D fake loss: 0.000000], [G loss(mse): 0.681180, G loss(w): 0.265298]\n",
      "Epoch 141/10001\n",
      "[D valid loss: 0.451094],[D fake loss: 0.000000], [G loss(mse): 0.687833, G loss(w): 0.277919]\n",
      "Epoch 142/10001\n",
      "[D valid loss: 0.450656],[D fake loss: 0.000000], [G loss(mse): 0.693374, G loss(w): 0.281124]\n",
      "Epoch 143/10001\n",
      "[D valid loss: 0.450340],[D fake loss: 0.000000], [G loss(mse): 0.681961, G loss(w): 0.271191]\n",
      "Epoch 144/10001\n",
      "[D valid loss: 0.449846],[D fake loss: 0.000000], [G loss(mse): 0.675197, G loss(w): 0.265673]\n",
      "Epoch 145/10001\n",
      "[D valid loss: 0.449425],[D fake loss: 0.000000], [G loss(mse): 0.660181, G loss(w): 0.248883]\n",
      "Epoch 146/10001\n",
      "[D valid loss: 0.448994],[D fake loss: 0.000000], [G loss(mse): 0.665725, G loss(w): 0.258682]\n",
      "Epoch 147/10001\n",
      "[D valid loss: 0.448567],[D fake loss: 0.000000], [G loss(mse): 0.672034, G loss(w): 0.266436]\n",
      "Epoch 148/10001\n",
      "[D valid loss: 0.448140],[D fake loss: 0.000000], [G loss(mse): 0.661360, G loss(w): 0.256408]\n",
      "Epoch 149/10001\n",
      "[D valid loss: 0.447758],[D fake loss: 0.000000], [G loss(mse): 0.671009, G loss(w): 0.263758]\n",
      "Epoch 150/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.447239],[D fake loss: 0.000000], [G loss(mse): 0.655570, G loss(w): 0.249422]\n",
      "Epoch 151/10001\n",
      "[D valid loss: 0.446818],[D fake loss: 0.000000], [G loss(mse): 0.663956, G loss(w): 0.260470]\n",
      "Epoch 152/10001\n",
      "[D valid loss: 0.446399],[D fake loss: 0.000000], [G loss(mse): 0.660656, G loss(w): 0.256892]\n",
      "Epoch 153/10001\n",
      "[D valid loss: 0.445952],[D fake loss: 0.000000], [G loss(mse): 0.655389, G loss(w): 0.252971]\n",
      "Epoch 154/10001\n",
      "[D valid loss: 0.445510],[D fake loss: 0.000000], [G loss(mse): 0.662400, G loss(w): 0.259037]\n",
      "Epoch 155/10001\n",
      "[D valid loss: 0.445176],[D fake loss: 0.000000], [G loss(mse): 0.650793, G loss(w): 0.248719]\n",
      "Epoch 156/10001\n",
      "[D valid loss: 0.444694],[D fake loss: 0.000000], [G loss(mse): 0.651903, G loss(w): 0.252826]\n",
      "Epoch 157/10001\n",
      "[D valid loss: 0.444249],[D fake loss: 0.000000], [G loss(mse): 0.659755, G loss(w): 0.262526]\n",
      "Epoch 158/10001\n",
      "[D valid loss: 0.443808],[D fake loss: 0.000000], [G loss(mse): 0.670842, G loss(w): 0.270548]\n",
      "Epoch 159/10001\n",
      "[D valid loss: 0.443366],[D fake loss: 0.000000], [G loss(mse): 0.646678, G loss(w): 0.251187]\n",
      "Epoch 160/10001\n",
      "[D valid loss: 0.442932],[D fake loss: 0.000000], [G loss(mse): 0.655447, G loss(w): 0.259059]\n",
      "Epoch 161/10001\n",
      "[D valid loss: 0.442455],[D fake loss: 0.000000], [G loss(mse): 0.649195, G loss(w): 0.252280]\n",
      "Epoch 162/10001\n",
      "[D valid loss: 0.442021],[D fake loss: 0.000000], [G loss(mse): 0.654186, G loss(w): 0.257272]\n",
      "Epoch 163/10001\n",
      "[D valid loss: 0.441586],[D fake loss: 0.000000], [G loss(mse): 0.642064, G loss(w): 0.242827]\n",
      "Epoch 164/10001\n",
      "[D valid loss: 0.441137],[D fake loss: 0.000000], [G loss(mse): 0.648586, G loss(w): 0.249957]\n",
      "Epoch 165/10001\n",
      "[D valid loss: 0.440721],[D fake loss: 0.000000], [G loss(mse): 0.652665, G loss(w): 0.254396]\n",
      "Epoch 166/10001\n",
      "[D valid loss: 0.440216],[D fake loss: 0.000000], [G loss(mse): 0.648751, G loss(w): 0.253728]\n",
      "Epoch 167/10001\n",
      "[D valid loss: 0.439793],[D fake loss: 0.000000], [G loss(mse): 0.645311, G loss(w): 0.245149]\n",
      "Epoch 168/10001\n",
      "[D valid loss: 0.439392],[D fake loss: 0.000000], [G loss(mse): 0.650999, G loss(w): 0.252024]\n",
      "Epoch 169/10001\n",
      "[D valid loss: 0.438920],[D fake loss: 0.000000], [G loss(mse): 0.644059, G loss(w): 0.244151]\n",
      "Epoch 170/10001\n",
      "[D valid loss: 0.438515],[D fake loss: 0.000000], [G loss(mse): 0.639708, G loss(w): 0.245642]\n",
      "Epoch 171/10001\n",
      "[D valid loss: 0.438025],[D fake loss: 0.000000], [G loss(mse): 0.651089, G loss(w): 0.253976]\n",
      "Epoch 172/10001\n",
      "[D valid loss: 0.437676],[D fake loss: 0.000000], [G loss(mse): 0.632276, G loss(w): 0.239227]\n",
      "Epoch 173/10001\n",
      "[D valid loss: 0.437152],[D fake loss: 0.000000], [G loss(mse): 0.635750, G loss(w): 0.237698]\n",
      "Epoch 174/10001\n",
      "[D valid loss: 0.436683],[D fake loss: 0.000000], [G loss(mse): 0.634773, G loss(w): 0.241504]\n",
      "Epoch 175/10001\n",
      "[D valid loss: 0.436234],[D fake loss: 0.000000], [G loss(mse): 0.637172, G loss(w): 0.242048]\n",
      "Epoch 176/10001\n",
      "[D valid loss: 0.435792],[D fake loss: 0.000000], [G loss(mse): 0.626192, G loss(w): 0.232016]\n",
      "Epoch 177/10001\n",
      "[D valid loss: 0.435261],[D fake loss: 0.000000], [G loss(mse): 0.634875, G loss(w): 0.246417]\n",
      "Epoch 178/10001\n",
      "[D valid loss: 0.434960],[D fake loss: 0.000000], [G loss(mse): 0.616290, G loss(w): 0.230891]\n",
      "Epoch 179/10001\n",
      "[D valid loss: 0.434466],[D fake loss: 0.000000], [G loss(mse): 0.632211, G loss(w): 0.241960]\n",
      "Epoch 180/10001\n",
      "[D valid loss: 0.433950],[D fake loss: 0.000000], [G loss(mse): 0.615736, G loss(w): 0.228466]\n",
      "Epoch 181/10001\n",
      "[D valid loss: 0.433554],[D fake loss: 0.000000], [G loss(mse): 0.621050, G loss(w): 0.231173]\n",
      "Epoch 182/10001\n",
      "[D valid loss: 0.432996],[D fake loss: 0.000000], [G loss(mse): 0.639159, G loss(w): 0.248644]\n",
      "Epoch 183/10001\n",
      "[D valid loss: 0.432612],[D fake loss: 0.000000], [G loss(mse): 0.619569, G loss(w): 0.229431]\n",
      "Epoch 184/10001\n",
      "[D valid loss: 0.432119],[D fake loss: 0.000000], [G loss(mse): 0.632579, G loss(w): 0.242125]\n",
      "Epoch 185/10001\n",
      "[D valid loss: 0.431624],[D fake loss: 0.000000], [G loss(mse): 0.615201, G loss(w): 0.226389]\n",
      "Epoch 186/10001\n",
      "[D valid loss: 0.431263],[D fake loss: 0.000000], [G loss(mse): 0.614692, G loss(w): 0.229003]\n",
      "Epoch 187/10001\n",
      "[D valid loss: 0.430770],[D fake loss: 0.000000], [G loss(mse): 0.612320, G loss(w): 0.226029]\n",
      "Epoch 188/10001\n",
      "[D valid loss: 0.430293],[D fake loss: 0.000000], [G loss(mse): 0.607659, G loss(w): 0.224503]\n",
      "Epoch 189/10001\n",
      "[D valid loss: 0.429857],[D fake loss: 0.000000], [G loss(mse): 0.618089, G loss(w): 0.233616]\n",
      "Epoch 190/10001\n",
      "[D valid loss: 0.429418],[D fake loss: 0.000000], [G loss(mse): 0.619131, G loss(w): 0.231057]\n",
      "Epoch 191/10001\n",
      "[D valid loss: 0.428866],[D fake loss: 0.000000], [G loss(mse): 0.627777, G loss(w): 0.240147]\n",
      "Epoch 192/10001\n",
      "[D valid loss: 0.428654],[D fake loss: 0.000000], [G loss(mse): 0.611065, G loss(w): 0.222908]\n",
      "Epoch 193/10001\n",
      "[D valid loss: 0.428018],[D fake loss: 0.000000], [G loss(mse): 0.607476, G loss(w): 0.225675]\n",
      "Epoch 194/10001\n",
      "[D valid loss: 0.427555],[D fake loss: 0.000000], [G loss(mse): 0.609928, G loss(w): 0.228076]\n",
      "Epoch 195/10001\n",
      "[D valid loss: 0.427190],[D fake loss: 0.000000], [G loss(mse): 0.598960, G loss(w): 0.218449]\n",
      "Epoch 196/10001\n",
      "[D valid loss: 0.426684],[D fake loss: 0.000000], [G loss(mse): 0.612508, G loss(w): 0.233754]\n",
      "Epoch 197/10001\n",
      "[D valid loss: 0.426235],[D fake loss: 0.000000], [G loss(mse): 0.598061, G loss(w): 0.214815]\n",
      "Epoch 198/10001\n",
      "[D valid loss: 0.425796],[D fake loss: 0.000000], [G loss(mse): 0.607856, G loss(w): 0.225912]\n",
      "Epoch 199/10001\n",
      "[D valid loss: 0.425249],[D fake loss: 0.000000], [G loss(mse): 0.601355, G loss(w): 0.222684]\n",
      "Epoch 200/10001\n",
      "[D valid loss: 0.424750],[D fake loss: 0.000000], [G loss(mse): 0.609040, G loss(w): 0.231430]\n",
      "Epoch 201/10001\n",
      "[D valid loss: 0.424326],[D fake loss: 0.000000], [G loss(mse): 0.605670, G loss(w): 0.229739]\n",
      "Epoch 202/10001\n",
      "[D valid loss: 0.423940],[D fake loss: 0.000000], [G loss(mse): 0.596559, G loss(w): 0.220863]\n",
      "Epoch 203/10001\n",
      "[D valid loss: 0.423519],[D fake loss: 0.000000], [G loss(mse): 0.591877, G loss(w): 0.217723]\n",
      "Epoch 204/10001\n",
      "[D valid loss: 0.422972],[D fake loss: 0.000000], [G loss(mse): 0.595258, G loss(w): 0.222031]\n",
      "Epoch 205/10001\n",
      "[D valid loss: 0.422510],[D fake loss: 0.000000], [G loss(mse): 0.599584, G loss(w): 0.223373]\n",
      "Epoch 206/10001\n",
      "[D valid loss: 0.422012],[D fake loss: 0.000000], [G loss(mse): 0.588594, G loss(w): 0.211566]\n",
      "Epoch 207/10001\n",
      "[D valid loss: 0.421531],[D fake loss: 0.000000], [G loss(mse): 0.606637, G loss(w): 0.232338]\n",
      "Epoch 208/10001\n",
      "[D valid loss: 0.421046],[D fake loss: 0.000000], [G loss(mse): 0.613354, G loss(w): 0.237736]\n",
      "Epoch 209/10001\n",
      "[D valid loss: 0.420581],[D fake loss: 0.000000], [G loss(mse): 0.592508, G loss(w): 0.220427]\n",
      "Epoch 210/10001\n",
      "[D valid loss: 0.420266],[D fake loss: 0.000000], [G loss(mse): 0.595240, G loss(w): 0.220262]\n",
      "Epoch 211/10001\n",
      "[D valid loss: 0.419750],[D fake loss: 0.000000], [G loss(mse): 0.577128, G loss(w): 0.209813]\n",
      "Epoch 212/10001\n",
      "[D valid loss: 0.419243],[D fake loss: 0.000000], [G loss(mse): 0.580522, G loss(w): 0.210826]\n",
      "Epoch 213/10001\n",
      "[D valid loss: 0.418758],[D fake loss: 0.000000], [G loss(mse): 0.585145, G loss(w): 0.219116]\n",
      "Epoch 214/10001\n",
      "[D valid loss: 0.418312],[D fake loss: 0.000000], [G loss(mse): 0.585164, G loss(w): 0.217125]\n",
      "Epoch 215/10001\n",
      "[D valid loss: 0.417713],[D fake loss: 0.000000], [G loss(mse): 0.581874, G loss(w): 0.218655]\n",
      "Epoch 216/10001\n",
      "[D valid loss: 0.417351],[D fake loss: 0.000000], [G loss(mse): 0.579598, G loss(w): 0.213794]\n",
      "Epoch 217/10001\n",
      "[D valid loss: 0.416987],[D fake loss: 0.000000], [G loss(mse): 0.578373, G loss(w): 0.208741]\n",
      "Epoch 218/10001\n",
      "[D valid loss: 0.416360],[D fake loss: 0.000000], [G loss(mse): 0.582901, G loss(w): 0.219138]\n",
      "Epoch 219/10001\n",
      "[D valid loss: 0.415873],[D fake loss: 0.000000], [G loss(mse): 0.587030, G loss(w): 0.219450]\n",
      "Epoch 220/10001\n",
      "[D valid loss: 0.415444],[D fake loss: 0.000000], [G loss(mse): 0.574116, G loss(w): 0.208068]\n",
      "Epoch 221/10001\n",
      "[D valid loss: 0.415011],[D fake loss: 0.000000], [G loss(mse): 0.581604, G loss(w): 0.212459]\n",
      "Epoch 222/10001\n",
      "[D valid loss: 0.414534],[D fake loss: 0.000000], [G loss(mse): 0.573148, G loss(w): 0.210645]\n",
      "Epoch 223/10001\n",
      "[D valid loss: 0.414010],[D fake loss: 0.000000], [G loss(mse): 0.573720, G loss(w): 0.208249]\n",
      "Epoch 224/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.413498],[D fake loss: 0.000000], [G loss(mse): 0.572642, G loss(w): 0.207941]\n",
      "Epoch 225/10001\n",
      "[D valid loss: 0.413109],[D fake loss: 0.000000], [G loss(mse): 0.563806, G loss(w): 0.201362]\n",
      "Epoch 226/10001\n",
      "[D valid loss: 0.412493],[D fake loss: 0.000000], [G loss(mse): 0.575160, G loss(w): 0.208948]\n",
      "Epoch 227/10001\n",
      "[D valid loss: 0.412070],[D fake loss: 0.000000], [G loss(mse): 0.570460, G loss(w): 0.206833]\n",
      "Epoch 228/10001\n",
      "[D valid loss: 0.411619],[D fake loss: 0.000000], [G loss(mse): 0.564580, G loss(w): 0.197469]\n",
      "Epoch 229/10001\n",
      "[D valid loss: 0.411149],[D fake loss: 0.000000], [G loss(mse): 0.581450, G loss(w): 0.212972]\n",
      "Epoch 230/10001\n",
      "[D valid loss: 0.410647],[D fake loss: 0.000000], [G loss(mse): 0.571578, G loss(w): 0.210097]\n",
      "Epoch 231/10001\n",
      "[D valid loss: 0.410128],[D fake loss: 0.000000], [G loss(mse): 0.573441, G loss(w): 0.211450]\n",
      "Epoch 232/10001\n",
      "[D valid loss: 0.409654],[D fake loss: 0.000000], [G loss(mse): 0.565656, G loss(w): 0.203498]\n",
      "Epoch 233/10001\n",
      "[D valid loss: 0.409197],[D fake loss: 0.000000], [G loss(mse): 0.573696, G loss(w): 0.211205]\n",
      "Epoch 234/10001\n",
      "[D valid loss: 0.408838],[D fake loss: 0.000000], [G loss(mse): 0.570062, G loss(w): 0.211925]\n",
      "Epoch 235/10001\n",
      "[D valid loss: 0.408447],[D fake loss: 0.000000], [G loss(mse): 0.561608, G loss(w): 0.202241]\n",
      "Epoch 236/10001\n",
      "[D valid loss: 0.407762],[D fake loss: 0.000000], [G loss(mse): 0.560303, G loss(w): 0.196193]\n",
      "Epoch 237/10001\n",
      "[D valid loss: 0.407304],[D fake loss: 0.000000], [G loss(mse): 0.560429, G loss(w): 0.198066]\n",
      "Epoch 238/10001\n",
      "[D valid loss: 0.406754],[D fake loss: 0.000000], [G loss(mse): 0.560187, G loss(w): 0.199582]\n",
      "Epoch 239/10001\n",
      "[D valid loss: 0.406360],[D fake loss: 0.000000], [G loss(mse): 0.555925, G loss(w): 0.188437]\n",
      "Epoch 240/10001\n",
      "[D valid loss: 0.405889],[D fake loss: 0.000000], [G loss(mse): 0.555878, G loss(w): 0.199863]\n",
      "Epoch 241/10001\n",
      "[D valid loss: 0.405280],[D fake loss: 0.000000], [G loss(mse): 0.561188, G loss(w): 0.198884]\n",
      "Epoch 242/10001\n",
      "[D valid loss: 0.404868],[D fake loss: 0.000000], [G loss(mse): 0.551863, G loss(w): 0.193419]\n",
      "Epoch 243/10001\n",
      "[D valid loss: 0.404449],[D fake loss: 0.000000], [G loss(mse): 0.556178, G loss(w): 0.201285]\n",
      "Epoch 244/10001\n",
      "[D valid loss: 0.403949],[D fake loss: 0.000000], [G loss(mse): 0.552591, G loss(w): 0.195896]\n",
      "Epoch 245/10001\n",
      "[D valid loss: 0.403427],[D fake loss: 0.000000], [G loss(mse): 0.554663, G loss(w): 0.197288]\n",
      "Epoch 246/10001\n",
      "[D valid loss: 0.402967],[D fake loss: 0.000000], [G loss(mse): 0.551789, G loss(w): 0.194377]\n",
      "Epoch 247/10001\n",
      "[D valid loss: 0.402407],[D fake loss: 0.000000], [G loss(mse): 0.548711, G loss(w): 0.195287]\n",
      "Epoch 248/10001\n",
      "[D valid loss: 0.401953],[D fake loss: 0.000000], [G loss(mse): 0.545096, G loss(w): 0.189398]\n",
      "Epoch 249/10001\n",
      "[D valid loss: 0.401483],[D fake loss: 0.000000], [G loss(mse): 0.555561, G loss(w): 0.199992]\n",
      "Epoch 250/10001\n",
      "[D valid loss: 0.401028],[D fake loss: 0.000000], [G loss(mse): 0.550040, G loss(w): 0.196558]\n",
      "Epoch 251/10001\n",
      "[D valid loss: 0.400622],[D fake loss: 0.000000], [G loss(mse): 0.546511, G loss(w): 0.190278]\n",
      "Epoch 252/10001\n",
      "[D valid loss: 0.400122],[D fake loss: 0.000000], [G loss(mse): 0.556341, G loss(w): 0.203345]\n",
      "Epoch 253/10001\n",
      "[D valid loss: 0.399526],[D fake loss: 0.000000], [G loss(mse): 0.542668, G loss(w): 0.187865]\n",
      "Epoch 254/10001\n",
      "[D valid loss: 0.399028],[D fake loss: 0.000000], [G loss(mse): 0.534608, G loss(w): 0.184263]\n",
      "Epoch 255/10001\n",
      "[D valid loss: 0.398590],[D fake loss: 0.000000], [G loss(mse): 0.538523, G loss(w): 0.193082]\n",
      "Epoch 256/10001\n",
      "[D valid loss: 0.398122],[D fake loss: 0.000000], [G loss(mse): 0.539833, G loss(w): 0.188769]\n",
      "Epoch 257/10001\n",
      "[D valid loss: 0.397663],[D fake loss: 0.000000], [G loss(mse): 0.534934, G loss(w): 0.183654]\n",
      "Epoch 258/10001\n",
      "[D valid loss: 0.397165],[D fake loss: 0.000000], [G loss(mse): 0.543442, G loss(w): 0.193333]\n",
      "Epoch 259/10001\n",
      "[D valid loss: 0.396675],[D fake loss: 0.000000], [G loss(mse): 0.550723, G loss(w): 0.201855]\n",
      "Epoch 260/10001\n",
      "[D valid loss: 0.396170],[D fake loss: 0.000000], [G loss(mse): 0.530982, G loss(w): 0.184662]\n",
      "Epoch 261/10001\n",
      "[D valid loss: 0.395683],[D fake loss: 0.000000], [G loss(mse): 0.528518, G loss(w): 0.186243]\n",
      "Epoch 262/10001\n",
      "[D valid loss: 0.395177],[D fake loss: 0.000000], [G loss(mse): 0.538423, G loss(w): 0.189846]\n",
      "Epoch 263/10001\n",
      "[D valid loss: 0.394689],[D fake loss: 0.000000], [G loss(mse): 0.534965, G loss(w): 0.183505]\n",
      "Epoch 264/10001\n",
      "[D valid loss: 0.394246],[D fake loss: 0.000000], [G loss(mse): 0.539508, G loss(w): 0.191643]\n",
      "Epoch 265/10001\n",
      "[D valid loss: 0.393709],[D fake loss: 0.000000], [G loss(mse): 0.535651, G loss(w): 0.185703]\n",
      "Epoch 266/10001\n",
      "[D valid loss: 0.393217],[D fake loss: 0.000000], [G loss(mse): 0.529782, G loss(w): 0.183414]\n",
      "Epoch 267/10001\n",
      "[D valid loss: 0.392806],[D fake loss: 0.000000], [G loss(mse): 0.533438, G loss(w): 0.185672]\n",
      "Epoch 268/10001\n",
      "[D valid loss: 0.392312],[D fake loss: 0.000000], [G loss(mse): 0.530687, G loss(w): 0.184864]\n",
      "Epoch 269/10001\n",
      "[D valid loss: 0.391758],[D fake loss: 0.000000], [G loss(mse): 0.537916, G loss(w): 0.183099]\n",
      "Epoch 270/10001\n",
      "[D valid loss: 0.391259],[D fake loss: 0.000000], [G loss(mse): 0.541722, G loss(w): 0.194422]\n",
      "Epoch 271/10001\n",
      "[D valid loss: 0.390722],[D fake loss: 0.000000], [G loss(mse): 0.538058, G loss(w): 0.188921]\n",
      "Epoch 272/10001\n",
      "[D valid loss: 0.390286],[D fake loss: 0.000000], [G loss(mse): 0.540184, G loss(w): 0.191872]\n",
      "Epoch 273/10001\n",
      "[D valid loss: 0.389859],[D fake loss: 0.000000], [G loss(mse): 0.529605, G loss(w): 0.183700]\n",
      "Epoch 274/10001\n",
      "[D valid loss: 0.389377],[D fake loss: 0.000000], [G loss(mse): 0.523649, G loss(w): 0.176691]\n",
      "Epoch 275/10001\n",
      "[D valid loss: 0.388792],[D fake loss: 0.000000], [G loss(mse): 0.533132, G loss(w): 0.188215]\n",
      "Epoch 276/10001\n",
      "[D valid loss: 0.388396],[D fake loss: 0.000000], [G loss(mse): 0.525285, G loss(w): 0.182657]\n",
      "Epoch 277/10001\n",
      "[D valid loss: 0.387931],[D fake loss: 0.000000], [G loss(mse): 0.522286, G loss(w): 0.188728]\n",
      "Epoch 278/10001\n",
      "[D valid loss: 0.387272],[D fake loss: 0.000000], [G loss(mse): 0.512902, G loss(w): 0.181313]\n",
      "Epoch 279/10001\n",
      "[D valid loss: 0.386921],[D fake loss: 0.000000], [G loss(mse): 0.516979, G loss(w): 0.179776]\n",
      "Epoch 280/10001\n",
      "[D valid loss: 0.386304],[D fake loss: 0.000000], [G loss(mse): 0.520707, G loss(w): 0.188402]\n",
      "Epoch 281/10001\n",
      "[D valid loss: 0.385900],[D fake loss: 0.000000], [G loss(mse): 0.523400, G loss(w): 0.184582]\n",
      "Epoch 282/10001\n",
      "[D valid loss: 0.385370],[D fake loss: 0.000000], [G loss(mse): 0.519599, G loss(w): 0.185423]\n",
      "Epoch 283/10001\n",
      "[D valid loss: 0.384859],[D fake loss: 0.000000], [G loss(mse): 0.510843, G loss(w): 0.173362]\n",
      "Epoch 284/10001\n",
      "[D valid loss: 0.384439],[D fake loss: 0.000000], [G loss(mse): 0.522121, G loss(w): 0.183493]\n",
      "Epoch 285/10001\n",
      "[D valid loss: 0.383979],[D fake loss: 0.000000], [G loss(mse): 0.519491, G loss(w): 0.181317]\n",
      "Epoch 286/10001\n",
      "[D valid loss: 0.383379],[D fake loss: 0.000000], [G loss(mse): 0.519685, G loss(w): 0.180403]\n",
      "Epoch 287/10001\n",
      "[D valid loss: 0.382844],[D fake loss: 0.000000], [G loss(mse): 0.523274, G loss(w): 0.179833]\n",
      "Epoch 288/10001\n",
      "[D valid loss: 0.382396],[D fake loss: 0.000000], [G loss(mse): 0.525539, G loss(w): 0.190170]\n",
      "Epoch 289/10001\n",
      "[D valid loss: 0.381887],[D fake loss: 0.000000], [G loss(mse): 0.515823, G loss(w): 0.178764]\n",
      "Epoch 290/10001\n",
      "[D valid loss: 0.381447],[D fake loss: 0.000000], [G loss(mse): 0.511099, G loss(w): 0.173539]\n",
      "Epoch 291/10001\n",
      "[D valid loss: 0.380916],[D fake loss: 0.000000], [G loss(mse): 0.511184, G loss(w): 0.175687]\n",
      "Epoch 292/10001\n",
      "[D valid loss: 0.380513],[D fake loss: 0.000000], [G loss(mse): 0.508034, G loss(w): 0.173950]\n",
      "Epoch 293/10001\n",
      "[D valid loss: 0.379937],[D fake loss: 0.000000], [G loss(mse): 0.506604, G loss(w): 0.173372]\n",
      "Epoch 294/10001\n",
      "[D valid loss: 0.379600],[D fake loss: 0.000000], [G loss(mse): 0.501525, G loss(w): 0.173195]\n",
      "Epoch 295/10001\n",
      "[D valid loss: 0.378939],[D fake loss: 0.000000], [G loss(mse): 0.502041, G loss(w): 0.171484]\n",
      "Epoch 296/10001\n",
      "[D valid loss: 0.378555],[D fake loss: 0.000000], [G loss(mse): 0.499526, G loss(w): 0.172995]\n",
      "Epoch 297/10001\n",
      "[D valid loss: 0.377997],[D fake loss: 0.000000], [G loss(mse): 0.508354, G loss(w): 0.179131]\n",
      "Epoch 298/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.377469],[D fake loss: 0.000000], [G loss(mse): 0.494717, G loss(w): 0.170901]\n",
      "Epoch 299/10001\n",
      "[D valid loss: 0.376947],[D fake loss: 0.000000], [G loss(mse): 0.506086, G loss(w): 0.181262]\n",
      "Epoch 300/10001\n",
      "[D valid loss: 0.376536],[D fake loss: 0.000000], [G loss(mse): 0.501730, G loss(w): 0.174814]\n",
      "Epoch 301/10001\n",
      "[D valid loss: 0.375944],[D fake loss: 0.000000], [G loss(mse): 0.496693, G loss(w): 0.170218]\n",
      "Epoch 302/10001\n",
      "[D valid loss: 0.375503],[D fake loss: 0.000000], [G loss(mse): 0.486609, G loss(w): 0.167939]\n",
      "Epoch 303/10001\n",
      "[D valid loss: 0.375158],[D fake loss: 0.000000], [G loss(mse): 0.502713, G loss(w): 0.177249]\n",
      "Epoch 304/10001\n",
      "[D valid loss: 0.374579],[D fake loss: 0.000000], [G loss(mse): 0.499653, G loss(w): 0.171584]\n",
      "Epoch 305/10001\n",
      "[D valid loss: 0.373905],[D fake loss: 0.000000], [G loss(mse): 0.499185, G loss(w): 0.176562]\n",
      "Epoch 306/10001\n",
      "[D valid loss: 0.373483],[D fake loss: 0.000000], [G loss(mse): 0.481114, G loss(w): 0.159128]\n",
      "Epoch 307/10001\n",
      "[D valid loss: 0.373017],[D fake loss: 0.000000], [G loss(mse): 0.485548, G loss(w): 0.166908]\n",
      "Epoch 308/10001\n",
      "[D valid loss: 0.372457],[D fake loss: 0.000000], [G loss(mse): 0.485612, G loss(w): 0.161999]\n",
      "Epoch 309/10001\n",
      "[D valid loss: 0.372116],[D fake loss: 0.000000], [G loss(mse): 0.500232, G loss(w): 0.178747]\n",
      "Epoch 310/10001\n",
      "[D valid loss: 0.371664],[D fake loss: 0.000000], [G loss(mse): 0.490117, G loss(w): 0.169709]\n",
      "Epoch 311/10001\n",
      "[D valid loss: 0.371001],[D fake loss: 0.000000], [G loss(mse): 0.486409, G loss(w): 0.165887]\n",
      "Epoch 312/10001\n",
      "[D valid loss: 0.370517],[D fake loss: 0.000000], [G loss(mse): 0.489249, G loss(w): 0.164603]\n",
      "Epoch 313/10001\n",
      "[D valid loss: 0.370154],[D fake loss: 0.000000], [G loss(mse): 0.476050, G loss(w): 0.156759]\n",
      "Epoch 314/10001\n",
      "[D valid loss: 0.369515],[D fake loss: 0.000000], [G loss(mse): 0.498003, G loss(w): 0.173434]\n",
      "Epoch 315/10001\n",
      "[D valid loss: 0.368932],[D fake loss: 0.000000], [G loss(mse): 0.491839, G loss(w): 0.169366]\n",
      "Epoch 316/10001\n",
      "[D valid loss: 0.368462],[D fake loss: 0.000000], [G loss(mse): 0.487255, G loss(w): 0.168068]\n",
      "Epoch 317/10001\n",
      "[D valid loss: 0.368112],[D fake loss: 0.000000], [G loss(mse): 0.469352, G loss(w): 0.154460]\n",
      "Epoch 318/10001\n",
      "[D valid loss: 0.367580],[D fake loss: 0.000000], [G loss(mse): 0.493592, G loss(w): 0.172750]\n",
      "Epoch 319/10001\n",
      "[D valid loss: 0.367106],[D fake loss: 0.000000], [G loss(mse): 0.475842, G loss(w): 0.162168]\n",
      "Epoch 320/10001\n",
      "[D valid loss: 0.366564],[D fake loss: 0.000000], [G loss(mse): 0.479314, G loss(w): 0.163204]\n",
      "Epoch 321/10001\n",
      "[D valid loss: 0.366120],[D fake loss: 0.000000], [G loss(mse): 0.485880, G loss(w): 0.169015]\n",
      "Epoch 322/10001\n",
      "[D valid loss: 0.365624],[D fake loss: 0.000000], [G loss(mse): 0.484678, G loss(w): 0.166452]\n",
      "Epoch 323/10001\n",
      "[D valid loss: 0.365086],[D fake loss: 0.000000], [G loss(mse): 0.488101, G loss(w): 0.169040]\n",
      "Epoch 324/10001\n",
      "[D valid loss: 0.364581],[D fake loss: 0.000000], [G loss(mse): 0.474602, G loss(w): 0.158744]\n",
      "Epoch 325/10001\n",
      "[D valid loss: 0.364074],[D fake loss: 0.000000], [G loss(mse): 0.489679, G loss(w): 0.176165]\n",
      "Epoch 326/10001\n",
      "[D valid loss: 0.363492],[D fake loss: 0.000000], [G loss(mse): 0.473992, G loss(w): 0.158923]\n",
      "Epoch 327/10001\n",
      "[D valid loss: 0.363121],[D fake loss: 0.000000], [G loss(mse): 0.475414, G loss(w): 0.163213]\n",
      "Epoch 328/10001\n",
      "[D valid loss: 0.362538],[D fake loss: 0.000000], [G loss(mse): 0.485818, G loss(w): 0.169245]\n",
      "Epoch 329/10001\n",
      "[D valid loss: 0.362098],[D fake loss: 0.000000], [G loss(mse): 0.489833, G loss(w): 0.173616]\n",
      "Epoch 330/10001\n",
      "[D valid loss: 0.361617],[D fake loss: 0.000000], [G loss(mse): 0.481508, G loss(w): 0.167338]\n",
      "Epoch 331/10001\n",
      "[D valid loss: 0.361177],[D fake loss: 0.000000], [G loss(mse): 0.468854, G loss(w): 0.156831]\n",
      "Epoch 332/10001\n",
      "[D valid loss: 0.360584],[D fake loss: 0.000000], [G loss(mse): 0.478427, G loss(w): 0.162852]\n",
      "Epoch 333/10001\n",
      "[D valid loss: 0.360023],[D fake loss: 0.000000], [G loss(mse): 0.470135, G loss(w): 0.154158]\n",
      "Epoch 334/10001\n",
      "[D valid loss: 0.359865],[D fake loss: 0.000000], [G loss(mse): 0.467583, G loss(w): 0.155973]\n",
      "Epoch 335/10001\n",
      "[D valid loss: 0.359035],[D fake loss: 0.000000], [G loss(mse): 0.460841, G loss(w): 0.150409]\n",
      "Epoch 336/10001\n",
      "[D valid loss: 0.358568],[D fake loss: 0.000000], [G loss(mse): 0.465736, G loss(w): 0.158999]\n",
      "Epoch 337/10001\n",
      "[D valid loss: 0.358100],[D fake loss: 0.000000], [G loss(mse): 0.468729, G loss(w): 0.162356]\n",
      "Epoch 338/10001\n",
      "[D valid loss: 0.357600],[D fake loss: 0.000000], [G loss(mse): 0.462157, G loss(w): 0.154301]\n",
      "Epoch 339/10001\n",
      "[D valid loss: 0.356995],[D fake loss: 0.000000], [G loss(mse): 0.464898, G loss(w): 0.155452]\n",
      "Epoch 340/10001\n",
      "[D valid loss: 0.356624],[D fake loss: 0.000000], [G loss(mse): 0.476934, G loss(w): 0.170100]\n",
      "Epoch 341/10001\n",
      "[D valid loss: 0.356087],[D fake loss: 0.000000], [G loss(mse): 0.460960, G loss(w): 0.154140]\n",
      "Epoch 342/10001\n",
      "[D valid loss: 0.355596],[D fake loss: 0.000000], [G loss(mse): 0.459224, G loss(w): 0.156414]\n",
      "Epoch 343/10001\n",
      "[D valid loss: 0.354986],[D fake loss: 0.000000], [G loss(mse): 0.456437, G loss(w): 0.148742]\n",
      "Epoch 344/10001\n",
      "[D valid loss: 0.354625],[D fake loss: 0.000000], [G loss(mse): 0.473617, G loss(w): 0.162284]\n",
      "Epoch 345/10001\n",
      "[D valid loss: 0.354266],[D fake loss: 0.000000], [G loss(mse): 0.472492, G loss(w): 0.160679]\n",
      "Epoch 346/10001\n",
      "[D valid loss: 0.353627],[D fake loss: 0.000000], [G loss(mse): 0.459966, G loss(w): 0.155182]\n",
      "Epoch 347/10001\n",
      "[D valid loss: 0.353104],[D fake loss: 0.000000], [G loss(mse): 0.461763, G loss(w): 0.155286]\n",
      "Epoch 348/10001\n",
      "[D valid loss: 0.352533],[D fake loss: 0.000000], [G loss(mse): 0.461615, G loss(w): 0.159931]\n",
      "Epoch 349/10001\n",
      "[D valid loss: 0.352044],[D fake loss: 0.000000], [G loss(mse): 0.458561, G loss(w): 0.151278]\n",
      "Epoch 350/10001\n",
      "[D valid loss: 0.351689],[D fake loss: 0.000000], [G loss(mse): 0.469149, G loss(w): 0.158366]\n",
      "Epoch 351/10001\n",
      "[D valid loss: 0.351147],[D fake loss: 0.000000], [G loss(mse): 0.458408, G loss(w): 0.151593]\n",
      "Epoch 352/10001\n",
      "[D valid loss: 0.350621],[D fake loss: 0.000000], [G loss(mse): 0.462565, G loss(w): 0.158366]\n",
      "Epoch 353/10001\n",
      "[D valid loss: 0.349956],[D fake loss: 0.000000], [G loss(mse): 0.459749, G loss(w): 0.153325]\n",
      "Epoch 354/10001\n",
      "[D valid loss: 0.349916],[D fake loss: 0.000000], [G loss(mse): 0.462292, G loss(w): 0.154633]\n",
      "Epoch 355/10001\n",
      "[D valid loss: 0.349095],[D fake loss: 0.000000], [G loss(mse): 0.461460, G loss(w): 0.152899]\n",
      "Epoch 356/10001\n",
      "[D valid loss: 0.348608],[D fake loss: 0.000000], [G loss(mse): 0.462425, G loss(w): 0.158442]\n",
      "Epoch 357/10001\n",
      "[D valid loss: 0.348063],[D fake loss: 0.000000], [G loss(mse): 0.455396, G loss(w): 0.147236]\n",
      "Epoch 358/10001\n",
      "[D valid loss: 0.347605],[D fake loss: 0.000000], [G loss(mse): 0.463659, G loss(w): 0.156898]\n",
      "Epoch 359/10001\n",
      "[D valid loss: 0.346942],[D fake loss: 0.000000], [G loss(mse): 0.451996, G loss(w): 0.146809]\n",
      "Epoch 360/10001\n",
      "[D valid loss: 0.346580],[D fake loss: 0.000000], [G loss(mse): 0.452530, G loss(w): 0.150228]\n",
      "Epoch 361/10001\n",
      "[D valid loss: 0.345980],[D fake loss: 0.000000], [G loss(mse): 0.458896, G loss(w): 0.154587]\n",
      "Epoch 362/10001\n",
      "[D valid loss: 0.345432],[D fake loss: 0.000000], [G loss(mse): 0.451266, G loss(w): 0.144061]\n",
      "Epoch 363/10001\n",
      "[D valid loss: 0.345010],[D fake loss: 0.000000], [G loss(mse): 0.452918, G loss(w): 0.148548]\n",
      "Epoch 364/10001\n",
      "[D valid loss: 0.344539],[D fake loss: 0.000000], [G loss(mse): 0.448252, G loss(w): 0.147504]\n",
      "Epoch 365/10001\n",
      "[D valid loss: 0.344035],[D fake loss: 0.000000], [G loss(mse): 0.455510, G loss(w): 0.153293]\n",
      "Epoch 366/10001\n",
      "[D valid loss: 0.343486],[D fake loss: 0.000000], [G loss(mse): 0.450749, G loss(w): 0.152407]\n",
      "Epoch 367/10001\n",
      "[D valid loss: 0.343074],[D fake loss: 0.000000], [G loss(mse): 0.446007, G loss(w): 0.148563]\n",
      "Epoch 368/10001\n",
      "[D valid loss: 0.342956],[D fake loss: 0.000000], [G loss(mse): 0.450116, G loss(w): 0.152771]\n",
      "Epoch 369/10001\n",
      "[D valid loss: 0.342413],[D fake loss: 0.000000], [G loss(mse): 0.438154, G loss(w): 0.143718]\n",
      "Epoch 370/10001\n",
      "[D valid loss: 0.341557],[D fake loss: 0.000000], [G loss(mse): 0.451973, G loss(w): 0.160591]\n",
      "Epoch 371/10001\n",
      "[D valid loss: 0.341095],[D fake loss: 0.000000], [G loss(mse): 0.445337, G loss(w): 0.150487]\n",
      "Epoch 372/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.340549],[D fake loss: 0.000000], [G loss(mse): 0.438829, G loss(w): 0.141249]\n",
      "Epoch 373/10001\n",
      "[D valid loss: 0.340121],[D fake loss: 0.000000], [G loss(mse): 0.438447, G loss(w): 0.144358]\n",
      "Epoch 374/10001\n",
      "[D valid loss: 0.339704],[D fake loss: 0.000000], [G loss(mse): 0.438703, G loss(w): 0.144882]\n",
      "Epoch 375/10001\n",
      "[D valid loss: 0.339085],[D fake loss: 0.000000], [G loss(mse): 0.443179, G loss(w): 0.144924]\n",
      "Epoch 376/10001\n",
      "[D valid loss: 0.338714],[D fake loss: 0.000000], [G loss(mse): 0.443612, G loss(w): 0.152154]\n",
      "Epoch 377/10001\n",
      "[D valid loss: 0.338122],[D fake loss: 0.000000], [G loss(mse): 0.441738, G loss(w): 0.150490]\n",
      "Epoch 378/10001\n",
      "[D valid loss: 0.337501],[D fake loss: 0.000000], [G loss(mse): 0.441617, G loss(w): 0.149175]\n",
      "Epoch 379/10001\n",
      "[D valid loss: 0.337126],[D fake loss: 0.000000], [G loss(mse): 0.444082, G loss(w): 0.148702]\n",
      "Epoch 380/10001\n",
      "[D valid loss: 0.336708],[D fake loss: 0.000000], [G loss(mse): 0.446973, G loss(w): 0.151706]\n",
      "Epoch 381/10001\n",
      "[D valid loss: 0.336241],[D fake loss: 0.000000], [G loss(mse): 0.437515, G loss(w): 0.143674]\n",
      "Epoch 382/10001\n",
      "[D valid loss: 0.335598],[D fake loss: 0.000000], [G loss(mse): 0.447165, G loss(w): 0.152559]\n",
      "Epoch 383/10001\n",
      "[D valid loss: 0.335151],[D fake loss: 0.000000], [G loss(mse): 0.433230, G loss(w): 0.139141]\n",
      "Epoch 384/10001\n",
      "[D valid loss: 0.334765],[D fake loss: 0.000000], [G loss(mse): 0.442325, G loss(w): 0.147405]\n",
      "Epoch 385/10001\n",
      "[D valid loss: 0.334183],[D fake loss: 0.000000], [G loss(mse): 0.440610, G loss(w): 0.145353]\n",
      "Epoch 386/10001\n",
      "[D valid loss: 0.333636],[D fake loss: 0.000000], [G loss(mse): 0.447793, G loss(w): 0.153513]\n",
      "Epoch 387/10001\n",
      "[D valid loss: 0.333123],[D fake loss: 0.000000], [G loss(mse): 0.439325, G loss(w): 0.140882]\n",
      "Epoch 388/10001\n",
      "[D valid loss: 0.332951],[D fake loss: 0.000000], [G loss(mse): 0.433158, G loss(w): 0.140309]\n",
      "Epoch 389/10001\n",
      "[D valid loss: 0.332092],[D fake loss: 0.000000], [G loss(mse): 0.432177, G loss(w): 0.140677]\n",
      "Epoch 390/10001\n",
      "[D valid loss: 0.331673],[D fake loss: 0.000000], [G loss(mse): 0.442291, G loss(w): 0.149131]\n",
      "Epoch 391/10001\n",
      "[D valid loss: 0.331040],[D fake loss: 0.000000], [G loss(mse): 0.432997, G loss(w): 0.139079]\n",
      "Epoch 392/10001\n",
      "[D valid loss: 0.330673],[D fake loss: 0.000000], [G loss(mse): 0.427388, G loss(w): 0.137427]\n",
      "Epoch 393/10001\n",
      "[D valid loss: 0.330126],[D fake loss: 0.000000], [G loss(mse): 0.437621, G loss(w): 0.146347]\n",
      "Epoch 394/10001\n",
      "[D valid loss: 0.329734],[D fake loss: 0.000000], [G loss(mse): 0.442836, G loss(w): 0.150027]\n",
      "Epoch 395/10001\n",
      "[D valid loss: 0.329304],[D fake loss: 0.000000], [G loss(mse): 0.442586, G loss(w): 0.150381]\n",
      "Epoch 396/10001\n",
      "[D valid loss: 0.328740],[D fake loss: 0.000000], [G loss(mse): 0.444024, G loss(w): 0.149616]\n",
      "Epoch 397/10001\n",
      "[D valid loss: 0.328206],[D fake loss: 0.000000], [G loss(mse): 0.442078, G loss(w): 0.149699]\n",
      "Epoch 398/10001\n",
      "[D valid loss: 0.327593],[D fake loss: 0.000000], [G loss(mse): 0.432005, G loss(w): 0.142120]\n",
      "Epoch 399/10001\n",
      "[D valid loss: 0.327280],[D fake loss: 0.000000], [G loss(mse): 0.430727, G loss(w): 0.140028]\n",
      "Epoch 400/10001\n",
      "[D valid loss: 0.326604],[D fake loss: 0.000000], [G loss(mse): 0.431750, G loss(w): 0.142325]\n",
      "Epoch 401/10001\n",
      "[D valid loss: 0.326207],[D fake loss: 0.000000], [G loss(mse): 0.430611, G loss(w): 0.142415]\n",
      "Epoch 402/10001\n",
      "[D valid loss: 0.325669],[D fake loss: 0.000000], [G loss(mse): 0.428549, G loss(w): 0.137063]\n",
      "Epoch 403/10001\n",
      "[D valid loss: 0.325205],[D fake loss: 0.000000], [G loss(mse): 0.429641, G loss(w): 0.139088]\n",
      "Epoch 404/10001\n",
      "[D valid loss: 0.324697],[D fake loss: 0.000000], [G loss(mse): 0.419178, G loss(w): 0.127330]\n",
      "Epoch 405/10001\n",
      "[D valid loss: 0.324063],[D fake loss: 0.000000], [G loss(mse): 0.426100, G loss(w): 0.138679]\n",
      "Epoch 406/10001\n",
      "[D valid loss: 0.323602],[D fake loss: 0.000000], [G loss(mse): 0.432242, G loss(w): 0.143852]\n",
      "Epoch 407/10001\n",
      "[D valid loss: 0.323241],[D fake loss: 0.000000], [G loss(mse): 0.423933, G loss(w): 0.135221]\n",
      "Epoch 408/10001\n",
      "[D valid loss: 0.322760],[D fake loss: 0.000000], [G loss(mse): 0.433816, G loss(w): 0.144034]\n",
      "Epoch 409/10001\n",
      "[D valid loss: 0.322102],[D fake loss: 0.000000], [G loss(mse): 0.430074, G loss(w): 0.143973]\n",
      "Epoch 410/10001\n",
      "[D valid loss: 0.321646],[D fake loss: 0.000000], [G loss(mse): 0.422363, G loss(w): 0.133408]\n",
      "Epoch 411/10001\n",
      "[D valid loss: 0.321128],[D fake loss: 0.000000], [G loss(mse): 0.419701, G loss(w): 0.136673]\n",
      "Epoch 412/10001\n",
      "[D valid loss: 0.320584],[D fake loss: 0.000000], [G loss(mse): 0.422871, G loss(w): 0.137654]\n",
      "Epoch 413/10001\n",
      "[D valid loss: 0.320147],[D fake loss: 0.000000], [G loss(mse): 0.431145, G loss(w): 0.138090]\n",
      "Epoch 414/10001\n",
      "[D valid loss: 0.319802],[D fake loss: 0.000000], [G loss(mse): 0.433057, G loss(w): 0.142926]\n",
      "Epoch 415/10001\n",
      "[D valid loss: 0.319245],[D fake loss: 0.000000], [G loss(mse): 0.423852, G loss(w): 0.137797]\n",
      "Epoch 416/10001\n",
      "[D valid loss: 0.318774],[D fake loss: 0.000000], [G loss(mse): 0.426638, G loss(w): 0.138982]\n",
      "Epoch 417/10001\n",
      "[D valid loss: 0.318250],[D fake loss: 0.000000], [G loss(mse): 0.422019, G loss(w): 0.139635]\n",
      "Epoch 418/10001\n",
      "[D valid loss: 0.317815],[D fake loss: 0.000000], [G loss(mse): 0.431189, G loss(w): 0.149587]\n",
      "Epoch 419/10001\n",
      "[D valid loss: 0.317250],[D fake loss: 0.000000], [G loss(mse): 0.424878, G loss(w): 0.141598]\n",
      "Epoch 420/10001\n",
      "[D valid loss: 0.316643],[D fake loss: 0.000000], [G loss(mse): 0.416345, G loss(w): 0.137855]\n",
      "Epoch 421/10001\n",
      "[D valid loss: 0.316283],[D fake loss: 0.000000], [G loss(mse): 0.415121, G loss(w): 0.134730]\n",
      "Epoch 422/10001\n",
      "[D valid loss: 0.315725],[D fake loss: 0.000000], [G loss(mse): 0.422183, G loss(w): 0.140867]\n",
      "Epoch 423/10001\n",
      "[D valid loss: 0.315224],[D fake loss: 0.000000], [G loss(mse): 0.418816, G loss(w): 0.141417]\n",
      "Epoch 424/10001\n",
      "[D valid loss: 0.314866],[D fake loss: 0.000000], [G loss(mse): 0.410581, G loss(w): 0.135948]\n",
      "Epoch 425/10001\n",
      "[D valid loss: 0.314326],[D fake loss: 0.000000], [G loss(mse): 0.411912, G loss(w): 0.130856]\n",
      "Epoch 426/10001\n",
      "[D valid loss: 0.313880],[D fake loss: 0.000000], [G loss(mse): 0.413924, G loss(w): 0.133762]\n",
      "Epoch 427/10001\n",
      "[D valid loss: 0.313241],[D fake loss: 0.000000], [G loss(mse): 0.416957, G loss(w): 0.139879]\n",
      "Epoch 428/10001\n",
      "[D valid loss: 0.312769],[D fake loss: 0.000000], [G loss(mse): 0.420957, G loss(w): 0.140371]\n",
      "Epoch 429/10001\n",
      "[D valid loss: 0.312478],[D fake loss: 0.000000], [G loss(mse): 0.410619, G loss(w): 0.133205]\n",
      "Epoch 430/10001\n",
      "[D valid loss: 0.311905],[D fake loss: 0.000000], [G loss(mse): 0.415986, G loss(w): 0.139270]\n",
      "Epoch 431/10001\n",
      "[D valid loss: 0.311155],[D fake loss: 0.000000], [G loss(mse): 0.418489, G loss(w): 0.140255]\n",
      "Epoch 432/10001\n",
      "[D valid loss: 0.310875],[D fake loss: 0.000000], [G loss(mse): 0.418823, G loss(w): 0.137706]\n",
      "Epoch 433/10001\n",
      "[D valid loss: 0.310665],[D fake loss: 0.000000], [G loss(mse): 0.409057, G loss(w): 0.129786]\n",
      "Epoch 434/10001\n",
      "[D valid loss: 0.309825],[D fake loss: 0.000000], [G loss(mse): 0.416301, G loss(w): 0.132735]\n",
      "Epoch 435/10001\n",
      "[D valid loss: 0.309360],[D fake loss: 0.000000], [G loss(mse): 0.417321, G loss(w): 0.141270]\n",
      "Epoch 436/10001\n",
      "[D valid loss: 0.308903],[D fake loss: 0.000000], [G loss(mse): 0.409219, G loss(w): 0.135141]\n",
      "Epoch 437/10001\n",
      "[D valid loss: 0.308305],[D fake loss: 0.000000], [G loss(mse): 0.414317, G loss(w): 0.137664]\n",
      "Epoch 438/10001\n",
      "[D valid loss: 0.307816],[D fake loss: 0.000000], [G loss(mse): 0.408249, G loss(w): 0.127987]\n",
      "Epoch 439/10001\n",
      "[D valid loss: 0.307506],[D fake loss: 0.000000], [G loss(mse): 0.410595, G loss(w): 0.133704]\n",
      "Epoch 440/10001\n",
      "[D valid loss: 0.306951],[D fake loss: 0.000000], [G loss(mse): 0.406678, G loss(w): 0.132712]\n",
      "Epoch 441/10001\n",
      "[D valid loss: 0.306439],[D fake loss: 0.000000], [G loss(mse): 0.406522, G loss(w): 0.131188]\n",
      "Epoch 442/10001\n",
      "[D valid loss: 0.305799],[D fake loss: 0.000000], [G loss(mse): 0.411728, G loss(w): 0.132179]\n",
      "Epoch 443/10001\n",
      "[D valid loss: 0.305353],[D fake loss: 0.000000], [G loss(mse): 0.412648, G loss(w): 0.136118]\n",
      "Epoch 444/10001\n",
      "[D valid loss: 0.304995],[D fake loss: 0.000000], [G loss(mse): 0.419156, G loss(w): 0.140979]\n",
      "Epoch 445/10001\n",
      "[D valid loss: 0.304595],[D fake loss: 0.000000], [G loss(mse): 0.401353, G loss(w): 0.123431]\n",
      "Epoch 446/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.304076],[D fake loss: 0.000000], [G loss(mse): 0.405328, G loss(w): 0.129450]\n",
      "Epoch 447/10001\n",
      "[D valid loss: 0.303395],[D fake loss: 0.000000], [G loss(mse): 0.401486, G loss(w): 0.130290]\n",
      "Epoch 448/10001\n",
      "[D valid loss: 0.302965],[D fake loss: 0.000000], [G loss(mse): 0.402944, G loss(w): 0.131010]\n",
      "Epoch 449/10001\n",
      "[D valid loss: 0.302419],[D fake loss: 0.000000], [G loss(mse): 0.398966, G loss(w): 0.130162]\n",
      "Epoch 450/10001\n",
      "[D valid loss: 0.302154],[D fake loss: 0.000000], [G loss(mse): 0.394818, G loss(w): 0.121712]\n",
      "Epoch 451/10001\n",
      "[D valid loss: 0.301619],[D fake loss: 0.000000], [G loss(mse): 0.405254, G loss(w): 0.137808]\n",
      "Epoch 452/10001\n",
      "[D valid loss: 0.301044],[D fake loss: 0.000000], [G loss(mse): 0.400212, G loss(w): 0.134090]\n",
      "Epoch 453/10001\n",
      "[D valid loss: 0.300503],[D fake loss: 0.000000], [G loss(mse): 0.404644, G loss(w): 0.136063]\n",
      "Epoch 454/10001\n",
      "[D valid loss: 0.300032],[D fake loss: 0.000000], [G loss(mse): 0.399105, G loss(w): 0.130522]\n",
      "Epoch 455/10001\n",
      "[D valid loss: 0.299550],[D fake loss: 0.000000], [G loss(mse): 0.399233, G loss(w): 0.134628]\n",
      "Epoch 456/10001\n",
      "[D valid loss: 0.299043],[D fake loss: 0.000000], [G loss(mse): 0.394455, G loss(w): 0.130453]\n",
      "Epoch 457/10001\n",
      "[D valid loss: 0.298623],[D fake loss: 0.000000], [G loss(mse): 0.397775, G loss(w): 0.129070]\n",
      "Epoch 458/10001\n",
      "[D valid loss: 0.298510],[D fake loss: 0.000000], [G loss(mse): 0.395174, G loss(w): 0.128501]\n",
      "Epoch 459/10001\n",
      "[D valid loss: 0.297639],[D fake loss: 0.000000], [G loss(mse): 0.396505, G loss(w): 0.124093]\n",
      "Epoch 460/10001\n",
      "[D valid loss: 0.297205],[D fake loss: 0.000000], [G loss(mse): 0.397684, G loss(w): 0.130614]\n",
      "Epoch 461/10001\n",
      "[D valid loss: 0.296553],[D fake loss: 0.000000], [G loss(mse): 0.399883, G loss(w): 0.128881]\n",
      "Epoch 462/10001\n",
      "[D valid loss: 0.296231],[D fake loss: 0.000000], [G loss(mse): 0.393241, G loss(w): 0.129399]\n",
      "Epoch 463/10001\n",
      "[D valid loss: 0.295556],[D fake loss: 0.000000], [G loss(mse): 0.398955, G loss(w): 0.130627]\n",
      "Epoch 464/10001\n",
      "[D valid loss: 0.295459],[D fake loss: 0.000000], [G loss(mse): 0.399864, G loss(w): 0.130615]\n",
      "Epoch 465/10001\n",
      "[D valid loss: 0.294755],[D fake loss: 0.000000], [G loss(mse): 0.396914, G loss(w): 0.132943]\n",
      "Epoch 466/10001\n",
      "[D valid loss: 0.294175],[D fake loss: 0.000000], [G loss(mse): 0.393774, G loss(w): 0.122124]\n",
      "Epoch 467/10001\n",
      "[D valid loss: 0.293757],[D fake loss: 0.000000], [G loss(mse): 0.388672, G loss(w): 0.123244]\n",
      "Epoch 468/10001\n",
      "[D valid loss: 0.293241],[D fake loss: 0.000000], [G loss(mse): 0.397516, G loss(w): 0.130004]\n",
      "Epoch 469/10001\n",
      "[D valid loss: 0.292889],[D fake loss: 0.000000], [G loss(mse): 0.394328, G loss(w): 0.130069]\n",
      "Epoch 470/10001\n",
      "[D valid loss: 0.292353],[D fake loss: 0.000000], [G loss(mse): 0.390974, G loss(w): 0.126671]\n",
      "Epoch 471/10001\n",
      "[D valid loss: 0.291945],[D fake loss: 0.000000], [G loss(mse): 0.390211, G loss(w): 0.123100]\n",
      "Epoch 472/10001\n",
      "[D valid loss: 0.291280],[D fake loss: 0.000000], [G loss(mse): 0.381795, G loss(w): 0.121357]\n",
      "Epoch 473/10001\n",
      "[D valid loss: 0.290763],[D fake loss: 0.000000], [G loss(mse): 0.385266, G loss(w): 0.126642]\n",
      "Epoch 474/10001\n",
      "[D valid loss: 0.290621],[D fake loss: 0.000000], [G loss(mse): 0.382537, G loss(w): 0.123719]\n",
      "Epoch 475/10001\n",
      "[D valid loss: 0.289852],[D fake loss: 0.000000], [G loss(mse): 0.390796, G loss(w): 0.130992]\n",
      "Epoch 476/10001\n",
      "[D valid loss: 0.289805],[D fake loss: 0.000000], [G loss(mse): 0.387661, G loss(w): 0.130201]\n",
      "Epoch 477/10001\n",
      "[D valid loss: 0.289449],[D fake loss: 0.000000], [G loss(mse): 0.382852, G loss(w): 0.126543]\n",
      "Epoch 478/10001\n",
      "[D valid loss: 0.288460],[D fake loss: 0.000000], [G loss(mse): 0.382936, G loss(w): 0.123859]\n",
      "Epoch 479/10001\n",
      "[D valid loss: 0.287975],[D fake loss: 0.000000], [G loss(mse): 0.385703, G loss(w): 0.126915]\n",
      "Epoch 480/10001\n",
      "[D valid loss: 0.287494],[D fake loss: 0.000000], [G loss(mse): 0.379635, G loss(w): 0.122009]\n",
      "Epoch 481/10001\n",
      "[D valid loss: 0.287209],[D fake loss: 0.000000], [G loss(mse): 0.387783, G loss(w): 0.127989]\n",
      "Epoch 482/10001\n",
      "[D valid loss: 0.286645],[D fake loss: 0.000000], [G loss(mse): 0.386580, G loss(w): 0.124190]\n",
      "Epoch 483/10001\n",
      "[D valid loss: 0.286139],[D fake loss: 0.000000], [G loss(mse): 0.380667, G loss(w): 0.122629]\n",
      "Epoch 484/10001\n",
      "[D valid loss: 0.285670],[D fake loss: 0.000000], [G loss(mse): 0.380628, G loss(w): 0.124582]\n",
      "Epoch 485/10001\n",
      "[D valid loss: 0.285009],[D fake loss: 0.000000], [G loss(mse): 0.380765, G loss(w): 0.124817]\n",
      "Epoch 486/10001\n",
      "[D valid loss: 0.284592],[D fake loss: 0.000000], [G loss(mse): 0.383413, G loss(w): 0.131624]\n",
      "Epoch 487/10001\n",
      "[D valid loss: 0.284166],[D fake loss: 0.000000], [G loss(mse): 0.379446, G loss(w): 0.126295]\n",
      "Epoch 488/10001\n",
      "[D valid loss: 0.283703],[D fake loss: 0.000000], [G loss(mse): 0.371769, G loss(w): 0.121377]\n",
      "Epoch 489/10001\n",
      "[D valid loss: 0.283181],[D fake loss: 0.000000], [G loss(mse): 0.375921, G loss(w): 0.122601]\n",
      "Epoch 490/10001\n",
      "[D valid loss: 0.282783],[D fake loss: 0.000000], [G loss(mse): 0.376002, G loss(w): 0.124676]\n",
      "Epoch 491/10001\n",
      "[D valid loss: 0.282174],[D fake loss: 0.000000], [G loss(mse): 0.378503, G loss(w): 0.120738]\n",
      "Epoch 492/10001\n",
      "[D valid loss: 0.281737],[D fake loss: 0.000000], [G loss(mse): 0.371317, G loss(w): 0.117547]\n",
      "Epoch 493/10001\n",
      "[D valid loss: 0.281553],[D fake loss: 0.000000], [G loss(mse): 0.371722, G loss(w): 0.119960]\n",
      "Epoch 494/10001\n",
      "[D valid loss: 0.280767],[D fake loss: 0.000000], [G loss(mse): 0.374640, G loss(w): 0.123583]\n",
      "Epoch 495/10001\n",
      "[D valid loss: 0.280316],[D fake loss: 0.000000], [G loss(mse): 0.374967, G loss(w): 0.124420]\n",
      "Epoch 496/10001\n",
      "[D valid loss: 0.279909],[D fake loss: 0.000000], [G loss(mse): 0.373143, G loss(w): 0.125805]\n",
      "Epoch 497/10001\n",
      "[D valid loss: 0.279416],[D fake loss: 0.000000], [G loss(mse): 0.374104, G loss(w): 0.125790]\n",
      "Epoch 498/10001\n",
      "[D valid loss: 0.278926],[D fake loss: 0.000000], [G loss(mse): 0.370341, G loss(w): 0.120863]\n",
      "Epoch 499/10001\n",
      "[D valid loss: 0.278595],[D fake loss: 0.000000], [G loss(mse): 0.377629, G loss(w): 0.125040]\n",
      "Epoch 500/10001\n",
      "[D valid loss: 0.278000],[D fake loss: 0.000000], [G loss(mse): 0.368873, G loss(w): 0.119059]\n",
      "Epoch 501/10001\n",
      "[D valid loss: 0.277379],[D fake loss: 0.000000], [G loss(mse): 0.366837, G loss(w): 0.115634]\n",
      "Epoch 502/10001\n",
      "[D valid loss: 0.277033],[D fake loss: 0.000000], [G loss(mse): 0.364339, G loss(w): 0.115695]\n",
      "Epoch 503/10001\n",
      "[D valid loss: 0.276511],[D fake loss: 0.000000], [G loss(mse): 0.364395, G loss(w): 0.114291]\n",
      "Epoch 504/10001\n",
      "[D valid loss: 0.276036],[D fake loss: 0.000000], [G loss(mse): 0.370773, G loss(w): 0.119880]\n",
      "Epoch 505/10001\n",
      "[D valid loss: 0.275928],[D fake loss: 0.000000], [G loss(mse): 0.361263, G loss(w): 0.118026]\n",
      "Epoch 506/10001\n",
      "[D valid loss: 0.274969],[D fake loss: 0.000000], [G loss(mse): 0.364498, G loss(w): 0.120684]\n",
      "Epoch 507/10001\n",
      "[D valid loss: 0.274648],[D fake loss: 0.000000], [G loss(mse): 0.363689, G loss(w): 0.113669]\n",
      "Epoch 508/10001\n",
      "[D valid loss: 0.274244],[D fake loss: 0.000000], [G loss(mse): 0.361848, G loss(w): 0.115290]\n",
      "Epoch 509/10001\n",
      "[D valid loss: 0.274019],[D fake loss: 0.000000], [G loss(mse): 0.363413, G loss(w): 0.117179]\n",
      "Epoch 510/10001\n",
      "[D valid loss: 0.273441],[D fake loss: 0.000000], [G loss(mse): 0.361867, G loss(w): 0.115972]\n",
      "Epoch 511/10001\n",
      "[D valid loss: 0.272756],[D fake loss: 0.000000], [G loss(mse): 0.358709, G loss(w): 0.116900]\n",
      "Epoch 512/10001\n",
      "[D valid loss: 0.272301],[D fake loss: 0.000000], [G loss(mse): 0.364295, G loss(w): 0.121171]\n",
      "Epoch 513/10001\n",
      "[D valid loss: 0.271805],[D fake loss: 0.000000], [G loss(mse): 0.367115, G loss(w): 0.125205]\n",
      "Epoch 514/10001\n",
      "[D valid loss: 0.271360],[D fake loss: 0.000000], [G loss(mse): 0.364685, G loss(w): 0.120898]\n",
      "Epoch 515/10001\n",
      "[D valid loss: 0.270836],[D fake loss: 0.000000], [G loss(mse): 0.357844, G loss(w): 0.116971]\n",
      "Epoch 516/10001\n",
      "[D valid loss: 0.270478],[D fake loss: 0.000000], [G loss(mse): 0.354870, G loss(w): 0.116225]\n",
      "Epoch 517/10001\n",
      "[D valid loss: 0.269757],[D fake loss: 0.000000], [G loss(mse): 0.359589, G loss(w): 0.120359]\n",
      "Epoch 518/10001\n",
      "[D valid loss: 0.269413],[D fake loss: 0.000000], [G loss(mse): 0.355363, G loss(w): 0.116564]\n",
      "Epoch 519/10001\n",
      "[D valid loss: 0.269047],[D fake loss: 0.000000], [G loss(mse): 0.361815, G loss(w): 0.120064]\n",
      "Epoch 520/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.268584],[D fake loss: 0.000000], [G loss(mse): 0.360850, G loss(w): 0.118489]\n",
      "Epoch 521/10001\n",
      "[D valid loss: 0.268092],[D fake loss: 0.000000], [G loss(mse): 0.363246, G loss(w): 0.120669]\n",
      "Epoch 522/10001\n",
      "[D valid loss: 0.267558],[D fake loss: 0.000000], [G loss(mse): 0.354442, G loss(w): 0.118307]\n",
      "Epoch 523/10001\n",
      "[D valid loss: 0.267035],[D fake loss: 0.000000], [G loss(mse): 0.348051, G loss(w): 0.109831]\n",
      "Epoch 524/10001\n",
      "[D valid loss: 0.266641],[D fake loss: 0.000000], [G loss(mse): 0.355119, G loss(w): 0.112631]\n",
      "Epoch 525/10001\n",
      "[D valid loss: 0.266301],[D fake loss: 0.000000], [G loss(mse): 0.351807, G loss(w): 0.112705]\n",
      "Epoch 526/10001\n",
      "[D valid loss: 0.265767],[D fake loss: 0.000000], [G loss(mse): 0.351138, G loss(w): 0.113578]\n",
      "Epoch 527/10001\n",
      "[D valid loss: 0.265193],[D fake loss: 0.000000], [G loss(mse): 0.352908, G loss(w): 0.118245]\n",
      "Epoch 528/10001\n",
      "[D valid loss: 0.265151],[D fake loss: 0.000000], [G loss(mse): 0.349455, G loss(w): 0.110149]\n",
      "Epoch 529/10001\n",
      "[D valid loss: 0.264445],[D fake loss: 0.000000], [G loss(mse): 0.353522, G loss(w): 0.119027]\n",
      "Epoch 530/10001\n",
      "[D valid loss: 0.263802],[D fake loss: 0.000000], [G loss(mse): 0.360063, G loss(w): 0.123391]\n",
      "Epoch 531/10001\n",
      "[D valid loss: 0.263514],[D fake loss: 0.000000], [G loss(mse): 0.347441, G loss(w): 0.113378]\n",
      "Epoch 532/10001\n",
      "[D valid loss: 0.262991],[D fake loss: 0.000000], [G loss(mse): 0.352237, G loss(w): 0.118262]\n",
      "Epoch 533/10001\n",
      "[D valid loss: 0.262323],[D fake loss: 0.000000], [G loss(mse): 0.345965, G loss(w): 0.112325]\n",
      "Epoch 534/10001\n",
      "[D valid loss: 0.262069],[D fake loss: 0.000000], [G loss(mse): 0.344881, G loss(w): 0.113134]\n",
      "Epoch 535/10001\n",
      "[D valid loss: 0.261645],[D fake loss: 0.000000], [G loss(mse): 0.352096, G loss(w): 0.117292]\n",
      "Epoch 536/10001\n",
      "[D valid loss: 0.261279],[D fake loss: 0.000000], [G loss(mse): 0.344905, G loss(w): 0.113150]\n",
      "Epoch 537/10001\n",
      "[D valid loss: 0.260661],[D fake loss: 0.000000], [G loss(mse): 0.346625, G loss(w): 0.115410]\n",
      "Epoch 538/10001\n",
      "[D valid loss: 0.260138],[D fake loss: 0.000000], [G loss(mse): 0.348886, G loss(w): 0.110998]\n",
      "Epoch 539/10001\n",
      "[D valid loss: 0.259650],[D fake loss: 0.000000], [G loss(mse): 0.342669, G loss(w): 0.111523]\n",
      "Epoch 540/10001\n",
      "[D valid loss: 0.259476],[D fake loss: 0.000000], [G loss(mse): 0.344233, G loss(w): 0.114231]\n",
      "Epoch 541/10001\n",
      "[D valid loss: 0.258829],[D fake loss: 0.000000], [G loss(mse): 0.341572, G loss(w): 0.112264]\n",
      "Epoch 542/10001\n",
      "[D valid loss: 0.258489],[D fake loss: 0.000000], [G loss(mse): 0.347503, G loss(w): 0.118034]\n",
      "Epoch 543/10001\n",
      "[D valid loss: 0.258055],[D fake loss: 0.000000], [G loss(mse): 0.345463, G loss(w): 0.113571]\n",
      "Epoch 544/10001\n",
      "[D valid loss: 0.257366],[D fake loss: 0.000000], [G loss(mse): 0.343023, G loss(w): 0.115399]\n",
      "Epoch 545/10001\n",
      "[D valid loss: 0.257166],[D fake loss: 0.000000], [G loss(mse): 0.342331, G loss(w): 0.108423]\n",
      "Epoch 546/10001\n",
      "[D valid loss: 0.256829],[D fake loss: 0.000000], [G loss(mse): 0.343037, G loss(w): 0.111911]\n",
      "Epoch 547/10001\n",
      "[D valid loss: 0.256194],[D fake loss: 0.000000], [G loss(mse): 0.336785, G loss(w): 0.105022]\n",
      "Epoch 548/10001\n",
      "[D valid loss: 0.255467],[D fake loss: 0.000000], [G loss(mse): 0.337732, G loss(w): 0.109761]\n",
      "Epoch 549/10001\n",
      "[D valid loss: 0.255177],[D fake loss: 0.000000], [G loss(mse): 0.343032, G loss(w): 0.117758]\n",
      "Epoch 550/10001\n",
      "[D valid loss: 0.254788],[D fake loss: 0.000000], [G loss(mse): 0.348608, G loss(w): 0.120979]\n",
      "Epoch 551/10001\n",
      "[D valid loss: 0.254255],[D fake loss: 0.000000], [G loss(mse): 0.346067, G loss(w): 0.120233]\n",
      "Epoch 552/10001\n",
      "[D valid loss: 0.253768],[D fake loss: 0.000000], [G loss(mse): 0.344811, G loss(w): 0.112188]\n",
      "Epoch 553/10001\n",
      "[D valid loss: 0.253259],[D fake loss: 0.000000], [G loss(mse): 0.339412, G loss(w): 0.112170]\n",
      "Epoch 554/10001\n",
      "[D valid loss: 0.252979],[D fake loss: 0.000000], [G loss(mse): 0.340966, G loss(w): 0.113074]\n",
      "Epoch 555/10001\n",
      "[D valid loss: 0.252297],[D fake loss: 0.000000], [G loss(mse): 0.337048, G loss(w): 0.107317]\n",
      "Epoch 556/10001\n",
      "[D valid loss: 0.252012],[D fake loss: 0.000000], [G loss(mse): 0.344699, G loss(w): 0.117377]\n",
      "Epoch 557/10001\n",
      "[D valid loss: 0.251879],[D fake loss: 0.000000], [G loss(mse): 0.335086, G loss(w): 0.110625]\n",
      "Epoch 558/10001\n",
      "[D valid loss: 0.251143],[D fake loss: 0.000000], [G loss(mse): 0.341061, G loss(w): 0.114042]\n",
      "Epoch 559/10001\n",
      "[D valid loss: 0.250846],[D fake loss: 0.000000], [G loss(mse): 0.329685, G loss(w): 0.106365]\n",
      "Epoch 560/10001\n",
      "[D valid loss: 0.250105],[D fake loss: 0.000000], [G loss(mse): 0.337050, G loss(w): 0.112762]\n",
      "Epoch 561/10001\n",
      "[D valid loss: 0.249635],[D fake loss: 0.000000], [G loss(mse): 0.333276, G loss(w): 0.113646]\n",
      "Epoch 562/10001\n",
      "[D valid loss: 0.249228],[D fake loss: 0.000000], [G loss(mse): 0.335188, G loss(w): 0.113069]\n",
      "Epoch 563/10001\n",
      "[D valid loss: 0.248770],[D fake loss: 0.000000], [G loss(mse): 0.333802, G loss(w): 0.112204]\n",
      "Epoch 564/10001\n",
      "[D valid loss: 0.248218],[D fake loss: 0.000000], [G loss(mse): 0.325205, G loss(w): 0.105122]\n",
      "Epoch 565/10001\n",
      "[D valid loss: 0.247974],[D fake loss: 0.000000], [G loss(mse): 0.339840, G loss(w): 0.116347]\n",
      "Epoch 566/10001\n",
      "[D valid loss: 0.247352],[D fake loss: 0.000000], [G loss(mse): 0.329718, G loss(w): 0.109096]\n",
      "Epoch 567/10001\n",
      "[D valid loss: 0.247279],[D fake loss: 0.000000], [G loss(mse): 0.335139, G loss(w): 0.112015]\n",
      "Epoch 568/10001\n",
      "[D valid loss: 0.246895],[D fake loss: 0.000000], [G loss(mse): 0.338902, G loss(w): 0.116678]\n",
      "Epoch 569/10001\n",
      "[D valid loss: 0.246380],[D fake loss: 0.000000], [G loss(mse): 0.327404, G loss(w): 0.109934]\n",
      "Epoch 570/10001\n",
      "[D valid loss: 0.245618],[D fake loss: 0.000000], [G loss(mse): 0.332708, G loss(w): 0.112384]\n",
      "Epoch 571/10001\n",
      "[D valid loss: 0.245266],[D fake loss: 0.000000], [G loss(mse): 0.327058, G loss(w): 0.110627]\n",
      "Epoch 572/10001\n",
      "[D valid loss: 0.244584],[D fake loss: 0.000000], [G loss(mse): 0.328354, G loss(w): 0.111560]\n",
      "Epoch 573/10001\n",
      "[D valid loss: 0.244608],[D fake loss: 0.000000], [G loss(mse): 0.324232, G loss(w): 0.108872]\n",
      "Epoch 574/10001\n",
      "[D valid loss: 0.244566],[D fake loss: 0.000000], [G loss(mse): 0.330213, G loss(w): 0.114804]\n",
      "Epoch 575/10001\n",
      "[D valid loss: 0.243562],[D fake loss: 0.000000], [G loss(mse): 0.321052, G loss(w): 0.108133]\n",
      "Epoch 576/10001\n",
      "[D valid loss: 0.243146],[D fake loss: 0.000000], [G loss(mse): 0.322897, G loss(w): 0.109692]\n",
      "Epoch 577/10001\n",
      "[D valid loss: 0.242701],[D fake loss: 0.000000], [G loss(mse): 0.317926, G loss(w): 0.106026]\n",
      "Epoch 578/10001\n",
      "[D valid loss: 0.241975],[D fake loss: 0.000000], [G loss(mse): 0.316044, G loss(w): 0.105307]\n",
      "Epoch 579/10001\n",
      "[D valid loss: 0.241622],[D fake loss: 0.000000], [G loss(mse): 0.326682, G loss(w): 0.114045]\n",
      "Epoch 580/10001\n",
      "[D valid loss: 0.241095],[D fake loss: 0.000000], [G loss(mse): 0.325954, G loss(w): 0.112937]\n",
      "Epoch 581/10001\n",
      "[D valid loss: 0.240771],[D fake loss: 0.000000], [G loss(mse): 0.325924, G loss(w): 0.112431]\n",
      "Epoch 582/10001\n",
      "[D valid loss: 0.240300],[D fake loss: 0.000000], [G loss(mse): 0.319504, G loss(w): 0.107316]\n",
      "Epoch 583/10001\n",
      "[D valid loss: 0.240237],[D fake loss: 0.000000], [G loss(mse): 0.322519, G loss(w): 0.107883]\n",
      "Epoch 584/10001\n",
      "[D valid loss: 0.239597],[D fake loss: 0.000000], [G loss(mse): 0.328063, G loss(w): 0.113576]\n",
      "Epoch 585/10001\n",
      "[D valid loss: 0.239054],[D fake loss: 0.000000], [G loss(mse): 0.321653, G loss(w): 0.108005]\n",
      "Epoch 586/10001\n",
      "[D valid loss: 0.238723],[D fake loss: 0.000000], [G loss(mse): 0.325202, G loss(w): 0.113760]\n",
      "Epoch 587/10001\n",
      "[D valid loss: 0.238222],[D fake loss: 0.000000], [G loss(mse): 0.319822, G loss(w): 0.105646]\n",
      "Epoch 588/10001\n",
      "[D valid loss: 0.237787],[D fake loss: 0.000000], [G loss(mse): 0.321680, G loss(w): 0.106702]\n",
      "Epoch 589/10001\n",
      "[D valid loss: 0.237434],[D fake loss: 0.000000], [G loss(mse): 0.312904, G loss(w): 0.102975]\n",
      "Epoch 590/10001\n",
      "[D valid loss: 0.236833],[D fake loss: 0.000000], [G loss(mse): 0.309438, G loss(w): 0.100723]\n",
      "Epoch 591/10001\n",
      "[D valid loss: 0.236330],[D fake loss: 0.000000], [G loss(mse): 0.314499, G loss(w): 0.107774]\n",
      "Epoch 592/10001\n",
      "[D valid loss: 0.235758],[D fake loss: 0.000000], [G loss(mse): 0.305880, G loss(w): 0.100417]\n",
      "Epoch 593/10001\n",
      "[D valid loss: 0.235501],[D fake loss: 0.000000], [G loss(mse): 0.314034, G loss(w): 0.102661]\n",
      "Epoch 594/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.235009],[D fake loss: 0.000000], [G loss(mse): 0.318712, G loss(w): 0.112633]\n",
      "Epoch 595/10001\n",
      "[D valid loss: 0.234474],[D fake loss: 0.000000], [G loss(mse): 0.313741, G loss(w): 0.106644]\n",
      "Epoch 596/10001\n",
      "[D valid loss: 0.234023],[D fake loss: 0.000000], [G loss(mse): 0.318325, G loss(w): 0.111043]\n",
      "Epoch 597/10001\n",
      "[D valid loss: 0.233849],[D fake loss: 0.000000], [G loss(mse): 0.315740, G loss(w): 0.112143]\n",
      "Epoch 598/10001\n",
      "[D valid loss: 0.233440],[D fake loss: 0.000000], [G loss(mse): 0.311076, G loss(w): 0.106276]\n",
      "Epoch 599/10001\n",
      "[D valid loss: 0.232706],[D fake loss: 0.000000], [G loss(mse): 0.307671, G loss(w): 0.104311]\n",
      "Epoch 600/10001\n",
      "[D valid loss: 0.232463],[D fake loss: 0.000000], [G loss(mse): 0.308052, G loss(w): 0.104192]\n",
      "Epoch 601/10001\n",
      "[D valid loss: 0.232151],[D fake loss: 0.000000], [G loss(mse): 0.309058, G loss(w): 0.108390]\n",
      "Epoch 602/10001\n",
      "[D valid loss: 0.231605],[D fake loss: 0.000000], [G loss(mse): 0.315834, G loss(w): 0.110185]\n",
      "Epoch 603/10001\n",
      "[D valid loss: 0.231022],[D fake loss: 0.000000], [G loss(mse): 0.310898, G loss(w): 0.106888]\n",
      "Epoch 604/10001\n",
      "[D valid loss: 0.230487],[D fake loss: 0.000000], [G loss(mse): 0.307439, G loss(w): 0.106175]\n",
      "Epoch 605/10001\n",
      "[D valid loss: 0.230281],[D fake loss: 0.000000], [G loss(mse): 0.307696, G loss(w): 0.105971]\n",
      "Epoch 606/10001\n",
      "[D valid loss: 0.230013],[D fake loss: 0.000000], [G loss(mse): 0.297674, G loss(w): 0.095538]\n",
      "Epoch 607/10001\n",
      "[D valid loss: 0.229192],[D fake loss: 0.000000], [G loss(mse): 0.309915, G loss(w): 0.104846]\n",
      "Epoch 608/10001\n",
      "[D valid loss: 0.228814],[D fake loss: 0.000000], [G loss(mse): 0.306283, G loss(w): 0.103887]\n",
      "Epoch 609/10001\n",
      "[D valid loss: 0.228466],[D fake loss: 0.000000], [G loss(mse): 0.308445, G loss(w): 0.107417]\n",
      "Epoch 610/10001\n",
      "[D valid loss: 0.228195],[D fake loss: 0.000000], [G loss(mse): 0.306185, G loss(w): 0.108658]\n",
      "Epoch 611/10001\n",
      "[D valid loss: 0.227600],[D fake loss: 0.000000], [G loss(mse): 0.311427, G loss(w): 0.111396]\n",
      "Epoch 612/10001\n",
      "[D valid loss: 0.227258],[D fake loss: 0.000000], [G loss(mse): 0.303051, G loss(w): 0.104566]\n",
      "Epoch 613/10001\n",
      "[D valid loss: 0.226876],[D fake loss: 0.000000], [G loss(mse): 0.303604, G loss(w): 0.106002]\n",
      "Epoch 614/10001\n",
      "[D valid loss: 0.226289],[D fake loss: 0.000000], [G loss(mse): 0.307244, G loss(w): 0.108695]\n",
      "Epoch 615/10001\n",
      "[D valid loss: 0.226038],[D fake loss: 0.000000], [G loss(mse): 0.298288, G loss(w): 0.101736]\n",
      "Epoch 616/10001\n",
      "[D valid loss: 0.225600],[D fake loss: 0.000000], [G loss(mse): 0.303573, G loss(w): 0.106580]\n",
      "Epoch 617/10001\n",
      "[D valid loss: 0.225002],[D fake loss: 0.000000], [G loss(mse): 0.303787, G loss(w): 0.105835]\n",
      "Epoch 618/10001\n",
      "[D valid loss: 0.224874],[D fake loss: 0.000000], [G loss(mse): 0.297361, G loss(w): 0.105155]\n",
      "Epoch 619/10001\n",
      "[D valid loss: 0.224178],[D fake loss: 0.000000], [G loss(mse): 0.303189, G loss(w): 0.109185]\n",
      "Epoch 620/10001\n",
      "[D valid loss: 0.223710],[D fake loss: 0.000000], [G loss(mse): 0.294427, G loss(w): 0.102005]\n",
      "Epoch 621/10001\n",
      "[D valid loss: 0.223343],[D fake loss: 0.000000], [G loss(mse): 0.293638, G loss(w): 0.101962]\n",
      "Epoch 622/10001\n",
      "[D valid loss: 0.222895],[D fake loss: 0.000000], [G loss(mse): 0.295034, G loss(w): 0.101271]\n",
      "Epoch 623/10001\n",
      "[D valid loss: 0.222350],[D fake loss: 0.000000], [G loss(mse): 0.296463, G loss(w): 0.104912]\n",
      "Epoch 624/10001\n",
      "[D valid loss: 0.221909],[D fake loss: 0.000000], [G loss(mse): 0.295737, G loss(w): 0.101509]\n",
      "Epoch 625/10001\n",
      "[D valid loss: 0.221854],[D fake loss: 0.000000], [G loss(mse): 0.298115, G loss(w): 0.107672]\n",
      "Epoch 626/10001\n",
      "[D valid loss: 0.221182],[D fake loss: 0.000000], [G loss(mse): 0.292985, G loss(w): 0.100257]\n",
      "Epoch 627/10001\n",
      "[D valid loss: 0.220736],[D fake loss: 0.000000], [G loss(mse): 0.300583, G loss(w): 0.106473]\n",
      "Epoch 628/10001\n",
      "[D valid loss: 0.220326],[D fake loss: 0.000000], [G loss(mse): 0.301008, G loss(w): 0.110673]\n",
      "Epoch 629/10001\n",
      "[D valid loss: 0.219841],[D fake loss: 0.000000], [G loss(mse): 0.299014, G loss(w): 0.107022]\n",
      "Epoch 630/10001\n",
      "[D valid loss: 0.219639],[D fake loss: 0.000000], [G loss(mse): 0.296599, G loss(w): 0.103326]\n",
      "Epoch 631/10001\n",
      "[D valid loss: 0.218955],[D fake loss: 0.000000], [G loss(mse): 0.294178, G loss(w): 0.100678]\n",
      "Epoch 632/10001\n",
      "[D valid loss: 0.218551],[D fake loss: 0.000000], [G loss(mse): 0.291292, G loss(w): 0.102910]\n",
      "Epoch 633/10001\n",
      "[D valid loss: 0.218163],[D fake loss: 0.000000], [G loss(mse): 0.291325, G loss(w): 0.103840]\n",
      "Epoch 634/10001\n",
      "[D valid loss: 0.217895],[D fake loss: 0.000000], [G loss(mse): 0.289159, G loss(w): 0.098760]\n",
      "Epoch 635/10001\n",
      "[D valid loss: 0.217413],[D fake loss: 0.000000], [G loss(mse): 0.288354, G loss(w): 0.098226]\n",
      "Epoch 636/10001\n",
      "[D valid loss: 0.217269],[D fake loss: 0.000000], [G loss(mse): 0.299870, G loss(w): 0.108154]\n",
      "Epoch 637/10001\n",
      "[D valid loss: 0.216693],[D fake loss: 0.000000], [G loss(mse): 0.291530, G loss(w): 0.101835]\n",
      "Epoch 638/10001\n",
      "[D valid loss: 0.216031],[D fake loss: 0.000000], [G loss(mse): 0.293484, G loss(w): 0.104286]\n",
      "Epoch 639/10001\n",
      "[D valid loss: 0.215860],[D fake loss: 0.000000], [G loss(mse): 0.287007, G loss(w): 0.100471]\n",
      "Epoch 640/10001\n",
      "[D valid loss: 0.215208],[D fake loss: 0.000000], [G loss(mse): 0.289978, G loss(w): 0.101588]\n",
      "Epoch 641/10001\n",
      "[D valid loss: 0.214653],[D fake loss: 0.000000], [G loss(mse): 0.290761, G loss(w): 0.107101]\n",
      "Epoch 642/10001\n",
      "[D valid loss: 0.214797],[D fake loss: 0.000000], [G loss(mse): 0.284085, G loss(w): 0.096948]\n",
      "Epoch 643/10001\n",
      "[D valid loss: 0.214649],[D fake loss: 0.000000], [G loss(mse): 0.288784, G loss(w): 0.101113]\n",
      "Epoch 644/10001\n",
      "[D valid loss: 0.213542],[D fake loss: 0.000000], [G loss(mse): 0.287016, G loss(w): 0.099260]\n",
      "Epoch 645/10001\n",
      "[D valid loss: 0.213269],[D fake loss: 0.000000], [G loss(mse): 0.288281, G loss(w): 0.100780]\n",
      "Epoch 646/10001\n",
      "[D valid loss: 0.213008],[D fake loss: 0.000000], [G loss(mse): 0.286016, G loss(w): 0.100618]\n",
      "Epoch 647/10001\n",
      "[D valid loss: 0.212282],[D fake loss: 0.000000], [G loss(mse): 0.285164, G loss(w): 0.101883]\n",
      "Epoch 648/10001\n",
      "[D valid loss: 0.211923],[D fake loss: 0.000000], [G loss(mse): 0.281241, G loss(w): 0.098145]\n",
      "Epoch 649/10001\n",
      "[D valid loss: 0.211418],[D fake loss: 0.000000], [G loss(mse): 0.283954, G loss(w): 0.101034]\n",
      "Epoch 650/10001\n",
      "[D valid loss: 0.211155],[D fake loss: 0.000000], [G loss(mse): 0.284438, G loss(w): 0.102865]\n",
      "Epoch 651/10001\n",
      "[D valid loss: 0.210823],[D fake loss: 0.000000], [G loss(mse): 0.286128, G loss(w): 0.103258]\n",
      "Epoch 652/10001\n",
      "[D valid loss: 0.210505],[D fake loss: 0.000000], [G loss(mse): 0.280936, G loss(w): 0.100306]\n",
      "Epoch 653/10001\n",
      "[D valid loss: 0.209691],[D fake loss: 0.000000], [G loss(mse): 0.280467, G loss(w): 0.101089]\n",
      "Epoch 654/10001\n",
      "[D valid loss: 0.209587],[D fake loss: 0.000000], [G loss(mse): 0.283448, G loss(w): 0.104278]\n",
      "Epoch 655/10001\n",
      "[D valid loss: 0.209088],[D fake loss: 0.000000], [G loss(mse): 0.276288, G loss(w): 0.097467]\n",
      "Epoch 656/10001\n",
      "[D valid loss: 0.208703],[D fake loss: 0.000000], [G loss(mse): 0.282738, G loss(w): 0.101217]\n",
      "Epoch 657/10001\n",
      "[D valid loss: 0.208219],[D fake loss: 0.000000], [G loss(mse): 0.289154, G loss(w): 0.100786]\n",
      "Epoch 658/10001\n",
      "[D valid loss: 0.207938],[D fake loss: 0.000000], [G loss(mse): 0.285412, G loss(w): 0.101524]\n",
      "Epoch 659/10001\n",
      "[D valid loss: 0.207546],[D fake loss: 0.000000], [G loss(mse): 0.273964, G loss(w): 0.097082]\n",
      "Epoch 660/10001\n",
      "[D valid loss: 0.206946],[D fake loss: 0.000000], [G loss(mse): 0.271547, G loss(w): 0.094348]\n",
      "Epoch 661/10001\n",
      "[D valid loss: 0.206672],[D fake loss: 0.000000], [G loss(mse): 0.278133, G loss(w): 0.099717]\n",
      "Epoch 662/10001\n",
      "[D valid loss: 0.206459],[D fake loss: 0.000000], [G loss(mse): 0.275072, G loss(w): 0.099267]\n",
      "Epoch 663/10001\n",
      "[D valid loss: 0.205924],[D fake loss: 0.000000], [G loss(mse): 0.279574, G loss(w): 0.100954]\n",
      "Epoch 664/10001\n",
      "[D valid loss: 0.205634],[D fake loss: 0.000000], [G loss(mse): 0.279104, G loss(w): 0.101640]\n",
      "Epoch 665/10001\n",
      "[D valid loss: 0.205280],[D fake loss: 0.000000], [G loss(mse): 0.284694, G loss(w): 0.106743]\n",
      "Epoch 666/10001\n",
      "[D valid loss: 0.204544],[D fake loss: 0.000000], [G loss(mse): 0.278444, G loss(w): 0.100915]\n",
      "Epoch 667/10001\n",
      "[D valid loss: 0.204297],[D fake loss: 0.000000], [G loss(mse): 0.279048, G loss(w): 0.103475]\n",
      "Epoch 668/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.203887],[D fake loss: 0.000000], [G loss(mse): 0.275989, G loss(w): 0.099987]\n",
      "Epoch 669/10001\n",
      "[D valid loss: 0.203699],[D fake loss: 0.000000], [G loss(mse): 0.276511, G loss(w): 0.099455]\n",
      "Epoch 670/10001\n",
      "[D valid loss: 0.203335],[D fake loss: 0.000000], [G loss(mse): 0.276406, G loss(w): 0.096480]\n",
      "Epoch 671/10001\n",
      "[D valid loss: 0.202845],[D fake loss: 0.000000], [G loss(mse): 0.275737, G loss(w): 0.094109]\n",
      "Epoch 672/10001\n",
      "[D valid loss: 0.202305],[D fake loss: 0.000000], [G loss(mse): 0.273623, G loss(w): 0.097816]\n",
      "Epoch 673/10001\n",
      "[D valid loss: 0.201851],[D fake loss: 0.000000], [G loss(mse): 0.274336, G loss(w): 0.096760]\n",
      "Epoch 674/10001\n",
      "[D valid loss: 0.201469],[D fake loss: 0.000000], [G loss(mse): 0.280157, G loss(w): 0.101564]\n",
      "Epoch 675/10001\n",
      "[D valid loss: 0.201007],[D fake loss: 0.000000], [G loss(mse): 0.273876, G loss(w): 0.096230]\n",
      "Epoch 676/10001\n",
      "[D valid loss: 0.200685],[D fake loss: 0.000000], [G loss(mse): 0.275340, G loss(w): 0.099240]\n",
      "Epoch 677/10001\n",
      "[D valid loss: 0.200281],[D fake loss: 0.000000], [G loss(mse): 0.268690, G loss(w): 0.094976]\n",
      "Epoch 678/10001\n",
      "[D valid loss: 0.199901],[D fake loss: 0.000000], [G loss(mse): 0.276559, G loss(w): 0.098543]\n",
      "Epoch 679/10001\n",
      "[D valid loss: 0.199564],[D fake loss: 0.000000], [G loss(mse): 0.263898, G loss(w): 0.091309]\n",
      "Epoch 680/10001\n",
      "[D valid loss: 0.199189],[D fake loss: 0.000000], [G loss(mse): 0.271109, G loss(w): 0.097340]\n",
      "Epoch 681/10001\n",
      "[D valid loss: 0.198495],[D fake loss: 0.000000], [G loss(mse): 0.267940, G loss(w): 0.096229]\n",
      "Epoch 682/10001\n",
      "[D valid loss: 0.198600],[D fake loss: 0.000000], [G loss(mse): 0.272602, G loss(w): 0.101564]\n",
      "Epoch 683/10001\n",
      "[D valid loss: 0.197772],[D fake loss: 0.000000], [G loss(mse): 0.269329, G loss(w): 0.101361]\n",
      "Epoch 684/10001\n",
      "[D valid loss: 0.197497],[D fake loss: 0.000000], [G loss(mse): 0.268301, G loss(w): 0.096810]\n",
      "Epoch 685/10001\n",
      "[D valid loss: 0.197017],[D fake loss: 0.000000], [G loss(mse): 0.270266, G loss(w): 0.094454]\n",
      "Epoch 686/10001\n",
      "[D valid loss: 0.196750],[D fake loss: 0.000000], [G loss(mse): 0.264535, G loss(w): 0.094260]\n",
      "Epoch 687/10001\n",
      "[D valid loss: 0.196645],[D fake loss: 0.000000], [G loss(mse): 0.265324, G loss(w): 0.092846]\n",
      "Epoch 688/10001\n",
      "[D valid loss: 0.196040],[D fake loss: 0.000000], [G loss(mse): 0.273696, G loss(w): 0.101502]\n",
      "Epoch 689/10001\n",
      "[D valid loss: 0.195628],[D fake loss: 0.000000], [G loss(mse): 0.266487, G loss(w): 0.096315]\n",
      "Epoch 690/10001\n",
      "[D valid loss: 0.195056],[D fake loss: 0.000000], [G loss(mse): 0.260425, G loss(w): 0.093754]\n",
      "Epoch 691/10001\n",
      "[D valid loss: 0.194695],[D fake loss: 0.000000], [G loss(mse): 0.263039, G loss(w): 0.095992]\n",
      "Epoch 692/10001\n",
      "[D valid loss: 0.194607],[D fake loss: 0.000000], [G loss(mse): 0.262358, G loss(w): 0.095686]\n",
      "Epoch 693/10001\n",
      "[D valid loss: 0.194275],[D fake loss: 0.000000], [G loss(mse): 0.266650, G loss(w): 0.100266]\n",
      "Epoch 694/10001\n",
      "[D valid loss: 0.193701],[D fake loss: 0.000000], [G loss(mse): 0.264989, G loss(w): 0.096133]\n",
      "Epoch 695/10001\n",
      "[D valid loss: 0.193190],[D fake loss: 0.000000], [G loss(mse): 0.265235, G loss(w): 0.096255]\n",
      "Epoch 696/10001\n",
      "[D valid loss: 0.192843],[D fake loss: 0.000000], [G loss(mse): 0.265486, G loss(w): 0.099145]\n",
      "Epoch 697/10001\n",
      "[D valid loss: 0.192528],[D fake loss: 0.000000], [G loss(mse): 0.262568, G loss(w): 0.099057]\n",
      "Epoch 698/10001\n",
      "[D valid loss: 0.191973],[D fake loss: 0.000000], [G loss(mse): 0.259933, G loss(w): 0.094410]\n",
      "Epoch 699/10001\n",
      "[D valid loss: 0.191417],[D fake loss: 0.000000], [G loss(mse): 0.259512, G loss(w): 0.092679]\n",
      "Epoch 700/10001\n",
      "[D valid loss: 0.191450],[D fake loss: 0.000000], [G loss(mse): 0.262261, G loss(w): 0.093275]\n",
      "Epoch 701/10001\n",
      "[D valid loss: 0.191003],[D fake loss: 0.000000], [G loss(mse): 0.263481, G loss(w): 0.096565]\n",
      "Epoch 702/10001\n",
      "[D valid loss: 0.190499],[D fake loss: 0.000000], [G loss(mse): 0.260688, G loss(w): 0.095505]\n",
      "Epoch 703/10001\n",
      "[D valid loss: 0.190720],[D fake loss: 0.000000], [G loss(mse): 0.264164, G loss(w): 0.094166]\n",
      "Epoch 704/10001\n",
      "[D valid loss: 0.190066],[D fake loss: 0.000000], [G loss(mse): 0.263328, G loss(w): 0.096354]\n",
      "Epoch 705/10001\n",
      "[D valid loss: 0.189512],[D fake loss: 0.000000], [G loss(mse): 0.266388, G loss(w): 0.097649]\n",
      "Epoch 706/10001\n",
      "[D valid loss: 0.189649],[D fake loss: 0.000000], [G loss(mse): 0.261241, G loss(w): 0.095210]\n",
      "Epoch 707/10001\n",
      "[D valid loss: 0.188894],[D fake loss: 0.000000], [G loss(mse): 0.257758, G loss(w): 0.088196]\n",
      "Epoch 708/10001\n",
      "[D valid loss: 0.188404],[D fake loss: 0.000000], [G loss(mse): 0.266249, G loss(w): 0.097808]\n",
      "Epoch 709/10001\n",
      "[D valid loss: 0.187853],[D fake loss: 0.000000], [G loss(mse): 0.260088, G loss(w): 0.093988]\n",
      "Epoch 710/10001\n",
      "[D valid loss: 0.187397],[D fake loss: 0.000000], [G loss(mse): 0.259599, G loss(w): 0.093411]\n",
      "Epoch 711/10001\n",
      "[D valid loss: 0.187004],[D fake loss: 0.000000], [G loss(mse): 0.259388, G loss(w): 0.097138]\n",
      "Epoch 712/10001\n",
      "[D valid loss: 0.186745],[D fake loss: 0.000000], [G loss(mse): 0.262677, G loss(w): 0.096897]\n",
      "Epoch 713/10001\n",
      "[D valid loss: 0.186301],[D fake loss: 0.000000], [G loss(mse): 0.256810, G loss(w): 0.093425]\n",
      "Epoch 714/10001\n",
      "[D valid loss: 0.185885],[D fake loss: 0.000000], [G loss(mse): 0.258198, G loss(w): 0.097097]\n",
      "Epoch 715/10001\n",
      "[D valid loss: 0.185746],[D fake loss: 0.000000], [G loss(mse): 0.256730, G loss(w): 0.097086]\n",
      "Epoch 716/10001\n",
      "[D valid loss: 0.185119],[D fake loss: 0.000000], [G loss(mse): 0.252456, G loss(w): 0.092751]\n",
      "Epoch 717/10001\n",
      "[D valid loss: 0.184661],[D fake loss: 0.000000], [G loss(mse): 0.250175, G loss(w): 0.091192]\n",
      "Epoch 718/10001\n",
      "[D valid loss: 0.184370],[D fake loss: 0.000000], [G loss(mse): 0.254543, G loss(w): 0.096783]\n",
      "Epoch 719/10001\n",
      "[D valid loss: 0.184069],[D fake loss: 0.000000], [G loss(mse): 0.250142, G loss(w): 0.094138]\n",
      "Epoch 720/10001\n",
      "[D valid loss: 0.183654],[D fake loss: 0.000000], [G loss(mse): 0.255425, G loss(w): 0.097821]\n",
      "Epoch 721/10001\n",
      "[D valid loss: 0.183199],[D fake loss: 0.000000], [G loss(mse): 0.250126, G loss(w): 0.095047]\n",
      "Epoch 722/10001\n",
      "[D valid loss: 0.182716],[D fake loss: 0.000000], [G loss(mse): 0.243512, G loss(w): 0.087862]\n",
      "Epoch 723/10001\n",
      "[D valid loss: 0.182267],[D fake loss: 0.000000], [G loss(mse): 0.247043, G loss(w): 0.092208]\n",
      "Epoch 724/10001\n",
      "[D valid loss: 0.181991],[D fake loss: 0.000000], [G loss(mse): 0.242907, G loss(w): 0.089926]\n",
      "Epoch 725/10001\n",
      "[D valid loss: 0.181686],[D fake loss: 0.000000], [G loss(mse): 0.252288, G loss(w): 0.099065]\n",
      "Epoch 726/10001\n",
      "[D valid loss: 0.181218],[D fake loss: 0.000000], [G loss(mse): 0.240665, G loss(w): 0.088758]\n",
      "Epoch 727/10001\n",
      "[D valid loss: 0.180801],[D fake loss: 0.000000], [G loss(mse): 0.239244, G loss(w): 0.088855]\n",
      "Epoch 728/10001\n",
      "[D valid loss: 0.180565],[D fake loss: 0.000000], [G loss(mse): 0.245741, G loss(w): 0.095226]\n",
      "Epoch 729/10001\n",
      "[D valid loss: 0.180178],[D fake loss: 0.000000], [G loss(mse): 0.244887, G loss(w): 0.091136]\n",
      "Epoch 730/10001\n",
      "[D valid loss: 0.180445],[D fake loss: 0.000000], [G loss(mse): 0.235880, G loss(w): 0.085698]\n",
      "Epoch 731/10001\n",
      "[D valid loss: 0.179448],[D fake loss: 0.000000], [G loss(mse): 0.243205, G loss(w): 0.093619]\n",
      "Epoch 732/10001\n",
      "[D valid loss: 0.179467],[D fake loss: 0.000000], [G loss(mse): 0.248525, G loss(w): 0.093691]\n",
      "Epoch 733/10001\n",
      "[D valid loss: 0.179236],[D fake loss: 0.000000], [G loss(mse): 0.237121, G loss(w): 0.086969]\n",
      "Epoch 734/10001\n",
      "[D valid loss: 0.178562],[D fake loss: 0.000000], [G loss(mse): 0.242862, G loss(w): 0.093706]\n",
      "Epoch 735/10001\n",
      "[D valid loss: 0.178205],[D fake loss: 0.000000], [G loss(mse): 0.246934, G loss(w): 0.093815]\n",
      "Epoch 736/10001\n",
      "[D valid loss: 0.177856],[D fake loss: 0.000000], [G loss(mse): 0.236770, G loss(w): 0.087441]\n",
      "Epoch 737/10001\n",
      "[D valid loss: 0.177167],[D fake loss: 0.000000], [G loss(mse): 0.240605, G loss(w): 0.092932]\n",
      "Epoch 738/10001\n",
      "[D valid loss: 0.176795],[D fake loss: 0.000000], [G loss(mse): 0.240549, G loss(w): 0.091167]\n",
      "Epoch 739/10001\n",
      "[D valid loss: 0.176510],[D fake loss: 0.000000], [G loss(mse): 0.242992, G loss(w): 0.092228]\n",
      "Epoch 740/10001\n",
      "[D valid loss: 0.176141],[D fake loss: 0.000000], [G loss(mse): 0.236654, G loss(w): 0.088518]\n",
      "Epoch 741/10001\n",
      "[D valid loss: 0.175810],[D fake loss: 0.000000], [G loss(mse): 0.240376, G loss(w): 0.092878]\n",
      "Epoch 742/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.175464],[D fake loss: 0.000000], [G loss(mse): 0.243970, G loss(w): 0.097743]\n",
      "Epoch 743/10001\n",
      "[D valid loss: 0.174983],[D fake loss: 0.000000], [G loss(mse): 0.238665, G loss(w): 0.090789]\n",
      "Epoch 744/10001\n",
      "[D valid loss: 0.174701],[D fake loss: 0.000000], [G loss(mse): 0.238049, G loss(w): 0.092023]\n",
      "Epoch 745/10001\n",
      "[D valid loss: 0.174441],[D fake loss: 0.000000], [G loss(mse): 0.230469, G loss(w): 0.088095]\n",
      "Epoch 746/10001\n",
      "[D valid loss: 0.174629],[D fake loss: 0.000000], [G loss(mse): 0.237687, G loss(w): 0.097561]\n",
      "Epoch 747/10001\n",
      "[D valid loss: 0.174243],[D fake loss: 0.000000], [G loss(mse): 0.235770, G loss(w): 0.094824]\n",
      "Epoch 748/10001\n",
      "[D valid loss: 0.173602],[D fake loss: 0.000000], [G loss(mse): 0.235622, G loss(w): 0.096664]\n",
      "Epoch 749/10001\n",
      "[D valid loss: 0.173020],[D fake loss: 0.000000], [G loss(mse): 0.232650, G loss(w): 0.091000]\n",
      "Epoch 750/10001\n",
      "[D valid loss: 0.172483],[D fake loss: 0.000000], [G loss(mse): 0.229156, G loss(w): 0.088464]\n",
      "Epoch 751/10001\n",
      "[D valid loss: 0.172386],[D fake loss: 0.000000], [G loss(mse): 0.230548, G loss(w): 0.087630]\n",
      "Epoch 752/10001\n",
      "[D valid loss: 0.172198],[D fake loss: 0.000000], [G loss(mse): 0.238332, G loss(w): 0.094457]\n",
      "Epoch 753/10001\n",
      "[D valid loss: 0.171630],[D fake loss: 0.000000], [G loss(mse): 0.235373, G loss(w): 0.091648]\n",
      "Epoch 754/10001\n",
      "[D valid loss: 0.171178],[D fake loss: 0.000000], [G loss(mse): 0.237276, G loss(w): 0.093864]\n",
      "Epoch 755/10001\n",
      "[D valid loss: 0.170741],[D fake loss: 0.000000], [G loss(mse): 0.233876, G loss(w): 0.091706]\n",
      "Epoch 756/10001\n",
      "[D valid loss: 0.170394],[D fake loss: 0.000000], [G loss(mse): 0.230171, G loss(w): 0.089357]\n",
      "Epoch 757/10001\n",
      "[D valid loss: 0.170095],[D fake loss: 0.000000], [G loss(mse): 0.230179, G loss(w): 0.088738]\n",
      "Epoch 758/10001\n",
      "[D valid loss: 0.169835],[D fake loss: 0.000000], [G loss(mse): 0.233599, G loss(w): 0.094307]\n",
      "Epoch 759/10001\n",
      "[D valid loss: 0.169203],[D fake loss: 0.000000], [G loss(mse): 0.231747, G loss(w): 0.091929]\n",
      "Epoch 760/10001\n",
      "[D valid loss: 0.169154],[D fake loss: 0.000000], [G loss(mse): 0.225091, G loss(w): 0.086287]\n",
      "Epoch 761/10001\n",
      "[D valid loss: 0.168510],[D fake loss: 0.000000], [G loss(mse): 0.227078, G loss(w): 0.087322]\n",
      "Epoch 762/10001\n",
      "[D valid loss: 0.168217],[D fake loss: 0.000000], [G loss(mse): 0.232864, G loss(w): 0.090221]\n",
      "Epoch 763/10001\n",
      "[D valid loss: 0.167876],[D fake loss: 0.000000], [G loss(mse): 0.222749, G loss(w): 0.083077]\n",
      "Epoch 764/10001\n",
      "[D valid loss: 0.167327],[D fake loss: 0.000000], [G loss(mse): 0.226452, G loss(w): 0.090845]\n",
      "Epoch 765/10001\n",
      "[D valid loss: 0.167092],[D fake loss: 0.000000], [G loss(mse): 0.230565, G loss(w): 0.089336]\n",
      "Epoch 766/10001\n",
      "[D valid loss: 0.166783],[D fake loss: 0.000000], [G loss(mse): 0.227836, G loss(w): 0.089442]\n",
      "Epoch 767/10001\n",
      "[D valid loss: 0.166602],[D fake loss: 0.000000], [G loss(mse): 0.231929, G loss(w): 0.091815]\n",
      "Epoch 768/10001\n",
      "[D valid loss: 0.166493],[D fake loss: 0.000000], [G loss(mse): 0.224867, G loss(w): 0.086566]\n",
      "Epoch 769/10001\n",
      "[D valid loss: 0.165859],[D fake loss: 0.000000], [G loss(mse): 0.225489, G loss(w): 0.086324]\n",
      "Epoch 770/10001\n",
      "[D valid loss: 0.165421],[D fake loss: 0.000000], [G loss(mse): 0.230943, G loss(w): 0.090555]\n",
      "Epoch 771/10001\n",
      "[D valid loss: 0.165272],[D fake loss: 0.000000], [G loss(mse): 0.223170, G loss(w): 0.085581]\n",
      "Epoch 772/10001\n",
      "[D valid loss: 0.164755],[D fake loss: 0.000000], [G loss(mse): 0.221884, G loss(w): 0.087358]\n",
      "Epoch 773/10001\n",
      "[D valid loss: 0.164319],[D fake loss: 0.000000], [G loss(mse): 0.229410, G loss(w): 0.094277]\n",
      "Epoch 774/10001\n",
      "[D valid loss: 0.164034],[D fake loss: 0.000000], [G loss(mse): 0.231773, G loss(w): 0.093064]\n",
      "Epoch 775/10001\n",
      "[D valid loss: 0.163736],[D fake loss: 0.000000], [G loss(mse): 0.229851, G loss(w): 0.091520]\n",
      "Epoch 776/10001\n",
      "[D valid loss: 0.163279],[D fake loss: 0.000000], [G loss(mse): 0.221834, G loss(w): 0.087533]\n",
      "Epoch 777/10001\n",
      "[D valid loss: 0.162949],[D fake loss: 0.000000], [G loss(mse): 0.225887, G loss(w): 0.093368]\n",
      "Epoch 778/10001\n",
      "[D valid loss: 0.162562],[D fake loss: 0.000000], [G loss(mse): 0.226001, G loss(w): 0.089411]\n",
      "Epoch 779/10001\n",
      "[D valid loss: 0.162340],[D fake loss: 0.000000], [G loss(mse): 0.223699, G loss(w): 0.089447]\n",
      "Epoch 780/10001\n",
      "[D valid loss: 0.161877],[D fake loss: 0.000000], [G loss(mse): 0.222303, G loss(w): 0.087385]\n",
      "Epoch 781/10001\n",
      "[D valid loss: 0.161480],[D fake loss: 0.000000], [G loss(mse): 0.226324, G loss(w): 0.094332]\n",
      "Epoch 782/10001\n",
      "[D valid loss: 0.161245],[D fake loss: 0.000000], [G loss(mse): 0.220495, G loss(w): 0.088335]\n",
      "Epoch 783/10001\n",
      "[D valid loss: 0.160850],[D fake loss: 0.000000], [G loss(mse): 0.219262, G loss(w): 0.086763]\n",
      "Epoch 784/10001\n",
      "[D valid loss: 0.160521],[D fake loss: 0.000000], [G loss(mse): 0.225555, G loss(w): 0.092483]\n",
      "Epoch 785/10001\n",
      "[D valid loss: 0.160235],[D fake loss: 0.000000], [G loss(mse): 0.214307, G loss(w): 0.084586]\n",
      "Epoch 786/10001\n",
      "[D valid loss: 0.160300],[D fake loss: 0.000000], [G loss(mse): 0.218775, G loss(w): 0.088060]\n",
      "Epoch 787/10001\n",
      "[D valid loss: 0.159861],[D fake loss: 0.000000], [G loss(mse): 0.217104, G loss(w): 0.090928]\n",
      "Epoch 788/10001\n",
      "[D valid loss: 0.159923],[D fake loss: 0.000000], [G loss(mse): 0.212215, G loss(w): 0.087266]\n",
      "Epoch 789/10001\n",
      "[D valid loss: 0.159849],[D fake loss: 0.000000], [G loss(mse): 0.205983, G loss(w): 0.084304]\n",
      "Epoch 790/10001\n",
      "[D valid loss: 0.159153],[D fake loss: 0.000000], [G loss(mse): 0.210005, G loss(w): 0.089570]\n",
      "Epoch 791/10001\n",
      "[D valid loss: 0.158623],[D fake loss: 0.000000], [G loss(mse): 0.212926, G loss(w): 0.094481]\n",
      "Epoch 792/10001\n",
      "[D valid loss: 0.159153],[D fake loss: 0.000000], [G loss(mse): 0.201207, G loss(w): 0.085363]\n",
      "Epoch 793/10001\n",
      "[D valid loss: 0.159598],[D fake loss: 0.000000], [G loss(mse): 0.199013, G loss(w): 0.085079]\n",
      "Epoch 794/10001\n",
      "[D valid loss: 0.159207],[D fake loss: 0.000000], [G loss(mse): 0.202538, G loss(w): 0.089054]\n",
      "Epoch 795/10001\n",
      "[D valid loss: 0.158386],[D fake loss: 0.000000], [G loss(mse): 0.198065, G loss(w): 0.084705]\n",
      "Epoch 796/10001\n",
      "[D valid loss: 0.156724],[D fake loss: 0.000000], [G loss(mse): 0.199524, G loss(w): 0.085827]\n",
      "Epoch 797/10001\n",
      "[D valid loss: 0.156279],[D fake loss: 0.000000], [G loss(mse): 0.200085, G loss(w): 0.086624]\n",
      "Epoch 798/10001\n",
      "[D valid loss: 0.156104],[D fake loss: 0.000000], [G loss(mse): 0.203675, G loss(w): 0.089084]\n",
      "Epoch 799/10001\n",
      "[D valid loss: 0.155453],[D fake loss: 0.000000], [G loss(mse): 0.204860, G loss(w): 0.088996]\n",
      "Epoch 800/10001\n",
      "[D valid loss: 0.155017],[D fake loss: 0.000000], [G loss(mse): 0.198949, G loss(w): 0.082370]\n",
      "Epoch 801/10001\n",
      "[D valid loss: 0.154953],[D fake loss: 0.000000], [G loss(mse): 0.205827, G loss(w): 0.084685]\n",
      "Epoch 802/10001\n",
      "[D valid loss: 0.154289],[D fake loss: 0.000000], [G loss(mse): 0.203573, G loss(w): 0.084526]\n",
      "Epoch 803/10001\n",
      "[D valid loss: 0.154283],[D fake loss: 0.000000], [G loss(mse): 0.207324, G loss(w): 0.088859]\n",
      "Epoch 804/10001\n",
      "[D valid loss: 0.153664],[D fake loss: 0.000000], [G loss(mse): 0.209211, G loss(w): 0.090106]\n",
      "Epoch 805/10001\n",
      "[D valid loss: 0.153436],[D fake loss: 0.000000], [G loss(mse): 0.201977, G loss(w): 0.082915]\n",
      "Epoch 806/10001\n",
      "[D valid loss: 0.153542],[D fake loss: 0.000000], [G loss(mse): 0.201919, G loss(w): 0.086214]\n",
      "Epoch 807/10001\n",
      "[D valid loss: 0.153207],[D fake loss: 0.000000], [G loss(mse): 0.202235, G loss(w): 0.086213]\n",
      "Epoch 808/10001\n",
      "[D valid loss: 0.152751],[D fake loss: 0.000000], [G loss(mse): 0.197280, G loss(w): 0.081863]\n",
      "Epoch 809/10001\n",
      "[D valid loss: 0.151858],[D fake loss: 0.000000], [G loss(mse): 0.200829, G loss(w): 0.083394]\n",
      "Epoch 810/10001\n",
      "[D valid loss: 0.152168],[D fake loss: 0.000000], [G loss(mse): 0.204666, G loss(w): 0.090234]\n",
      "Epoch 811/10001\n",
      "[D valid loss: 0.151765],[D fake loss: 0.000000], [G loss(mse): 0.200066, G loss(w): 0.087391]\n",
      "Epoch 812/10001\n",
      "[D valid loss: 0.151822],[D fake loss: 0.000000], [G loss(mse): 0.199031, G loss(w): 0.087896]\n",
      "Epoch 813/10001\n",
      "[D valid loss: 0.151073],[D fake loss: 0.000000], [G loss(mse): 0.200488, G loss(w): 0.087223]\n",
      "Epoch 814/10001\n",
      "[D valid loss: 0.150700],[D fake loss: 0.000000], [G loss(mse): 0.194239, G loss(w): 0.083515]\n",
      "Epoch 815/10001\n",
      "[D valid loss: 0.150015],[D fake loss: 0.000000], [G loss(mse): 0.195854, G loss(w): 0.085297]\n",
      "Epoch 816/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.150576],[D fake loss: 0.000000], [G loss(mse): 0.194491, G loss(w): 0.084050]\n",
      "Epoch 817/10001\n",
      "[D valid loss: 0.150110],[D fake loss: 0.000000], [G loss(mse): 0.197822, G loss(w): 0.089492]\n",
      "Epoch 818/10001\n",
      "[D valid loss: 0.149375],[D fake loss: 0.000000], [G loss(mse): 0.193647, G loss(w): 0.086061]\n",
      "Epoch 819/10001\n",
      "[D valid loss: 0.149654],[D fake loss: 0.000000], [G loss(mse): 0.199675, G loss(w): 0.091233]\n",
      "Epoch 820/10001\n",
      "[D valid loss: 0.149210],[D fake loss: 0.000000], [G loss(mse): 0.192228, G loss(w): 0.084292]\n",
      "Epoch 821/10001\n",
      "[D valid loss: 0.148135],[D fake loss: 0.000000], [G loss(mse): 0.197469, G loss(w): 0.087029]\n",
      "Epoch 822/10001\n",
      "[D valid loss: 0.148204],[D fake loss: 0.000000], [G loss(mse): 0.192521, G loss(w): 0.084932]\n",
      "Epoch 823/10001\n",
      "[D valid loss: 0.147726],[D fake loss: 0.000000], [G loss(mse): 0.191544, G loss(w): 0.084926]\n",
      "Epoch 824/10001\n",
      "[D valid loss: 0.147178],[D fake loss: 0.000000], [G loss(mse): 0.190046, G loss(w): 0.084779]\n",
      "Epoch 825/10001\n",
      "[D valid loss: 0.146811],[D fake loss: 0.000000], [G loss(mse): 0.196015, G loss(w): 0.089390]\n",
      "Epoch 826/10001\n",
      "[D valid loss: 0.146455],[D fake loss: 0.000000], [G loss(mse): 0.198192, G loss(w): 0.088563]\n",
      "Epoch 827/10001\n",
      "[D valid loss: 0.145924],[D fake loss: 0.000000], [G loss(mse): 0.201971, G loss(w): 0.092689]\n",
      "Epoch 828/10001\n",
      "[D valid loss: 0.145917],[D fake loss: 0.000000], [G loss(mse): 0.200166, G loss(w): 0.088781]\n",
      "Epoch 829/10001\n",
      "[D valid loss: 0.145401],[D fake loss: 0.000000], [G loss(mse): 0.196199, G loss(w): 0.083814]\n",
      "Epoch 830/10001\n",
      "[D valid loss: 0.144927],[D fake loss: 0.000000], [G loss(mse): 0.199624, G loss(w): 0.084192]\n",
      "Epoch 831/10001\n",
      "[D valid loss: 0.144977],[D fake loss: 0.000000], [G loss(mse): 0.202750, G loss(w): 0.089990]\n",
      "Epoch 832/10001\n",
      "[D valid loss: 0.144399],[D fake loss: 0.000000], [G loss(mse): 0.199649, G loss(w): 0.087365]\n",
      "Epoch 833/10001\n",
      "[D valid loss: 0.144119],[D fake loss: 0.000000], [G loss(mse): 0.200029, G loss(w): 0.086405]\n",
      "Epoch 834/10001\n",
      "[D valid loss: 0.143906],[D fake loss: 0.000000], [G loss(mse): 0.204349, G loss(w): 0.085349]\n",
      "Epoch 835/10001\n",
      "[D valid loss: 0.143948],[D fake loss: 0.000000], [G loss(mse): 0.204576, G loss(w): 0.088796]\n",
      "Epoch 836/10001\n",
      "[D valid loss: 0.143652],[D fake loss: 0.000000], [G loss(mse): 0.197293, G loss(w): 0.081383]\n",
      "Epoch 837/10001\n",
      "[D valid loss: 0.142877],[D fake loss: 0.000000], [G loss(mse): 0.202346, G loss(w): 0.085356]\n",
      "Epoch 838/10001\n",
      "[D valid loss: 0.142684],[D fake loss: 0.000000], [G loss(mse): 0.198832, G loss(w): 0.081063]\n",
      "Epoch 839/10001\n",
      "[D valid loss: 0.142383],[D fake loss: 0.000000], [G loss(mse): 0.199698, G loss(w): 0.086297]\n",
      "Epoch 840/10001\n",
      "[D valid loss: 0.141787],[D fake loss: 0.000000], [G loss(mse): 0.205974, G loss(w): 0.091686]\n",
      "Epoch 841/10001\n",
      "[D valid loss: 0.141604],[D fake loss: 0.000000], [G loss(mse): 0.194302, G loss(w): 0.081108]\n",
      "Epoch 842/10001\n",
      "[D valid loss: 0.141644],[D fake loss: 0.000000], [G loss(mse): 0.199278, G loss(w): 0.086732]\n",
      "Epoch 843/10001\n",
      "[D valid loss: 0.141143],[D fake loss: 0.000000], [G loss(mse): 0.198739, G loss(w): 0.084099]\n",
      "Epoch 844/10001\n",
      "[D valid loss: 0.140730],[D fake loss: 0.000000], [G loss(mse): 0.195574, G loss(w): 0.082899]\n",
      "Epoch 845/10001\n",
      "[D valid loss: 0.140339],[D fake loss: 0.000000], [G loss(mse): 0.198279, G loss(w): 0.084735]\n",
      "Epoch 846/10001\n",
      "[D valid loss: 0.140040],[D fake loss: 0.000000], [G loss(mse): 0.195949, G loss(w): 0.085034]\n",
      "Epoch 847/10001\n",
      "[D valid loss: 0.140276],[D fake loss: 0.000000], [G loss(mse): 0.191688, G loss(w): 0.081086]\n",
      "Epoch 848/10001\n",
      "[D valid loss: 0.139733],[D fake loss: 0.000000], [G loss(mse): 0.197924, G loss(w): 0.087131]\n",
      "Epoch 849/10001\n",
      "[D valid loss: 0.139267],[D fake loss: 0.000000], [G loss(mse): 0.194733, G loss(w): 0.085845]\n",
      "Epoch 850/10001\n",
      "[D valid loss: 0.139058],[D fake loss: 0.000000], [G loss(mse): 0.195144, G loss(w): 0.083271]\n",
      "Epoch 851/10001\n",
      "[D valid loss: 0.138650],[D fake loss: 0.000000], [G loss(mse): 0.195934, G loss(w): 0.085445]\n",
      "Epoch 852/10001\n",
      "[D valid loss: 0.138078],[D fake loss: 0.000000], [G loss(mse): 0.190959, G loss(w): 0.079368]\n",
      "Epoch 853/10001\n",
      "[D valid loss: 0.137921],[D fake loss: 0.000000], [G loss(mse): 0.194778, G loss(w): 0.083243]\n",
      "Epoch 854/10001\n",
      "[D valid loss: 0.137575],[D fake loss: 0.000000], [G loss(mse): 0.201707, G loss(w): 0.086620]\n",
      "Epoch 855/10001\n",
      "[D valid loss: 0.137813],[D fake loss: 0.000000], [G loss(mse): 0.190547, G loss(w): 0.079989]\n",
      "Epoch 856/10001\n",
      "[D valid loss: 0.136777],[D fake loss: 0.000000], [G loss(mse): 0.194259, G loss(w): 0.080824]\n",
      "Epoch 857/10001\n",
      "[D valid loss: 0.136564],[D fake loss: 0.000000], [G loss(mse): 0.193460, G loss(w): 0.082058]\n",
      "Epoch 858/10001\n",
      "[D valid loss: 0.136195],[D fake loss: 0.000000], [G loss(mse): 0.199108, G loss(w): 0.086326]\n",
      "Epoch 859/10001\n",
      "[D valid loss: 0.135956],[D fake loss: 0.000000], [G loss(mse): 0.193310, G loss(w): 0.081786]\n",
      "Epoch 860/10001\n",
      "[D valid loss: 0.135715],[D fake loss: 0.000000], [G loss(mse): 0.195430, G loss(w): 0.085303]\n",
      "Epoch 861/10001\n",
      "[D valid loss: 0.135532],[D fake loss: 0.000000], [G loss(mse): 0.194976, G loss(w): 0.084597]\n",
      "Epoch 862/10001\n",
      "[D valid loss: 0.135229],[D fake loss: 0.000000], [G loss(mse): 0.189527, G loss(w): 0.078575]\n",
      "Epoch 863/10001\n",
      "[D valid loss: 0.134923],[D fake loss: 0.000000], [G loss(mse): 0.195476, G loss(w): 0.084576]\n",
      "Epoch 864/10001\n",
      "[D valid loss: 0.134587],[D fake loss: 0.000000], [G loss(mse): 0.196132, G loss(w): 0.087320]\n",
      "Epoch 865/10001\n",
      "[D valid loss: 0.134172],[D fake loss: 0.000000], [G loss(mse): 0.189688, G loss(w): 0.081180]\n",
      "Epoch 866/10001\n",
      "[D valid loss: 0.134205],[D fake loss: 0.000000], [G loss(mse): 0.192631, G loss(w): 0.079969]\n",
      "Epoch 867/10001\n",
      "[D valid loss: 0.133865],[D fake loss: 0.000000], [G loss(mse): 0.187968, G loss(w): 0.078658]\n",
      "Epoch 868/10001\n",
      "[D valid loss: 0.133695],[D fake loss: 0.000000], [G loss(mse): 0.189927, G loss(w): 0.080918]\n",
      "Epoch 869/10001\n",
      "[D valid loss: 0.133450],[D fake loss: 0.000000], [G loss(mse): 0.192081, G loss(w): 0.081079]\n",
      "Epoch 870/10001\n",
      "[D valid loss: 0.132681],[D fake loss: 0.000000], [G loss(mse): 0.196454, G loss(w): 0.088542]\n",
      "Epoch 871/10001\n",
      "[D valid loss: 0.132463],[D fake loss: 0.000000], [G loss(mse): 0.194338, G loss(w): 0.085805]\n",
      "Epoch 872/10001\n",
      "[D valid loss: 0.132375],[D fake loss: 0.000000], [G loss(mse): 0.192013, G loss(w): 0.084870]\n",
      "Epoch 873/10001\n",
      "[D valid loss: 0.131940],[D fake loss: 0.000000], [G loss(mse): 0.190811, G loss(w): 0.081464]\n",
      "Epoch 874/10001\n",
      "[D valid loss: 0.131659],[D fake loss: 0.000000], [G loss(mse): 0.192985, G loss(w): 0.083305]\n",
      "Epoch 875/10001\n",
      "[D valid loss: 0.131560],[D fake loss: 0.000000], [G loss(mse): 0.192801, G loss(w): 0.084120]\n",
      "Epoch 876/10001\n",
      "[D valid loss: 0.131193],[D fake loss: 0.000000], [G loss(mse): 0.186728, G loss(w): 0.080968]\n",
      "Epoch 877/10001\n",
      "[D valid loss: 0.130784],[D fake loss: 0.000000], [G loss(mse): 0.190349, G loss(w): 0.082413]\n",
      "Epoch 878/10001\n",
      "[D valid loss: 0.130634],[D fake loss: 0.000000], [G loss(mse): 0.189915, G loss(w): 0.083083]\n",
      "Epoch 879/10001\n",
      "[D valid loss: 0.130290],[D fake loss: 0.000000], [G loss(mse): 0.186148, G loss(w): 0.080632]\n",
      "Epoch 880/10001\n",
      "[D valid loss: 0.130089],[D fake loss: 0.000000], [G loss(mse): 0.194897, G loss(w): 0.087901]\n",
      "Epoch 881/10001\n",
      "[D valid loss: 0.129557],[D fake loss: 0.000000], [G loss(mse): 0.190067, G loss(w): 0.083685]\n",
      "Epoch 882/10001\n",
      "[D valid loss: 0.129368],[D fake loss: 0.000000], [G loss(mse): 0.186734, G loss(w): 0.078981]\n",
      "Epoch 883/10001\n",
      "[D valid loss: 0.129165],[D fake loss: 0.000000], [G loss(mse): 0.187383, G loss(w): 0.082111]\n",
      "Epoch 884/10001\n",
      "[D valid loss: 0.128713],[D fake loss: 0.000000], [G loss(mse): 0.192726, G loss(w): 0.086994]\n",
      "Epoch 885/10001\n",
      "[D valid loss: 0.128526],[D fake loss: 0.000000], [G loss(mse): 0.183331, G loss(w): 0.078869]\n",
      "Epoch 886/10001\n",
      "[D valid loss: 0.127902],[D fake loss: 0.000000], [G loss(mse): 0.190883, G loss(w): 0.086805]\n",
      "Epoch 887/10001\n",
      "[D valid loss: 0.127793],[D fake loss: 0.000000], [G loss(mse): 0.181415, G loss(w): 0.077427]\n",
      "Epoch 888/10001\n",
      "[D valid loss: 0.127604],[D fake loss: 0.000000], [G loss(mse): 0.188586, G loss(w): 0.083110]\n",
      "Epoch 889/10001\n",
      "[D valid loss: 0.127491],[D fake loss: 0.000000], [G loss(mse): 0.190774, G loss(w): 0.087125]\n",
      "Epoch 890/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.127143],[D fake loss: 0.000000], [G loss(mse): 0.180960, G loss(w): 0.078587]\n",
      "Epoch 891/10001\n",
      "[D valid loss: 0.126669],[D fake loss: 0.000000], [G loss(mse): 0.187020, G loss(w): 0.084416]\n",
      "Epoch 892/10001\n",
      "[D valid loss: 0.126276],[D fake loss: 0.000000], [G loss(mse): 0.180610, G loss(w): 0.078196]\n",
      "Epoch 893/10001\n",
      "[D valid loss: 0.126293],[D fake loss: 0.000000], [G loss(mse): 0.181838, G loss(w): 0.080589]\n",
      "Epoch 894/10001\n",
      "[D valid loss: 0.125949],[D fake loss: 0.000000], [G loss(mse): 0.191518, G loss(w): 0.089359]\n",
      "Epoch 895/10001\n",
      "[D valid loss: 0.125735],[D fake loss: 0.000000], [G loss(mse): 0.187431, G loss(w): 0.084761]\n",
      "Epoch 896/10001\n",
      "[D valid loss: 0.125473],[D fake loss: 0.000000], [G loss(mse): 0.179124, G loss(w): 0.079764]\n",
      "Epoch 897/10001\n",
      "[D valid loss: 0.125113],[D fake loss: 0.000000], [G loss(mse): 0.180736, G loss(w): 0.080769]\n",
      "Epoch 898/10001\n",
      "[D valid loss: 0.124703],[D fake loss: 0.000000], [G loss(mse): 0.182113, G loss(w): 0.083939]\n",
      "Epoch 899/10001\n",
      "[D valid loss: 0.124392],[D fake loss: 0.000000], [G loss(mse): 0.178647, G loss(w): 0.079948]\n",
      "Epoch 900/10001\n",
      "[D valid loss: 0.124129],[D fake loss: 0.000000], [G loss(mse): 0.185017, G loss(w): 0.086299]\n",
      "Epoch 901/10001\n",
      "[D valid loss: 0.123873],[D fake loss: 0.000000], [G loss(mse): 0.179923, G loss(w): 0.080754]\n",
      "Epoch 902/10001\n",
      "[D valid loss: 0.123453],[D fake loss: 0.000000], [G loss(mse): 0.180696, G loss(w): 0.083124]\n",
      "Epoch 903/10001\n",
      "[D valid loss: 0.123115],[D fake loss: 0.000000], [G loss(mse): 0.176857, G loss(w): 0.078821]\n",
      "Epoch 904/10001\n",
      "[D valid loss: 0.123030],[D fake loss: 0.000000], [G loss(mse): 0.174341, G loss(w): 0.079199]\n",
      "Epoch 905/10001\n",
      "[D valid loss: 0.122935],[D fake loss: 0.000000], [G loss(mse): 0.177367, G loss(w): 0.081540]\n",
      "Epoch 906/10001\n",
      "[D valid loss: 0.122698],[D fake loss: 0.000000], [G loss(mse): 0.176204, G loss(w): 0.081509]\n",
      "Epoch 907/10001\n",
      "[D valid loss: 0.122037],[D fake loss: 0.000000], [G loss(mse): 0.175613, G loss(w): 0.080394]\n",
      "Epoch 908/10001\n",
      "[D valid loss: 0.121749],[D fake loss: 0.000000], [G loss(mse): 0.177346, G loss(w): 0.081530]\n",
      "Epoch 909/10001\n",
      "[D valid loss: 0.121693],[D fake loss: 0.000000], [G loss(mse): 0.179927, G loss(w): 0.084398]\n",
      "Epoch 910/10001\n",
      "[D valid loss: 0.121366],[D fake loss: 0.000000], [G loss(mse): 0.177454, G loss(w): 0.081149]\n",
      "Epoch 911/10001\n",
      "[D valid loss: 0.121007],[D fake loss: 0.000000], [G loss(mse): 0.174115, G loss(w): 0.080412]\n",
      "Epoch 912/10001\n",
      "[D valid loss: 0.120884],[D fake loss: 0.000000], [G loss(mse): 0.172220, G loss(w): 0.079713]\n",
      "Epoch 913/10001\n",
      "[D valid loss: 0.120568],[D fake loss: 0.000000], [G loss(mse): 0.175816, G loss(w): 0.083664]\n",
      "Epoch 914/10001\n",
      "[D valid loss: 0.120311],[D fake loss: 0.000000], [G loss(mse): 0.174611, G loss(w): 0.082316]\n",
      "Epoch 915/10001\n",
      "[D valid loss: 0.120156],[D fake loss: 0.000000], [G loss(mse): 0.173025, G loss(w): 0.082162]\n",
      "Epoch 916/10001\n",
      "[D valid loss: 0.119954],[D fake loss: 0.000000], [G loss(mse): 0.173651, G loss(w): 0.081164]\n",
      "Epoch 917/10001\n",
      "[D valid loss: 0.119366],[D fake loss: 0.000000], [G loss(mse): 0.177199, G loss(w): 0.085935]\n",
      "Epoch 918/10001\n",
      "[D valid loss: 0.119168],[D fake loss: 0.000000], [G loss(mse): 0.168637, G loss(w): 0.077809]\n",
      "Epoch 919/10001\n",
      "[D valid loss: 0.119261],[D fake loss: 0.000000], [G loss(mse): 0.164212, G loss(w): 0.076426]\n",
      "Epoch 920/10001\n",
      "[D valid loss: 0.119370],[D fake loss: 0.000000], [G loss(mse): 0.164057, G loss(w): 0.078136]\n",
      "Epoch 921/10001\n",
      "[D valid loss: 0.118531],[D fake loss: 0.000000], [G loss(mse): 0.167076, G loss(w): 0.080013]\n",
      "Epoch 922/10001\n",
      "[D valid loss: 0.118303],[D fake loss: 0.000000], [G loss(mse): 0.164189, G loss(w): 0.078475]\n",
      "Epoch 923/10001\n",
      "[D valid loss: 0.117712],[D fake loss: 0.000000], [G loss(mse): 0.167070, G loss(w): 0.081987]\n",
      "Epoch 924/10001\n",
      "[D valid loss: 0.117237],[D fake loss: 0.000000], [G loss(mse): 0.168565, G loss(w): 0.084111]\n",
      "Epoch 925/10001\n",
      "[D valid loss: 0.117003],[D fake loss: 0.000000], [G loss(mse): 0.161975, G loss(w): 0.078544]\n",
      "Epoch 926/10001\n",
      "[D valid loss: 0.116778],[D fake loss: 0.000000], [G loss(mse): 0.167800, G loss(w): 0.083175]\n",
      "Epoch 927/10001\n",
      "[D valid loss: 0.116640],[D fake loss: 0.000000], [G loss(mse): 0.159521, G loss(w): 0.075340]\n",
      "Epoch 928/10001\n",
      "[D valid loss: 0.116406],[D fake loss: 0.000000], [G loss(mse): 0.165592, G loss(w): 0.079456]\n",
      "Epoch 929/10001\n",
      "[D valid loss: 0.115887],[D fake loss: 0.000000], [G loss(mse): 0.160074, G loss(w): 0.077302]\n",
      "Epoch 930/10001\n",
      "[D valid loss: 0.116364],[D fake loss: 0.000000], [G loss(mse): 0.161376, G loss(w): 0.077554]\n",
      "Epoch 931/10001\n",
      "[D valid loss: 0.115308],[D fake loss: 0.000000], [G loss(mse): 0.162561, G loss(w): 0.077198]\n",
      "Epoch 932/10001\n",
      "[D valid loss: 0.115327],[D fake loss: 0.000000], [G loss(mse): 0.167560, G loss(w): 0.079904]\n",
      "Epoch 933/10001\n",
      "[D valid loss: 0.114877],[D fake loss: 0.000000], [G loss(mse): 0.160918, G loss(w): 0.073704]\n",
      "Epoch 934/10001\n",
      "[D valid loss: 0.114916],[D fake loss: 0.000000], [G loss(mse): 0.167690, G loss(w): 0.079688]\n",
      "Epoch 935/10001\n",
      "[D valid loss: 0.114411],[D fake loss: 0.000000], [G loss(mse): 0.159552, G loss(w): 0.073178]\n",
      "Epoch 936/10001\n",
      "[D valid loss: 0.113996],[D fake loss: 0.000000], [G loss(mse): 0.172807, G loss(w): 0.084263]\n",
      "Epoch 937/10001\n",
      "[D valid loss: 0.114195],[D fake loss: 0.000000], [G loss(mse): 0.167873, G loss(w): 0.080328]\n",
      "Epoch 938/10001\n",
      "[D valid loss: 0.113919],[D fake loss: 0.000000], [G loss(mse): 0.163968, G loss(w): 0.077039]\n",
      "Epoch 939/10001\n",
      "[D valid loss: 0.113624],[D fake loss: 0.000000], [G loss(mse): 0.166592, G loss(w): 0.077851]\n",
      "Epoch 940/10001\n",
      "[D valid loss: 0.113406],[D fake loss: 0.000000], [G loss(mse): 0.169629, G loss(w): 0.081922]\n",
      "Epoch 941/10001\n",
      "[D valid loss: 0.113112],[D fake loss: 0.000000], [G loss(mse): 0.163623, G loss(w): 0.077135]\n",
      "Epoch 942/10001\n",
      "[D valid loss: 0.112999],[D fake loss: 0.000000], [G loss(mse): 0.169019, G loss(w): 0.080739]\n",
      "Epoch 943/10001\n",
      "[D valid loss: 0.112400],[D fake loss: 0.000000], [G loss(mse): 0.165542, G loss(w): 0.078219]\n",
      "Epoch 944/10001\n",
      "[D valid loss: 0.112196],[D fake loss: 0.000000], [G loss(mse): 0.170571, G loss(w): 0.083178]\n",
      "Epoch 945/10001\n",
      "[D valid loss: 0.111756],[D fake loss: 0.000000], [G loss(mse): 0.159513, G loss(w): 0.074109]\n",
      "Epoch 946/10001\n",
      "[D valid loss: 0.111503],[D fake loss: 0.000000], [G loss(mse): 0.161437, G loss(w): 0.075952]\n",
      "Epoch 947/10001\n",
      "[D valid loss: 0.111316],[D fake loss: 0.000000], [G loss(mse): 0.161398, G loss(w): 0.075990]\n",
      "Epoch 948/10001\n",
      "[D valid loss: 0.111257],[D fake loss: 0.000000], [G loss(mse): 0.160794, G loss(w): 0.075566]\n",
      "Epoch 949/10001\n",
      "[D valid loss: 0.110713],[D fake loss: 0.000000], [G loss(mse): 0.164862, G loss(w): 0.079305]\n",
      "Epoch 950/10001\n",
      "[D valid loss: 0.110811],[D fake loss: 0.000000], [G loss(mse): 0.161236, G loss(w): 0.077490]\n",
      "Epoch 951/10001\n",
      "[D valid loss: 0.110257],[D fake loss: 0.000000], [G loss(mse): 0.167352, G loss(w): 0.082134]\n",
      "Epoch 952/10001\n",
      "[D valid loss: 0.110021],[D fake loss: 0.000000], [G loss(mse): 0.158642, G loss(w): 0.075811]\n",
      "Epoch 953/10001\n",
      "[D valid loss: 0.109922],[D fake loss: 0.000000], [G loss(mse): 0.153892, G loss(w): 0.071227]\n",
      "Epoch 954/10001\n",
      "[D valid loss: 0.109865],[D fake loss: 0.000000], [G loss(mse): 0.156165, G loss(w): 0.074145]\n",
      "Epoch 955/10001\n",
      "[D valid loss: 0.109026],[D fake loss: 0.000000], [G loss(mse): 0.158485, G loss(w): 0.077768]\n",
      "Epoch 956/10001\n",
      "[D valid loss: 0.109196],[D fake loss: 0.000000], [G loss(mse): 0.161579, G loss(w): 0.080319]\n",
      "Epoch 957/10001\n",
      "[D valid loss: 0.108589],[D fake loss: 0.000000], [G loss(mse): 0.157794, G loss(w): 0.078427]\n",
      "Epoch 958/10001\n",
      "[D valid loss: 0.108854],[D fake loss: 0.000000], [G loss(mse): 0.151082, G loss(w): 0.072519]\n",
      "Epoch 959/10001\n",
      "[D valid loss: 0.108265],[D fake loss: 0.000000], [G loss(mse): 0.154929, G loss(w): 0.076984]\n",
      "Epoch 960/10001\n",
      "[D valid loss: 0.108120],[D fake loss: 0.000000], [G loss(mse): 0.154094, G loss(w): 0.078200]\n",
      "Epoch 961/10001\n",
      "[D valid loss: 0.108083],[D fake loss: 0.000000], [G loss(mse): 0.152854, G loss(w): 0.078036]\n",
      "Epoch 962/10001\n",
      "[D valid loss: 0.107648],[D fake loss: 0.000000], [G loss(mse): 0.150810, G loss(w): 0.076832]\n",
      "Epoch 963/10001\n",
      "[D valid loss: 0.107279],[D fake loss: 0.000000], [G loss(mse): 0.147376, G loss(w): 0.074368]\n",
      "Epoch 964/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.107079],[D fake loss: 0.000000], [G loss(mse): 0.151074, G loss(w): 0.077387]\n",
      "Epoch 965/10001\n",
      "[D valid loss: 0.106751],[D fake loss: 0.000000], [G loss(mse): 0.148143, G loss(w): 0.072858]\n",
      "Epoch 966/10001\n",
      "[D valid loss: 0.106390],[D fake loss: 0.000000], [G loss(mse): 0.153353, G loss(w): 0.079643]\n",
      "Epoch 967/10001\n",
      "[D valid loss: 0.106275],[D fake loss: 0.000000], [G loss(mse): 0.152206, G loss(w): 0.077554]\n",
      "Epoch 968/10001\n",
      "[D valid loss: 0.105768],[D fake loss: 0.000000], [G loss(mse): 0.153552, G loss(w): 0.082014]\n",
      "Epoch 969/10001\n",
      "[D valid loss: 0.105778],[D fake loss: 0.000000], [G loss(mse): 0.153723, G loss(w): 0.082394]\n",
      "Epoch 970/10001\n",
      "[D valid loss: 0.105421],[D fake loss: 0.000000], [G loss(mse): 0.149320, G loss(w): 0.077303]\n",
      "Epoch 971/10001\n",
      "[D valid loss: 0.105271],[D fake loss: 0.000000], [G loss(mse): 0.151783, G loss(w): 0.079280]\n",
      "Epoch 972/10001\n",
      "[D valid loss: 0.104951],[D fake loss: 0.000000], [G loss(mse): 0.145997, G loss(w): 0.072705]\n",
      "Epoch 973/10001\n",
      "[D valid loss: 0.104791],[D fake loss: 0.000000], [G loss(mse): 0.150221, G loss(w): 0.078082]\n",
      "Epoch 974/10001\n",
      "[D valid loss: 0.104446],[D fake loss: 0.000000], [G loss(mse): 0.148861, G loss(w): 0.078726]\n",
      "Epoch 975/10001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder(aae.encoder(X_train_scaled))\n",
    "predicted_values3 = aae.encoder(X_train_scaled)\n",
    "predicted_values4 = scaler.inverse_transform(X_train_scaled)\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    #predicted_values3 = scaler.inverse_transform(predicted_values3)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    #plt.scatter(X_train, y_train,c='orange') #sample\n",
    "    #plt.scatter(predicted_values[:,0],predicted_values[:,1],c='red') #decoder(latent space)\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1],)#encoder/decoder\n",
    "    plt.scatter(predicted_values3[:,0],predicted_values3[:,1],c='pink')#encoder(X_train_scaled)\n",
    "    #plt.scatter(predicted_values4[:,0],predicted_values4[:,1],c='grey')#X_trained_scaled\n",
    "    plt.scatter(latent_values[:,0],latent_values[:,1],c='grey')\n",
    "    plt.ylabel('Y')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "n_points = 900\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//9, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//9, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//9, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//9, n_features), x_input[3])\n",
    "predict5 = np.full((n_points//9, n_features), x_input[4])\n",
    "predict6 = np.full((n_points//9, n_features), x_input[5])\n",
    "predict7 = np.full((n_points//9, n_features), x_input[6])\n",
    "predict8 = np.full((n_points//9, n_features), x_input[7])\n",
    "predict9 = np.full((n_points//9, n_features), x_input[8])\n",
    "\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4, predict5, predict6, predict7, predict8, predict9))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder(aae.encoder.predict(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
