{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu (ELU)                       (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         elu[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "elu_1 (ELU)                     (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 952\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Decoder:\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,242\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Discriminator:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "elu_4 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                1680      \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "elu_8 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 5,801\n",
      "Trainable params: 5,401\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network20.build_encoder(Z, nodes, n_features)\n",
    "print(\"Encoder:\\n\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network20.build_decoder(Z,nodes, n_features)\n",
    "print(\"Decoder:\\n\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator=network20.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model20\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 100001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model20.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100001\n",
      "[C1 valid: -0.509513, C2 fake: 0.510633], [G loss: 0.318195, mse: 0.646096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05012/AAE_Model20.py:190: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_40_100001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_40_100001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_40_100001/assets\n",
      "Epoch 2/100001\n",
      "[C1 valid: -0.508556, C2 fake: 0.512354], [G loss: 0.267071, mse: 0.543833]\n",
      "Epoch 3/100001\n",
      "[C1 valid: -0.510442, C2 fake: 0.511349], [G loss: 0.220946, mse: 0.451549]\n",
      "Epoch 4/100001\n",
      "[C1 valid: -0.508954, C2 fake: 0.512403], [G loss: 0.198861, mse: 0.407344]\n",
      "Epoch 5/100001\n",
      "[C1 valid: -0.508667, C2 fake: 0.510521], [G loss: 0.170890, mse: 0.351515]\n",
      "Epoch 6/100001\n",
      "[C1 valid: -0.509554, C2 fake: 0.511724], [G loss: 0.146706, mse: 0.303151]\n",
      "Epoch 7/100001\n",
      "[C1 valid: -0.509317, C2 fake: 0.513168], [G loss: 0.132177, mse: 0.274121]\n",
      "Epoch 8/100001\n",
      "[C1 valid: -0.510042, C2 fake: 0.511715], [G loss: 0.118244, mse: 0.246098]\n",
      "Epoch 9/100001\n",
      "[C1 valid: -0.509297, C2 fake: 0.510922], [G loss: 0.106264, mse: 0.222141]\n",
      "Epoch 10/100001\n",
      "[C1 valid: -0.508754, C2 fake: 0.513219], [G loss: 0.103875, mse: 0.217344]\n",
      "Epoch 11/100001\n",
      "[C1 valid: -0.508141, C2 fake: 0.508458], [G loss: 0.089242, mse: 0.188154]\n",
      "Epoch 12/100001\n",
      "[C1 valid: -0.507913, C2 fake: 0.508337], [G loss: 0.085378, mse: 0.180387]\n",
      "Epoch 13/100001\n",
      "[C1 valid: -0.507058, C2 fake: 0.508538], [G loss: 0.080586, mse: 0.170769]\n",
      "Epoch 14/100001\n",
      "[C1 valid: -0.508001, C2 fake: 0.509309], [G loss: 0.077465, mse: 0.164818]\n",
      "Epoch 15/100001\n",
      "[C1 valid: -0.506938, C2 fake: 0.509109], [G loss: 0.071936, mse: 0.153622]\n",
      "Epoch 16/100001\n",
      "[C1 valid: -0.508637, C2 fake: 0.508810], [G loss: 0.064396, mse: 0.138634]\n",
      "Epoch 17/100001\n",
      "[C1 valid: -0.506483, C2 fake: 0.507670], [G loss: 0.062176, mse: 0.134236]\n",
      "Epoch 18/100001\n",
      "[C1 valid: -0.508168, C2 fake: 0.507079], [G loss: 0.060932, mse: 0.131600]\n",
      "Epoch 19/100001\n",
      "[C1 valid: -0.506074, C2 fake: 0.508428], [G loss: 0.057226, mse: 0.124200]\n",
      "Epoch 20/100001\n",
      "[C1 valid: -0.505493, C2 fake: 0.506987], [G loss: 0.052285, mse: 0.114413]\n",
      "Epoch 21/100001\n",
      "[C1 valid: -0.507374, C2 fake: 0.506639], [G loss: 0.049395, mse: 0.108587]\n",
      "Epoch 22/100001\n",
      "[C1 valid: -0.507422, C2 fake: 0.506955], [G loss: 0.048041, mse: 0.105582]\n",
      "Epoch 23/100001\n",
      "[C1 valid: -0.510211, C2 fake: 0.507461], [G loss: 0.042623, mse: 0.094843]\n",
      "Epoch 24/100001\n",
      "[C1 valid: -0.506390, C2 fake: 0.509870], [G loss: 0.042532, mse: 0.094587]\n",
      "Epoch 25/100001\n",
      "[C1 valid: -0.511019, C2 fake: 0.508076], [G loss: 0.040722, mse: 0.090714]\n",
      "Epoch 26/100001\n",
      "[C1 valid: -0.506470, C2 fake: 0.505372], [G loss: 0.037331, mse: 0.084059]\n",
      "Epoch 27/100001\n",
      "[C1 valid: -0.507240, C2 fake: 0.502813], [G loss: 0.040869, mse: 0.090968]\n",
      "Epoch 28/100001\n",
      "[C1 valid: -0.503424, C2 fake: 0.501183], [G loss: 0.034652, mse: 0.078528]\n",
      "Epoch 29/100001\n",
      "[C1 valid: -0.504475, C2 fake: 0.501472], [G loss: 0.036068, mse: 0.081125]\n",
      "Epoch 30/100001\n",
      "[C1 valid: -0.503157, C2 fake: 0.495878], [G loss: 0.030466, mse: 0.069777]\n",
      "Epoch 31/100001\n",
      "[C1 valid: -0.499723, C2 fake: 0.491305], [G loss: 0.029363, mse: 0.067521]\n",
      "Epoch 32/100001\n",
      "[C1 valid: -0.501386, C2 fake: 0.490792], [G loss: 0.029546, mse: 0.067603]\n",
      "Epoch 33/100001\n",
      "[C1 valid: -0.498848, C2 fake: 0.489174], [G loss: 0.028597, mse: 0.065334]\n",
      "Epoch 34/100001\n",
      "[C1 valid: -0.500127, C2 fake: 0.484462], [G loss: 0.026996, mse: 0.062424]\n",
      "Epoch 35/100001\n",
      "[C1 valid: -0.495051, C2 fake: 0.480870], [G loss: 0.027825, mse: 0.063993]\n",
      "Epoch 36/100001\n",
      "[C1 valid: -0.493817, C2 fake: 0.476695], [G loss: 0.026492, mse: 0.061340]\n",
      "Epoch 37/100001\n",
      "[C1 valid: -0.490193, C2 fake: 0.469689], [G loss: 0.025913, mse: 0.060062]\n",
      "Epoch 38/100001\n",
      "[C1 valid: -0.490455, C2 fake: 0.471327], [G loss: 0.023305, mse: 0.054725]\n",
      "Epoch 39/100001\n",
      "[C1 valid: -0.488185, C2 fake: 0.464538], [G loss: 0.022941, mse: 0.053974]\n",
      "Epoch 40/100001\n",
      "[C1 valid: -0.488213, C2 fake: 0.464768], [G loss: 0.022719, mse: 0.053607]\n",
      "Epoch 41/100001\n",
      "[C1 valid: -0.486213, C2 fake: 0.457042], [G loss: 0.022598, mse: 0.053215]\n",
      "Epoch 42/100001\n",
      "[C1 valid: -0.482788, C2 fake: 0.453732], [G loss: 0.021066, mse: 0.049997]\n",
      "Epoch 43/100001\n",
      "[C1 valid: -0.485329, C2 fake: 0.451998], [G loss: 0.019993, mse: 0.047893]\n",
      "Epoch 44/100001\n",
      "[C1 valid: -0.485326, C2 fake: 0.443819], [G loss: 0.020184, mse: 0.048299]\n",
      "Epoch 45/100001\n",
      "[C1 valid: -0.477376, C2 fake: 0.440795], [G loss: 0.021024, mse: 0.049753]\n",
      "Epoch 46/100001\n",
      "[C1 valid: -0.487092, C2 fake: 0.442184], [G loss: 0.017932, mse: 0.043822]\n",
      "Epoch 47/100001\n",
      "[C1 valid: -0.478929, C2 fake: 0.439052], [G loss: 0.017879, mse: 0.043780]\n",
      "Epoch 48/100001\n",
      "[C1 valid: -0.482482, C2 fake: 0.440436], [G loss: 0.018096, mse: 0.044273]\n",
      "Epoch 49/100001\n",
      "[C1 valid: -0.483504, C2 fake: 0.433885], [G loss: 0.017659, mse: 0.042791]\n",
      "Epoch 50/100001\n",
      "[C1 valid: -0.484432, C2 fake: 0.424953], [G loss: 0.016387, mse: 0.040577]\n",
      "Epoch 51/100001\n",
      "[C1 valid: -0.476409, C2 fake: 0.435722], [G loss: 0.015892, mse: 0.039497]\n",
      "Epoch 52/100001\n",
      "[C1 valid: -0.482338, C2 fake: 0.424665], [G loss: 0.015226, mse: 0.037948]\n",
      "Epoch 53/100001\n",
      "[C1 valid: -0.483122, C2 fake: 0.414699], [G loss: 0.014553, mse: 0.036245]\n",
      "Epoch 54/100001\n",
      "[C1 valid: -0.485259, C2 fake: 0.413931], [G loss: 0.014504, mse: 0.036297]\n",
      "Epoch 55/100001\n",
      "[C1 valid: -0.485213, C2 fake: 0.416005], [G loss: 0.014243, mse: 0.035738]\n",
      "Epoch 56/100001\n",
      "[C1 valid: -0.488110, C2 fake: 0.411940], [G loss: 0.014752, mse: 0.037363]\n",
      "Epoch 57/100001\n",
      "[C1 valid: -0.478469, C2 fake: 0.415663], [G loss: 0.014054, mse: 0.036019]\n",
      "Epoch 58/100001\n",
      "[C1 valid: -0.485299, C2 fake: 0.414476], [G loss: 0.013060, mse: 0.034152]\n",
      "Epoch 59/100001\n",
      "[C1 valid: -0.495116, C2 fake: 0.410955], [G loss: 0.012580, mse: 0.033503]\n",
      "Epoch 60/100001\n",
      "[C1 valid: -0.493268, C2 fake: 0.411018], [G loss: 0.012047, mse: 0.032179]\n",
      "Epoch 61/100001\n",
      "[C1 valid: -0.500208, C2 fake: 0.411240], [G loss: 0.012195, mse: 0.032971]\n",
      "Epoch 62/100001\n",
      "[C1 valid: -0.499309, C2 fake: 0.403204], [G loss: 0.010553, mse: 0.030810]\n",
      "Epoch 63/100001\n",
      "[C1 valid: -0.511762, C2 fake: 0.414203], [G loss: 0.010528, mse: 0.031519]\n",
      "Epoch 64/100001\n",
      "[C1 valid: -0.523795, C2 fake: 0.401995], [G loss: 0.009925, mse: 0.031037]\n",
      "Epoch 65/100001\n",
      "[C1 valid: -0.527973, C2 fake: 0.398045], [G loss: 0.009273, mse: 0.031224]\n",
      "Epoch 66/100001\n",
      "[C1 valid: -0.541193, C2 fake: 0.400202], [G loss: 0.008684, mse: 0.031665]\n",
      "Epoch 67/100001\n",
      "[C1 valid: -0.556942, C2 fake: 0.384463], [G loss: 0.007276, mse: 0.029372]\n",
      "Epoch 68/100001\n",
      "[C1 valid: -0.579997, C2 fake: 0.378789], [G loss: 0.006582, mse: 0.028595]\n",
      "Epoch 69/100001\n",
      "[C1 valid: -0.595554, C2 fake: 0.363803], [G loss: 0.006722, mse: 0.029093]\n",
      "Epoch 70/100001\n",
      "[C1 valid: -0.611734, C2 fake: 0.358870], [G loss: 0.006616, mse: 0.028744]\n",
      "Epoch 71/100001\n",
      "[C1 valid: -0.626242, C2 fake: 0.346115], [G loss: 0.006633, mse: 0.028768]\n",
      "Epoch 72/100001\n",
      "[C1 valid: -0.641309, C2 fake: 0.337320], [G loss: 0.006864, mse: 0.029353]\n",
      "Epoch 73/100001\n",
      "[C1 valid: -0.637899, C2 fake: 0.336789], [G loss: 0.006040, mse: 0.027690]\n",
      "Epoch 74/100001\n",
      "[C1 valid: -0.663622, C2 fake: 0.331015], [G loss: 0.005018, mse: 0.025805]\n",
      "Epoch 75/100001\n",
      "[C1 valid: -0.661998, C2 fake: 0.329546], [G loss: 0.005220, mse: 0.026251]\n",
      "Epoch 76/100001\n",
      "[C1 valid: -0.676004, C2 fake: 0.313713], [G loss: 0.004591, mse: 0.025065]\n",
      "Epoch 77/100001\n",
      "[C1 valid: -0.682854, C2 fake: 0.308450], [G loss: 0.004871, mse: 0.025628]\n",
      "Epoch 78/100001\n",
      "[C1 valid: -0.688680, C2 fake: 0.306261], [G loss: 0.004743, mse: 0.025431]\n",
      "Epoch 79/100001\n",
      "[C1 valid: -0.698023, C2 fake: 0.308040], [G loss: 0.005107, mse: 0.025971]\n",
      "Epoch 80/100001\n",
      "[C1 valid: -0.700532, C2 fake: 0.302782], [G loss: 0.004369, mse: 0.024701]\n",
      "Epoch 81/100001\n",
      "[C1 valid: -0.690700, C2 fake: 0.313657], [G loss: 0.004018, mse: 0.023933]\n",
      "Epoch 82/100001\n",
      "[C1 valid: -0.716414, C2 fake: 0.287112], [G loss: 0.003996, mse: 0.023930]\n",
      "Epoch 83/100001\n",
      "[C1 valid: -0.726509, C2 fake: 0.284020], [G loss: 0.003487, mse: 0.023054]\n",
      "Epoch 84/100001\n",
      "[C1 valid: -0.729246, C2 fake: 0.279965], [G loss: 0.003913, mse: 0.023772]\n",
      "Epoch 85/100001\n",
      "[C1 valid: -0.734727, C2 fake: 0.278688], [G loss: 0.003543, mse: 0.023134]\n",
      "Epoch 86/100001\n",
      "[C1 valid: -0.727975, C2 fake: 0.283845], [G loss: 0.004163, mse: 0.024417]\n",
      "Epoch 87/100001\n",
      "[C1 valid: -0.738041, C2 fake: 0.272810], [G loss: 0.003835, mse: 0.023548]\n",
      "Epoch 88/100001\n",
      "[C1 valid: -0.747378, C2 fake: 0.261885], [G loss: 0.003911, mse: 0.023314]\n",
      "Epoch 89/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.754549, C2 fake: 0.267893], [G loss: 0.003788, mse: 0.023331]\n",
      "Epoch 90/100001\n",
      "[C1 valid: -0.746824, C2 fake: 0.273584], [G loss: 0.003105, mse: 0.022008]\n",
      "Epoch 91/100001\n",
      "[C1 valid: -0.760958, C2 fake: 0.255624], [G loss: 0.003540, mse: 0.022120]\n",
      "Epoch 92/100001\n",
      "[C1 valid: -0.765245, C2 fake: 0.252847], [G loss: 0.003753, mse: 0.022697]\n",
      "Epoch 93/100001\n",
      "[C1 valid: -0.753850, C2 fake: 0.269133], [G loss: 0.002709, mse: 0.021492]\n",
      "Epoch 94/100001\n",
      "[C1 valid: -0.783100, C2 fake: 0.249463], [G loss: 0.003195, mse: 0.022288]\n",
      "Epoch 95/100001\n",
      "[C1 valid: -0.791429, C2 fake: 0.236908], [G loss: 0.002764, mse: 0.020765]\n",
      "Epoch 96/100001\n",
      "[C1 valid: -0.773589, C2 fake: 0.243445], [G loss: 0.002410, mse: 0.020623]\n",
      "Epoch 97/100001\n",
      "[C1 valid: -0.789975, C2 fake: 0.237957], [G loss: 0.002795, mse: 0.020172]\n",
      "Epoch 98/100001\n",
      "[C1 valid: -0.787701, C2 fake: 0.243261], [G loss: 0.001888, mse: 0.019689]\n",
      "Epoch 99/100001\n",
      "[C1 valid: -0.799425, C2 fake: 0.225141], [G loss: 0.002241, mse: 0.019874]\n",
      "Epoch 100/100001\n",
      "[C1 valid: -0.812444, C2 fake: 0.227412], [G loss: 0.005157, mse: 0.021212]\n",
      "Epoch 101/100001\n",
      "[C1 valid: -0.795305, C2 fake: 0.229550], [G loss: 0.002168, mse: 0.019680]\n",
      "Epoch 102/100001\n",
      "[C1 valid: -0.812975, C2 fake: 0.211858], [G loss: 0.002052, mse: 0.019228]\n",
      "Epoch 103/100001\n",
      "[C1 valid: -0.798775, C2 fake: 0.238117], [G loss: 0.001663, mse: 0.018799]\n",
      "Epoch 104/100001\n",
      "[C1 valid: -0.816783, C2 fake: 0.209824], [G loss: 0.002083, mse: 0.018642]\n",
      "Epoch 105/100001\n",
      "[C1 valid: -0.818388, C2 fake: 0.203446], [G loss: 0.002039, mse: 0.018996]\n",
      "Epoch 106/100001\n",
      "[C1 valid: -0.824469, C2 fake: 0.210608], [G loss: 0.002273, mse: 0.019974]\n",
      "Epoch 107/100001\n",
      "[C1 valid: -0.828230, C2 fake: 0.200982], [G loss: 0.002232, mse: 0.019352]\n",
      "Epoch 108/100001\n",
      "[C1 valid: -0.826007, C2 fake: 0.199731], [G loss: 0.002689, mse: 0.019083]\n",
      "Epoch 109/100001\n",
      "[C1 valid: -0.829336, C2 fake: 0.209263], [G loss: 0.002228, mse: 0.019263]\n",
      "Epoch 110/100001\n",
      "[C1 valid: -0.839788, C2 fake: 0.188989], [G loss: 0.002043, mse: 0.018685]\n",
      "Epoch 111/100001\n",
      "[C1 valid: -0.844151, C2 fake: 0.187638], [G loss: 0.003436, mse: 0.018830]\n",
      "Epoch 112/100001\n",
      "[C1 valid: -0.833291, C2 fake: 0.194639], [G loss: 0.002452, mse: 0.018291]\n",
      "Epoch 113/100001\n",
      "[C1 valid: -0.827429, C2 fake: 0.198638], [G loss: 0.001446, mse: 0.018179]\n",
      "Epoch 114/100001\n",
      "[C1 valid: -0.833599, C2 fake: 0.194961], [G loss: 0.001277, mse: 0.017668]\n",
      "Epoch 115/100001\n",
      "[C1 valid: -0.840866, C2 fake: 0.195625], [G loss: 0.000681, mse: 0.017442]\n",
      "Epoch 116/100001\n",
      "[C1 valid: -0.847473, C2 fake: 0.182215], [G loss: 0.000316, mse: 0.016622]\n",
      "Epoch 117/100001\n",
      "[C1 valid: -0.854769, C2 fake: 0.182292], [G loss: 0.004350, mse: 0.017367]\n",
      "Epoch 118/100001\n",
      "[C1 valid: -0.826813, C2 fake: 0.201360], [G loss: 0.000768, mse: 0.016896]\n",
      "Epoch 119/100001\n",
      "[C1 valid: -0.848964, C2 fake: 0.180086], [G loss: 0.001360, mse: 0.017106]\n",
      "Epoch 120/100001\n",
      "[C1 valid: -0.857074, C2 fake: 0.175735], [G loss: 0.000457, mse: 0.016802]\n",
      "Epoch 121/100001\n",
      "[C1 valid: -0.861477, C2 fake: 0.170539], [G loss: 0.001065, mse: 0.017122]\n",
      "Epoch 122/100001\n",
      "[C1 valid: -0.862586, C2 fake: 0.169414], [G loss: 0.000661, mse: 0.016989]\n",
      "Epoch 123/100001\n",
      "[C1 valid: -0.869766, C2 fake: 0.167017], [G loss: 0.000098, mse: 0.015402]\n",
      "Epoch 124/100001\n",
      "[C1 valid: -0.861298, C2 fake: 0.181552], [G loss: -0.000227, mse: 0.015878]\n",
      "Epoch 125/100001\n",
      "[C1 valid: -0.866438, C2 fake: 0.170698], [G loss: -0.000059, mse: 0.015616]\n",
      "Epoch 126/100001\n",
      "[C1 valid: -0.875207, C2 fake: 0.158427], [G loss: 0.001477, mse: 0.015780]\n",
      "Epoch 127/100001\n",
      "[C1 valid: -0.874151, C2 fake: 0.171188], [G loss: 0.000097, mse: 0.016140]\n",
      "Epoch 128/100001\n",
      "[C1 valid: -0.872097, C2 fake: 0.161673], [G loss: 0.000631, mse: 0.015116]\n",
      "Epoch 129/100001\n",
      "[C1 valid: -0.867603, C2 fake: 0.171620], [G loss: -0.000485, mse: 0.015086]\n",
      "Epoch 130/100001\n",
      "[C1 valid: -0.882276, C2 fake: 0.160936], [G loss: 0.000275, mse: 0.014450]\n",
      "Epoch 131/100001\n",
      "[C1 valid: -0.880461, C2 fake: 0.152193], [G loss: 0.000671, mse: 0.015351]\n",
      "Epoch 132/100001\n",
      "[C1 valid: -0.880395, C2 fake: 0.153260], [G loss: 0.000291, mse: 0.015363]\n",
      "Epoch 133/100001\n",
      "[C1 valid: -0.882995, C2 fake: 0.157811], [G loss: 0.001225, mse: 0.015221]\n",
      "Epoch 134/100001\n",
      "[C1 valid: -0.884377, C2 fake: 0.157488], [G loss: 0.000957, mse: 0.014962]\n",
      "Epoch 135/100001\n",
      "[C1 valid: -0.888708, C2 fake: 0.152993], [G loss: -0.001341, mse: 0.013975]\n",
      "Epoch 136/100001\n",
      "[C1 valid: -0.885741, C2 fake: 0.143536], [G loss: -0.000661, mse: 0.014644]\n",
      "Epoch 137/100001\n",
      "[C1 valid: -0.892694, C2 fake: 0.153833], [G loss: 0.000176, mse: 0.015795]\n",
      "Epoch 138/100001\n",
      "[C1 valid: -0.893548, C2 fake: 0.143887], [G loss: 0.001995, mse: 0.014061]\n",
      "Epoch 139/100001\n",
      "[C1 valid: -0.894788, C2 fake: 0.146367], [G loss: -0.000792, mse: 0.013407]\n",
      "Epoch 140/100001\n",
      "[C1 valid: -0.893940, C2 fake: 0.151500], [G loss: -0.001074, mse: 0.014244]\n",
      "Epoch 141/100001\n",
      "[C1 valid: -0.902378, C2 fake: 0.140889], [G loss: -0.001335, mse: 0.014144]\n",
      "Epoch 142/100001\n",
      "[C1 valid: -0.900296, C2 fake: 0.138253], [G loss: 0.000343, mse: 0.014657]\n",
      "Epoch 143/100001\n",
      "[C1 valid: -0.894768, C2 fake: 0.139522], [G loss: -0.000976, mse: 0.014122]\n",
      "Epoch 144/100001\n",
      "[C1 valid: -0.870567, C2 fake: 0.193419], [G loss: -0.000963, mse: 0.014619]\n",
      "Epoch 145/100001\n",
      "[C1 valid: -0.842047, C2 fake: 0.186428], [G loss: -0.002094, mse: 0.013237]\n",
      "Epoch 146/100001\n",
      "[C1 valid: -0.901440, C2 fake: 0.142891], [G loss: -0.001421, mse: 0.014172]\n",
      "Epoch 147/100001\n",
      "[C1 valid: -0.910888, C2 fake: 0.140768], [G loss: -0.001900, mse: 0.013288]\n",
      "Epoch 148/100001\n",
      "[C1 valid: -0.902755, C2 fake: 0.136891], [G loss: -0.001674, mse: 0.013811]\n",
      "Epoch 149/100001\n",
      "[C1 valid: -0.912629, C2 fake: 0.132736], [G loss: -0.002262, mse: 0.012397]\n",
      "Epoch 150/100001\n",
      "[C1 valid: -0.912526, C2 fake: 0.132257], [G loss: -0.001601, mse: 0.013004]\n",
      "Epoch 151/100001\n",
      "[C1 valid: -0.913779, C2 fake: 0.130492], [G loss: -0.000991, mse: 0.012920]\n",
      "Epoch 152/100001\n",
      "[C1 valid: -0.914907, C2 fake: 0.136769], [G loss: -0.001722, mse: 0.012997]\n",
      "Epoch 153/100001\n",
      "[C1 valid: -0.913101, C2 fake: 0.129976], [G loss: -0.001602, mse: 0.012780]\n",
      "Epoch 154/100001\n",
      "[C1 valid: -0.914537, C2 fake: 0.130854], [G loss: -0.001482, mse: 0.012524]\n",
      "Epoch 155/100001\n",
      "[C1 valid: -0.917084, C2 fake: 0.124849], [G loss: -0.002433, mse: 0.011741]\n",
      "Epoch 156/100001\n",
      "[C1 valid: -0.922697, C2 fake: 0.125179], [G loss: -0.001261, mse: 0.012407]\n",
      "Epoch 157/100001\n",
      "[C1 valid: -0.919251, C2 fake: 0.126693], [G loss: -0.001772, mse: 0.013470]\n",
      "Epoch 158/100001\n",
      "[C1 valid: -0.923736, C2 fake: 0.123353], [G loss: -0.000592, mse: 0.012172]\n",
      "Epoch 159/100001\n",
      "[C1 valid: -0.899947, C2 fake: 0.139376], [G loss: 0.000178, mse: 0.012559]\n",
      "Epoch 160/100001\n",
      "[C1 valid: -0.908552, C2 fake: 0.163168], [G loss: -0.002996, mse: 0.011840]\n",
      "Epoch 161/100001\n",
      "[C1 valid: -0.920459, C2 fake: 0.129361], [G loss: -0.003093, mse: 0.011917]\n",
      "Epoch 162/100001\n",
      "[C1 valid: -0.916208, C2 fake: 0.123738], [G loss: -0.002430, mse: 0.011684]\n",
      "Epoch 163/100001\n",
      "[C1 valid: -0.921761, C2 fake: 0.121358], [G loss: -0.002934, mse: 0.011934]\n",
      "Epoch 164/100001\n",
      "[C1 valid: -0.924398, C2 fake: 0.121890], [G loss: -0.002070, mse: 0.012416]\n",
      "Epoch 165/100001\n",
      "[C1 valid: -0.920395, C2 fake: 0.117388], [G loss: -0.002543, mse: 0.011907]\n",
      "Epoch 166/100001\n",
      "[C1 valid: -0.930166, C2 fake: 0.117642], [G loss: -0.002630, mse: 0.011285]\n",
      "Epoch 167/100001\n",
      "[C1 valid: -0.928895, C2 fake: 0.117911], [G loss: -0.002924, mse: 0.011713]\n",
      "Epoch 168/100001\n",
      "[C1 valid: -0.929516, C2 fake: 0.113726], [G loss: -0.002964, mse: 0.011264]\n",
      "Epoch 169/100001\n",
      "[C1 valid: -0.929498, C2 fake: 0.119088], [G loss: -0.001730, mse: 0.010824]\n",
      "Epoch 170/100001\n",
      "[C1 valid: -0.931169, C2 fake: 0.115084], [G loss: -0.003125, mse: 0.011036]\n",
      "Epoch 171/100001\n",
      "[C1 valid: -0.931708, C2 fake: 0.112975], [G loss: -0.002880, mse: 0.011150]\n",
      "Epoch 172/100001\n",
      "[C1 valid: -0.928247, C2 fake: 0.112313], [G loss: -0.002699, mse: 0.010113]\n",
      "Epoch 173/100001\n",
      "[C1 valid: -0.929164, C2 fake: 0.113196], [G loss: -0.003434, mse: 0.010529]\n",
      "Epoch 174/100001\n",
      "[C1 valid: -0.929710, C2 fake: 0.124165], [G loss: -0.000226, mse: 0.010870]\n",
      "Epoch 175/100001\n",
      "[C1 valid: -0.932006, C2 fake: 0.108350], [G loss: -0.003526, mse: 0.010940]\n",
      "Epoch 176/100001\n",
      "[C1 valid: -0.935645, C2 fake: 0.116743], [G loss: -0.002686, mse: 0.011342]\n",
      "Epoch 177/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.928039, C2 fake: 0.112508], [G loss: -0.003075, mse: 0.011128]\n",
      "Epoch 178/100001\n",
      "[C1 valid: -0.933200, C2 fake: 0.104823], [G loss: -0.003700, mse: 0.010063]\n",
      "Epoch 179/100001\n",
      "[C1 valid: -0.935909, C2 fake: 0.106753], [G loss: -0.003180, mse: 0.010614]\n",
      "Epoch 180/100001\n",
      "[C1 valid: -0.931604, C2 fake: 0.104346], [G loss: -0.002796, mse: 0.010638]\n",
      "Epoch 181/100001\n",
      "[C1 valid: -0.938231, C2 fake: 0.107550], [G loss: -0.002889, mse: 0.010291]\n",
      "Epoch 182/100001\n",
      "[C1 valid: -0.933997, C2 fake: 0.106812], [G loss: -0.003259, mse: 0.010525]\n",
      "Epoch 183/100001\n",
      "[C1 valid: -0.936234, C2 fake: 0.101890], [G loss: -0.003895, mse: 0.010231]\n",
      "Epoch 184/100001\n",
      "[C1 valid: -0.941262, C2 fake: 0.104019], [G loss: -0.003412, mse: 0.010424]\n",
      "Epoch 185/100001\n",
      "[C1 valid: -0.941607, C2 fake: 0.106725], [G loss: -0.003591, mse: 0.010044]\n",
      "Epoch 186/100001\n",
      "[C1 valid: -0.941274, C2 fake: 0.102947], [G loss: -0.003325, mse: 0.009534]\n",
      "Epoch 187/100001\n",
      "[C1 valid: -0.915652, C2 fake: 0.112814], [G loss: -0.003997, mse: 0.010565]\n",
      "Epoch 188/100001\n",
      "[C1 valid: -0.906002, C2 fake: 0.148691], [G loss: -0.004438, mse: 0.009889]\n",
      "Epoch 189/100001\n",
      "[C1 valid: -0.930569, C2 fake: 0.106260], [G loss: -0.004496, mse: 0.009833]\n",
      "Epoch 190/100001\n",
      "[C1 valid: -0.937512, C2 fake: 0.110413], [G loss: -0.004727, mse: 0.009375]\n",
      "Epoch 191/100001\n",
      "[C1 valid: -0.941113, C2 fake: 0.104421], [G loss: -0.004469, mse: 0.009753]\n",
      "Epoch 192/100001\n",
      "[C1 valid: -0.942194, C2 fake: 0.100758], [G loss: -0.004166, mse: 0.009885]\n",
      "Epoch 193/100001\n",
      "[C1 valid: -0.945489, C2 fake: 0.101557], [G loss: -0.004371, mse: 0.009272]\n",
      "Epoch 194/100001\n",
      "[C1 valid: -0.942279, C2 fake: 0.099926], [G loss: -0.004457, mse: 0.009591]\n",
      "Epoch 195/100001\n",
      "[C1 valid: -0.950481, C2 fake: 0.097224], [G loss: -0.004587, mse: 0.009173]\n",
      "Epoch 196/100001\n",
      "[C1 valid: -0.946306, C2 fake: 0.100841], [G loss: -0.004182, mse: 0.009189]\n",
      "Epoch 197/100001\n",
      "[C1 valid: -0.943484, C2 fake: 0.106966], [G loss: -0.004477, mse: 0.009316]\n",
      "Epoch 198/100001\n",
      "[C1 valid: -0.942827, C2 fake: 0.102114], [G loss: -0.004248, mse: 0.009500]\n",
      "Epoch 199/100001\n",
      "[C1 valid: -0.947239, C2 fake: 0.101277], [G loss: -0.004548, mse: 0.009125]\n",
      "Epoch 200/100001\n",
      "[C1 valid: -0.949596, C2 fake: 0.096677], [G loss: -0.004376, mse: 0.009124]\n",
      "Epoch 201/100001\n",
      "[C1 valid: -0.948919, C2 fake: 0.097359], [G loss: -0.004380, mse: 0.008935]\n",
      "Epoch 202/100001\n",
      "[C1 valid: -0.950965, C2 fake: 0.096095], [G loss: -0.004806, mse: 0.008875]\n",
      "Epoch 203/100001\n",
      "[C1 valid: -0.949784, C2 fake: 0.095422], [G loss: -0.003997, mse: 0.009383]\n",
      "Epoch 204/100001\n",
      "[C1 valid: -0.944623, C2 fake: 0.092395], [G loss: -0.004791, mse: 0.008647]\n",
      "Epoch 205/100001\n",
      "[C1 valid: -0.952074, C2 fake: 0.093833], [G loss: -0.004819, mse: 0.008469]\n",
      "Epoch 206/100001\n",
      "[C1 valid: -0.951055, C2 fake: 0.094685], [G loss: -0.004362, mse: 0.009017]\n",
      "Epoch 207/100001\n",
      "[C1 valid: -0.955191, C2 fake: 0.091851], [G loss: -0.004701, mse: 0.008904]\n",
      "Epoch 208/100001\n",
      "[C1 valid: -0.949771, C2 fake: 0.090036], [G loss: -0.004716, mse: 0.008919]\n",
      "Epoch 209/100001\n",
      "[C1 valid: -0.951182, C2 fake: 0.094549], [G loss: -0.004672, mse: 0.009062]\n",
      "Epoch 210/100001\n",
      "[C1 valid: -0.950663, C2 fake: 0.096109], [G loss: -0.004538, mse: 0.008966]\n",
      "Epoch 211/100001\n",
      "[C1 valid: -0.952223, C2 fake: 0.098371], [G loss: -0.004784, mse: 0.008967]\n",
      "Epoch 212/100001\n",
      "[C1 valid: -0.951300, C2 fake: 0.094572], [G loss: -0.004706, mse: 0.008798]\n",
      "Epoch 213/100001\n",
      "[C1 valid: -0.950437, C2 fake: 0.089466], [G loss: -0.004666, mse: 0.008455]\n",
      "Epoch 214/100001\n",
      "[C1 valid: -0.951048, C2 fake: 0.095382], [G loss: -0.004340, mse: 0.008575]\n",
      "Epoch 215/100001\n",
      "[C1 valid: -0.952666, C2 fake: 0.099496], [G loss: -0.004696, mse: 0.009123]\n",
      "Epoch 216/100001\n",
      "[C1 valid: -0.955425, C2 fake: 0.091145], [G loss: -0.004642, mse: 0.008726]\n",
      "Epoch 217/100001\n",
      "[C1 valid: -0.951868, C2 fake: 0.089704], [G loss: -0.004057, mse: 0.008308]\n",
      "Epoch 218/100001\n",
      "[C1 valid: -0.953138, C2 fake: 0.092425], [G loss: -0.005242, mse: 0.008136]\n",
      "Epoch 219/100001\n",
      "[C1 valid: -0.955600, C2 fake: 0.089443], [G loss: -0.005117, mse: 0.008146]\n",
      "Epoch 220/100001\n",
      "[C1 valid: -0.928253, C2 fake: 0.104595], [G loss: -0.004542, mse: 0.008331]\n",
      "Epoch 221/100001\n",
      "[C1 valid: -0.930693, C2 fake: 0.091777], [G loss: -0.004800, mse: 0.008150]\n",
      "Epoch 222/100001\n",
      "[C1 valid: -0.957319, C2 fake: 0.102991], [G loss: -0.005201, mse: 0.008640]\n",
      "Epoch 223/100001\n",
      "[C1 valid: -0.959276, C2 fake: 0.096937], [G loss: -0.005461, mse: 0.008039]\n",
      "Epoch 224/100001\n",
      "[C1 valid: -0.954085, C2 fake: 0.092162], [G loss: -0.005527, mse: 0.007953]\n",
      "Epoch 225/100001\n",
      "[C1 valid: -0.957686, C2 fake: 0.088365], [G loss: -0.005574, mse: 0.007782]\n",
      "Epoch 226/100001\n",
      "[C1 valid: -0.959264, C2 fake: 0.085996], [G loss: -0.005273, mse: 0.008029]\n",
      "Epoch 227/100001\n",
      "[C1 valid: -0.956366, C2 fake: 0.089062], [G loss: -0.005451, mse: 0.007881]\n",
      "Epoch 228/100001\n",
      "[C1 valid: -0.958069, C2 fake: 0.086321], [G loss: -0.005233, mse: 0.008350]\n",
      "Epoch 229/100001\n",
      "[C1 valid: -0.960850, C2 fake: 0.089986], [G loss: -0.005461, mse: 0.008249]\n",
      "Epoch 230/100001\n",
      "[C1 valid: -0.962655, C2 fake: 0.090954], [G loss: -0.005091, mse: 0.008076]\n",
      "Epoch 231/100001\n",
      "[C1 valid: -0.959452, C2 fake: 0.085100], [G loss: -0.005404, mse: 0.008136]\n",
      "Epoch 232/100001\n",
      "[C1 valid: -0.959131, C2 fake: 0.088925], [G loss: -0.005459, mse: 0.008084]\n",
      "Epoch 233/100001\n",
      "[C1 valid: -0.958412, C2 fake: 0.087255], [G loss: -0.005649, mse: 0.007598]\n",
      "Epoch 234/100001\n",
      "[C1 valid: -0.943646, C2 fake: 0.096586], [G loss: -0.005665, mse: 0.007768]\n",
      "Epoch 235/100001\n",
      "[C1 valid: -0.956900, C2 fake: 0.088285], [G loss: -0.005547, mse: 0.007816]\n",
      "Epoch 236/100001\n",
      "[C1 valid: -0.961076, C2 fake: 0.087441], [G loss: -0.005644, mse: 0.007861]\n",
      "Epoch 237/100001\n",
      "[C1 valid: -0.962737, C2 fake: 0.087104], [G loss: -0.005079, mse: 0.008155]\n",
      "Epoch 238/100001\n",
      "[C1 valid: -0.957244, C2 fake: 0.085392], [G loss: -0.005550, mse: 0.007424]\n",
      "Epoch 239/100001\n",
      "[C1 valid: -0.958349, C2 fake: 0.085401], [G loss: -0.005866, mse: 0.007231]\n",
      "Epoch 240/100001\n",
      "[C1 valid: -0.960211, C2 fake: 0.086621], [G loss: -0.005591, mse: 0.007722]\n",
      "Epoch 241/100001\n",
      "[C1 valid: -0.958563, C2 fake: 0.095579], [G loss: -0.004772, mse: 0.007641]\n",
      "Epoch 242/100001\n",
      "[C1 valid: -0.963799, C2 fake: 0.086545], [G loss: -0.005710, mse: 0.007366]\n",
      "Epoch 243/100001\n",
      "[C1 valid: -0.964539, C2 fake: 0.084787], [G loss: -0.004686, mse: 0.007468]\n",
      "Epoch 244/100001\n",
      "[C1 valid: -0.963132, C2 fake: 0.086775], [G loss: -0.005266, mse: 0.007825]\n",
      "Epoch 245/100001\n",
      "[C1 valid: -0.963151, C2 fake: 0.082910], [G loss: -0.005685, mse: 0.007086]\n",
      "Epoch 246/100001\n",
      "[C1 valid: -0.959620, C2 fake: 0.080727], [G loss: -0.005210, mse: 0.007598]\n",
      "Epoch 247/100001\n",
      "[C1 valid: -0.964400, C2 fake: 0.081394], [G loss: -0.005530, mse: 0.007500]\n",
      "Epoch 248/100001\n",
      "[C1 valid: -0.962933, C2 fake: 0.082599], [G loss: -0.005727, mse: 0.007156]\n",
      "Epoch 249/100001\n",
      "[C1 valid: -0.966782, C2 fake: 0.083184], [G loss: -0.004850, mse: 0.007358]\n",
      "Epoch 250/100001\n",
      "[C1 valid: -0.949820, C2 fake: 0.093424], [G loss: -0.005524, mse: 0.007198]\n",
      "Epoch 251/100001\n",
      "[C1 valid: -0.964586, C2 fake: 0.082689], [G loss: -0.005689, mse: 0.007536]\n",
      "Epoch 252/100001\n",
      "[C1 valid: -0.966682, C2 fake: 0.077481], [G loss: -0.005754, mse: 0.007115]\n",
      "Epoch 253/100001\n",
      "[C1 valid: -0.963187, C2 fake: 0.083040], [G loss: -0.005875, mse: 0.006950]\n",
      "Epoch 254/100001\n",
      "[C1 valid: -0.963303, C2 fake: 0.082977], [G loss: -0.005839, mse: 0.007008]\n",
      "Epoch 255/100001\n",
      "[C1 valid: -0.960505, C2 fake: 0.078526], [G loss: -0.005839, mse: 0.006959]\n",
      "Epoch 256/100001\n",
      "[C1 valid: -0.963774, C2 fake: 0.077076], [G loss: -0.005078, mse: 0.007144]\n",
      "Epoch 257/100001\n",
      "[C1 valid: -0.964824, C2 fake: 0.082748], [G loss: -0.005790, mse: 0.006953]\n",
      "Epoch 258/100001\n",
      "[C1 valid: -0.963076, C2 fake: 0.079370], [G loss: -0.005899, mse: 0.006528]\n",
      "Epoch 259/100001\n",
      "[C1 valid: -0.965211, C2 fake: 0.075751], [G loss: -0.005654, mse: 0.006968]\n",
      "Epoch 260/100001\n",
      "[C1 valid: -0.961516, C2 fake: 0.080295], [G loss: -0.005717, mse: 0.006695]\n",
      "Epoch 261/100001\n",
      "[C1 valid: -0.966199, C2 fake: 0.076115], [G loss: -0.005853, mse: 0.006687]\n",
      "Epoch 262/100001\n",
      "[C1 valid: -0.967068, C2 fake: 0.075153], [G loss: -0.005559, mse: 0.006532]\n",
      "Epoch 263/100001\n",
      "[C1 valid: -0.963132, C2 fake: 0.074071], [G loss: -0.005660, mse: 0.006877]\n",
      "Epoch 264/100001\n",
      "[C1 valid: -0.958244, C2 fake: 0.096726], [G loss: -0.002774, mse: 0.006984]\n",
      "Epoch 265/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.956532, C2 fake: 0.085027], [G loss: -0.005883, mse: 0.006653]\n",
      "Epoch 266/100001\n",
      "[C1 valid: -0.964535, C2 fake: 0.081722], [G loss: -0.005765, mse: 0.006891]\n",
      "Epoch 267/100001\n",
      "[C1 valid: -0.966668, C2 fake: 0.080139], [G loss: -0.005754, mse: 0.006877]\n",
      "Epoch 268/100001\n",
      "[C1 valid: -0.966598, C2 fake: 0.079037], [G loss: -0.005826, mse: 0.006723]\n",
      "Epoch 269/100001\n",
      "[C1 valid: -0.962627, C2 fake: 0.079079], [G loss: -0.005872, mse: 0.006894]\n",
      "Epoch 270/100001\n",
      "[C1 valid: -0.967157, C2 fake: 0.077161], [G loss: -0.006005, mse: 0.006528]\n",
      "Epoch 271/100001\n",
      "[C1 valid: -0.968511, C2 fake: 0.074553], [G loss: -0.005964, mse: 0.006436]\n",
      "Epoch 272/100001\n",
      "[C1 valid: -0.967624, C2 fake: 0.076414], [G loss: -0.005953, mse: 0.006731]\n",
      "Epoch 273/100001\n",
      "[C1 valid: -0.969947, C2 fake: 0.082729], [G loss: -0.006016, mse: 0.006628]\n",
      "Epoch 274/100001\n",
      "[C1 valid: -0.967943, C2 fake: 0.076750], [G loss: -0.005814, mse: 0.006709]\n",
      "Epoch 275/100001\n",
      "[C1 valid: -0.970811, C2 fake: 0.075107], [G loss: -0.005745, mse: 0.006443]\n",
      "Epoch 276/100001\n",
      "[C1 valid: -0.969555, C2 fake: 0.085650], [G loss: -0.002058, mse: 0.006449]\n",
      "Epoch 277/100001\n",
      "[C1 valid: -0.953593, C2 fake: 0.090232], [G loss: -0.006213, mse: 0.006456]\n",
      "Epoch 278/100001\n",
      "[C1 valid: -0.970255, C2 fake: 0.081089], [G loss: -0.005954, mse: 0.006628]\n",
      "Epoch 279/100001\n",
      "[C1 valid: -0.967301, C2 fake: 0.085023], [G loss: -0.006382, mse: 0.006350]\n",
      "Epoch 280/100001\n",
      "[C1 valid: -0.967145, C2 fake: 0.078118], [G loss: -0.006269, mse: 0.006387]\n",
      "Epoch 281/100001\n",
      "[C1 valid: -0.969930, C2 fake: 0.078537], [G loss: -0.006547, mse: 0.005927]\n",
      "Epoch 282/100001\n",
      "[C1 valid: -0.970693, C2 fake: 0.079861], [G loss: -0.006150, mse: 0.006542]\n",
      "Epoch 283/100001\n",
      "[C1 valid: -0.971326, C2 fake: 0.074041], [G loss: -0.006118, mse: 0.006491]\n",
      "Epoch 284/100001\n",
      "[C1 valid: -0.970346, C2 fake: 0.074826], [G loss: -0.006370, mse: 0.006071]\n",
      "Epoch 285/100001\n",
      "[C1 valid: -0.964797, C2 fake: 0.084909], [G loss: -0.006489, mse: 0.006201]\n",
      "Epoch 286/100001\n",
      "[C1 valid: -0.968635, C2 fake: 0.075217], [G loss: -0.006419, mse: 0.006163]\n",
      "Epoch 287/100001\n",
      "[C1 valid: -0.971905, C2 fake: 0.073191], [G loss: -0.006264, mse: 0.006516]\n",
      "Epoch 288/100001\n",
      "[C1 valid: -0.973829, C2 fake: 0.072898], [G loss: -0.006263, mse: 0.006290]\n",
      "Epoch 289/100001\n",
      "[C1 valid: -0.971994, C2 fake: 0.073734], [G loss: -0.006051, mse: 0.006355]\n",
      "Epoch 290/100001\n",
      "[C1 valid: -0.967877, C2 fake: 0.074107], [G loss: -0.006402, mse: 0.006144]\n",
      "Epoch 291/100001\n",
      "[C1 valid: -0.973931, C2 fake: 0.072504], [G loss: -0.006298, mse: 0.006055]\n",
      "Epoch 292/100001\n",
      "[C1 valid: -0.972308, C2 fake: 0.077148], [G loss: -0.006218, mse: 0.006523]\n",
      "Epoch 293/100001\n",
      "[C1 valid: -0.971091, C2 fake: 0.068956], [G loss: -0.005984, mse: 0.006539]\n",
      "Epoch 294/100001\n",
      "[C1 valid: -0.973724, C2 fake: 0.070953], [G loss: -0.006190, mse: 0.006289]\n",
      "Epoch 295/100001\n",
      "[C1 valid: -0.972937, C2 fake: 0.069225], [G loss: -0.006399, mse: 0.005691]\n",
      "Epoch 296/100001\n",
      "[C1 valid: -0.951565, C2 fake: 0.098339], [G loss: -0.005788, mse: 0.006173]\n",
      "Epoch 297/100001\n",
      "[C1 valid: -0.957448, C2 fake: 0.077253], [G loss: -0.006453, mse: 0.006140]\n",
      "Epoch 298/100001\n",
      "[C1 valid: -0.972691, C2 fake: 0.074961], [G loss: -0.006486, mse: 0.006078]\n",
      "Epoch 299/100001\n",
      "[C1 valid: -0.973765, C2 fake: 0.073956], [G loss: -0.006658, mse: 0.005864]\n",
      "Epoch 300/100001\n",
      "[C1 valid: -0.971257, C2 fake: 0.072295], [G loss: -0.006384, mse: 0.006242]\n",
      "Epoch 301/100001\n",
      "[C1 valid: -0.973511, C2 fake: 0.072982], [G loss: -0.006383, mse: 0.006269]\n",
      "Epoch 302/100001\n",
      "[C1 valid: -0.974070, C2 fake: 0.070669], [G loss: -0.006511, mse: 0.006152]\n",
      "Epoch 303/100001\n",
      "[C1 valid: -0.974764, C2 fake: 0.072719], [G loss: -0.006397, mse: 0.006091]\n",
      "Epoch 304/100001\n",
      "[C1 valid: -0.974437, C2 fake: 0.070382], [G loss: -0.006391, mse: 0.006373]\n",
      "Epoch 305/100001\n",
      "[C1 valid: -0.974358, C2 fake: 0.071580], [G loss: -0.006464, mse: 0.005885]\n",
      "Epoch 306/100001\n",
      "[C1 valid: -0.976428, C2 fake: 0.067902], [G loss: -0.006283, mse: 0.005863]\n",
      "Epoch 307/100001\n",
      "[C1 valid: -0.973665, C2 fake: 0.079295], [G loss: -0.004775, mse: 0.005732]\n",
      "Epoch 308/100001\n",
      "[C1 valid: -0.955849, C2 fake: 0.123245], [G loss: -0.005978, mse: 0.005855]\n",
      "Epoch 309/100001\n",
      "[C1 valid: -0.968024, C2 fake: 0.079340], [G loss: -0.006513, mse: 0.005827]\n",
      "Epoch 310/100001\n",
      "[C1 valid: -0.972805, C2 fake: 0.073244], [G loss: -0.006437, mse: 0.005962]\n",
      "Epoch 311/100001\n",
      "[C1 valid: -0.975423, C2 fake: 0.074526], [G loss: -0.006663, mse: 0.005678]\n",
      "Epoch 312/100001\n",
      "[C1 valid: -0.974226, C2 fake: 0.071187], [G loss: -0.005970, mse: 0.005861]\n",
      "Epoch 313/100001\n",
      "[C1 valid: -0.974335, C2 fake: 0.072455], [G loss: -0.006431, mse: 0.005682]\n",
      "Epoch 314/100001\n",
      "[C1 valid: -0.974658, C2 fake: 0.074503], [G loss: -0.006243, mse: 0.005881]\n",
      "Epoch 315/100001\n",
      "[C1 valid: -0.975551, C2 fake: 0.070629], [G loss: -0.006150, mse: 0.005692]\n",
      "Epoch 316/100001\n",
      "[C1 valid: -0.974591, C2 fake: 0.069698], [G loss: -0.006425, mse: 0.005604]\n",
      "Epoch 317/100001\n",
      "[C1 valid: -0.976367, C2 fake: 0.068661], [G loss: -0.006208, mse: 0.005634]\n",
      "Epoch 318/100001\n",
      "[C1 valid: -0.976583, C2 fake: 0.066578], [G loss: -0.006289, mse: 0.005736]\n",
      "Epoch 319/100001\n",
      "[C1 valid: -0.975083, C2 fake: 0.069046], [G loss: -0.006232, mse: 0.005719]\n",
      "Epoch 320/100001\n",
      "[C1 valid: -0.977180, C2 fake: 0.068290], [G loss: -0.006103, mse: 0.005884]\n",
      "Epoch 321/100001\n",
      "[C1 valid: -0.975720, C2 fake: 0.064579], [G loss: -0.006507, mse: 0.005402]\n",
      "Epoch 322/100001\n",
      "[C1 valid: -0.974778, C2 fake: 0.068914], [G loss: -0.006131, mse: 0.005562]\n",
      "Epoch 323/100001\n",
      "[C1 valid: -0.974154, C2 fake: 0.067240], [G loss: -0.006714, mse: 0.005347]\n",
      "Epoch 324/100001\n",
      "[C1 valid: -0.978813, C2 fake: 0.065439], [G loss: -0.006423, mse: 0.005602]\n",
      "Epoch 325/100001\n",
      "[C1 valid: -0.976494, C2 fake: 0.075562], [G loss: -0.006770, mse: 0.005322]\n",
      "Epoch 326/100001\n",
      "[C1 valid: -0.976858, C2 fake: 0.077386], [G loss: -0.006386, mse: 0.005871]\n",
      "Epoch 327/100001\n",
      "[C1 valid: -0.974644, C2 fake: 0.069196], [G loss: -0.006784, mse: 0.005400]\n",
      "Epoch 328/100001\n",
      "[C1 valid: -0.977084, C2 fake: 0.067792], [G loss: -0.006482, mse: 0.005713]\n",
      "Epoch 329/100001\n",
      "[C1 valid: -0.975669, C2 fake: 0.067057], [G loss: -0.006497, mse: 0.005683]\n",
      "Epoch 330/100001\n",
      "[C1 valid: -0.975205, C2 fake: 0.066663], [G loss: -0.006522, mse: 0.005644]\n",
      "Epoch 331/100001\n",
      "[C1 valid: -0.972668, C2 fake: 0.071441], [G loss: -0.006647, mse: 0.005474]\n",
      "Epoch 332/100001\n",
      "[C1 valid: -0.977213, C2 fake: 0.068942], [G loss: -0.006411, mse: 0.005569]\n",
      "Epoch 333/100001\n",
      "[C1 valid: -0.978940, C2 fake: 0.064827], [G loss: -0.006393, mse: 0.005257]\n",
      "Epoch 334/100001\n",
      "[C1 valid: -0.978484, C2 fake: 0.063884], [G loss: -0.006607, mse: 0.005518]\n",
      "Epoch 335/100001\n",
      "[C1 valid: -0.975716, C2 fake: 0.065277], [G loss: -0.006437, mse: 0.005199]\n",
      "Epoch 336/100001\n",
      "[C1 valid: -0.977542, C2 fake: 0.064010], [G loss: -0.006153, mse: 0.005168]\n",
      "Epoch 337/100001\n",
      "[C1 valid: -0.979232, C2 fake: 0.073900], [G loss: -0.006462, mse: 0.005382]\n",
      "Epoch 338/100001\n",
      "[C1 valid: -0.979779, C2 fake: 0.067973], [G loss: -0.006651, mse: 0.005642]\n",
      "Epoch 339/100001\n",
      "[C1 valid: -0.977523, C2 fake: 0.066891], [G loss: -0.006477, mse: 0.005409]\n",
      "Epoch 340/100001\n",
      "[C1 valid: -0.980303, C2 fake: 0.063544], [G loss: -0.006679, mse: 0.005239]\n",
      "Epoch 341/100001\n",
      "[C1 valid: -0.979977, C2 fake: 0.062816], [G loss: -0.006762, mse: 0.005486]\n",
      "Epoch 342/100001\n",
      "[C1 valid: -0.979113, C2 fake: 0.063151], [G loss: -0.006688, mse: 0.005184]\n",
      "Epoch 343/100001\n",
      "[C1 valid: -0.980407, C2 fake: 0.062593], [G loss: -0.006680, mse: 0.005217]\n",
      "Epoch 344/100001\n",
      "[C1 valid: -0.871373, C2 fake: 0.172217], [G loss: -0.006484, mse: 0.005441]\n",
      "Epoch 345/100001\n",
      "[C1 valid: -0.957229, C2 fake: 0.070122], [G loss: -0.006732, mse: 0.005224]\n",
      "Epoch 346/100001\n",
      "[C1 valid: -0.978207, C2 fake: 0.071251], [G loss: -0.006917, mse: 0.004995]\n",
      "Epoch 347/100001\n",
      "[C1 valid: -0.975507, C2 fake: 0.066575], [G loss: -0.007024, mse: 0.004930]\n",
      "Epoch 348/100001\n",
      "[C1 valid: -0.974855, C2 fake: 0.068185], [G loss: -0.007123, mse: 0.005079]\n",
      "Epoch 349/100001\n",
      "[C1 valid: -0.974781, C2 fake: 0.064841], [G loss: -0.007071, mse: 0.005114]\n",
      "Epoch 350/100001\n",
      "[C1 valid: -0.980772, C2 fake: 0.067471], [G loss: -0.007052, mse: 0.005248]\n",
      "Epoch 351/100001\n",
      "[C1 valid: -0.981950, C2 fake: 0.066644], [G loss: -0.007075, mse: 0.005197]\n",
      "Epoch 352/100001\n",
      "[C1 valid: -0.981030, C2 fake: 0.063794], [G loss: -0.007097, mse: 0.005025]\n",
      "Epoch 353/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.977382, C2 fake: 0.064990], [G loss: -0.007101, mse: 0.005049]\n",
      "Epoch 354/100001\n",
      "[C1 valid: -0.980543, C2 fake: 0.065051], [G loss: -0.007222, mse: 0.005027]\n",
      "Epoch 355/100001\n",
      "[C1 valid: -0.980592, C2 fake: 0.070628], [G loss: -0.005824, mse: 0.005040]\n",
      "Epoch 356/100001\n",
      "[C1 valid: -0.975233, C2 fake: 0.090714], [G loss: -0.005939, mse: 0.004991]\n",
      "Epoch 357/100001\n",
      "[C1 valid: -0.974605, C2 fake: 0.080012], [G loss: -0.006519, mse: 0.004939]\n",
      "Epoch 358/100001\n",
      "[C1 valid: -0.977686, C2 fake: 0.068778], [G loss: -0.006463, mse: 0.004968]\n",
      "Epoch 359/100001\n",
      "[C1 valid: -0.978857, C2 fake: 0.068102], [G loss: -0.006790, mse: 0.004981]\n",
      "Epoch 360/100001\n",
      "[C1 valid: -0.978692, C2 fake: 0.067850], [G loss: -0.006709, mse: 0.005070]\n",
      "Epoch 361/100001\n",
      "[C1 valid: -0.979570, C2 fake: 0.068031], [G loss: -0.006798, mse: 0.004902]\n",
      "Epoch 362/100001\n",
      "[C1 valid: -0.979166, C2 fake: 0.064457], [G loss: -0.006984, mse: 0.004849]\n",
      "Epoch 363/100001\n",
      "[C1 valid: -0.976106, C2 fake: 0.063056], [G loss: -0.006790, mse: 0.005002]\n",
      "Epoch 364/100001\n",
      "[C1 valid: -0.979836, C2 fake: 0.066241], [G loss: -0.006848, mse: 0.004905]\n",
      "Epoch 365/100001\n",
      "[C1 valid: -0.977673, C2 fake: 0.064898], [G loss: -0.006901, mse: 0.005037]\n",
      "Epoch 366/100001\n",
      "[C1 valid: -0.981190, C2 fake: 0.060000], [G loss: -0.006477, mse: 0.005456]\n",
      "Epoch 367/100001\n",
      "[C1 valid: -0.981010, C2 fake: 0.064703], [G loss: -0.006736, mse: 0.005209]\n",
      "Epoch 368/100001\n",
      "[C1 valid: -0.983137, C2 fake: 0.061638], [G loss: -0.006964, mse: 0.004991]\n",
      "Epoch 369/100001\n",
      "[C1 valid: -0.980857, C2 fake: 0.063887], [G loss: -0.006764, mse: 0.004733]\n",
      "Epoch 370/100001\n",
      "[C1 valid: -0.979681, C2 fake: 0.063991], [G loss: -0.006957, mse: 0.004846]\n",
      "Epoch 371/100001\n",
      "[C1 valid: -0.980009, C2 fake: 0.062288], [G loss: -0.006853, mse: 0.004667]\n",
      "Epoch 372/100001\n",
      "[C1 valid: -0.982048, C2 fake: 0.057311], [G loss: -0.007006, mse: 0.004584]\n",
      "Epoch 373/100001\n",
      "[C1 valid: -0.977138, C2 fake: 0.059010], [G loss: -0.006976, mse: 0.004852]\n",
      "Epoch 374/100001\n",
      "[C1 valid: -0.978524, C2 fake: 0.063275], [G loss: -0.006828, mse: 0.005047]\n",
      "Epoch 375/100001\n",
      "[C1 valid: -0.980273, C2 fake: 0.060055], [G loss: -0.006982, mse: 0.004774]\n",
      "Epoch 376/100001\n",
      "[C1 valid: -0.977822, C2 fake: 0.112641], [G loss: -0.002650, mse: 0.004640]\n",
      "Epoch 377/100001\n",
      "[C1 valid: -0.965073, C2 fake: 0.107872], [G loss: -0.006328, mse: 0.004717]\n",
      "Epoch 378/100001\n",
      "[C1 valid: -0.973696, C2 fake: 0.069053], [G loss: -0.006714, mse: 0.004814]\n",
      "Epoch 379/100001\n",
      "[C1 valid: -0.980963, C2 fake: 0.067192], [G loss: -0.006649, mse: 0.004849]\n",
      "Epoch 380/100001\n",
      "[C1 valid: -0.979091, C2 fake: 0.066546], [G loss: -0.006612, mse: 0.004870]\n",
      "Epoch 381/100001\n",
      "[C1 valid: -0.981532, C2 fake: 0.069190], [G loss: -0.006727, mse: 0.005168]\n",
      "Epoch 382/100001\n",
      "[C1 valid: -0.982305, C2 fake: 0.063744], [G loss: -0.006512, mse: 0.004630]\n",
      "Epoch 383/100001\n",
      "[C1 valid: -0.980365, C2 fake: 0.065773], [G loss: -0.006605, mse: 0.005084]\n",
      "Epoch 384/100001\n",
      "[C1 valid: -0.981172, C2 fake: 0.062602], [G loss: -0.006927, mse: 0.004566]\n",
      "Epoch 385/100001\n",
      "[C1 valid: -0.982788, C2 fake: 0.064827], [G loss: -0.006713, mse: 0.004795]\n",
      "Epoch 386/100001\n",
      "[C1 valid: -0.981865, C2 fake: 0.061862], [G loss: -0.006841, mse: 0.004745]\n",
      "Epoch 387/100001\n",
      "[C1 valid: -0.983671, C2 fake: 0.060525], [G loss: -0.006931, mse: 0.004419]\n",
      "Epoch 388/100001\n",
      "[C1 valid: -0.982917, C2 fake: 0.058137], [G loss: -0.006797, mse: 0.004496]\n",
      "Epoch 389/100001\n",
      "[C1 valid: -0.982551, C2 fake: 0.060698], [G loss: -0.006815, mse: 0.004656]\n",
      "Epoch 390/100001\n",
      "[C1 valid: -0.977646, C2 fake: 0.058641], [G loss: -0.006299, mse: 0.004724]\n",
      "Epoch 391/100001\n",
      "[C1 valid: -0.980602, C2 fake: 0.056740], [G loss: -0.006734, mse: 0.004608]\n",
      "Epoch 392/100001\n",
      "[C1 valid: -0.981185, C2 fake: 0.057054], [G loss: -0.006671, mse: 0.004588]\n",
      "Epoch 393/100001\n",
      "[C1 valid: -0.981307, C2 fake: 0.059228], [G loss: -0.006788, mse: 0.004626]\n",
      "Epoch 394/100001\n",
      "[C1 valid: -0.980116, C2 fake: 0.063995], [G loss: -0.006738, mse: 0.004500]\n",
      "Epoch 395/100001\n",
      "[C1 valid: -0.981139, C2 fake: 0.056127], [G loss: -0.006780, mse: 0.004607]\n",
      "Epoch 396/100001\n",
      "[C1 valid: -0.984253, C2 fake: 0.060262], [G loss: -0.006819, mse: 0.004600]\n",
      "Epoch 397/100001\n",
      "[C1 valid: -0.983111, C2 fake: 0.060414], [G loss: -0.006639, mse: 0.004366]\n",
      "Epoch 398/100001\n",
      "[C1 valid: -0.982606, C2 fake: 0.059649], [G loss: -0.006086, mse: 0.004716]\n",
      "Epoch 399/100001\n",
      "[C1 valid: -0.984385, C2 fake: 0.058323], [G loss: -0.006458, mse: 0.004592]\n",
      "Epoch 400/100001\n",
      "[C1 valid: -0.983036, C2 fake: 0.056945], [G loss: -0.006440, mse: 0.004670]\n",
      "Epoch 401/100001\n",
      "[C1 valid: -0.984407, C2 fake: 0.057007], [G loss: -0.006448, mse: 0.004460]\n",
      "Epoch 402/100001\n",
      "[C1 valid: -0.982692, C2 fake: 0.056865], [G loss: -0.006630, mse: 0.004329]\n",
      "Epoch 403/100001\n",
      "[C1 valid: -0.982191, C2 fake: 0.055042], [G loss: -0.006862, mse: 0.004617]\n",
      "Epoch 404/100001\n",
      "[C1 valid: -0.982770, C2 fake: 0.054618], [G loss: -0.007003, mse: 0.004474]\n",
      "Epoch 405/100001\n",
      "[C1 valid: -0.984732, C2 fake: 0.058385], [G loss: -0.006757, mse: 0.004528]\n",
      "Epoch 406/100001\n",
      "[C1 valid: -0.967610, C2 fake: 0.067466], [G loss: -0.006832, mse: 0.004437]\n",
      "Epoch 407/100001\n",
      "[C1 valid: -0.968949, C2 fake: 0.057041], [G loss: -0.007251, mse: 0.004549]\n",
      "Epoch 408/100001\n",
      "[C1 valid: -0.982687, C2 fake: 0.060055], [G loss: -0.007349, mse: 0.004454]\n",
      "Epoch 409/100001\n",
      "[C1 valid: -0.980135, C2 fake: 0.071143], [G loss: -0.004181, mse: 0.004398]\n",
      "Epoch 410/100001\n",
      "[C1 valid: -0.975939, C2 fake: 0.111534], [G loss: -0.007180, mse: 0.004197]\n",
      "Epoch 411/100001\n",
      "[C1 valid: -0.983650, C2 fake: 0.073855], [G loss: -0.007511, mse: 0.004307]\n",
      "Epoch 412/100001\n",
      "[C1 valid: -0.982529, C2 fake: 0.064117], [G loss: -0.007613, mse: 0.004303]\n",
      "Epoch 413/100001\n",
      "[C1 valid: -0.982203, C2 fake: 0.060304], [G loss: -0.007651, mse: 0.004130]\n",
      "Epoch 414/100001\n",
      "[C1 valid: -0.983682, C2 fake: 0.060389], [G loss: -0.007565, mse: 0.004138]\n",
      "Epoch 415/100001\n",
      "[C1 valid: -0.986017, C2 fake: 0.058681], [G loss: -0.007290, mse: 0.004507]\n",
      "Epoch 416/100001\n",
      "[C1 valid: -0.980853, C2 fake: 0.057682], [G loss: -0.007175, mse: 0.004361]\n",
      "Epoch 417/100001\n",
      "[C1 valid: -0.983878, C2 fake: 0.056740], [G loss: -0.007109, mse: 0.004454]\n",
      "Epoch 418/100001\n",
      "[C1 valid: -0.984476, C2 fake: 0.054214], [G loss: -0.007264, mse: 0.004293]\n",
      "Epoch 419/100001\n",
      "[C1 valid: -0.984203, C2 fake: 0.056469], [G loss: -0.007325, mse: 0.004219]\n",
      "Epoch 420/100001\n",
      "[C1 valid: -0.982855, C2 fake: 0.056784], [G loss: -0.007541, mse: 0.004040]\n",
      "Epoch 421/100001\n",
      "[C1 valid: -0.983567, C2 fake: 0.059101], [G loss: -0.007512, mse: 0.004169]\n",
      "Epoch 422/100001\n",
      "[C1 valid: -0.985916, C2 fake: 0.054397], [G loss: -0.007467, mse: 0.004222]\n",
      "Epoch 423/100001\n",
      "[C1 valid: -0.986136, C2 fake: 0.057462], [G loss: -0.007550, mse: 0.004113]\n",
      "Epoch 424/100001\n",
      "[C1 valid: -0.985010, C2 fake: 0.059978], [G loss: -0.007244, mse: 0.004383]\n",
      "Epoch 425/100001\n",
      "[C1 valid: -0.985705, C2 fake: 0.057140], [G loss: -0.007561, mse: 0.004091]\n",
      "Epoch 426/100001\n",
      "[C1 valid: -0.985241, C2 fake: 0.075310], [G loss: -0.005273, mse: 0.004142]\n",
      "Epoch 427/100001\n",
      "[C1 valid: -0.967894, C2 fake: 0.084005], [G loss: -0.006859, mse: 0.004237]\n",
      "Epoch 428/100001\n",
      "[C1 valid: -0.983071, C2 fake: 0.060624], [G loss: -0.006992, mse: 0.004053]\n",
      "Epoch 429/100001\n",
      "[C1 valid: -0.985312, C2 fake: 0.057870], [G loss: -0.006897, mse: 0.004120]\n",
      "Epoch 430/100001\n",
      "[C1 valid: -0.983219, C2 fake: 0.060089], [G loss: -0.007117, mse: 0.004133]\n",
      "Epoch 431/100001\n",
      "[C1 valid: -0.984886, C2 fake: 0.061569], [G loss: -0.007024, mse: 0.004339]\n",
      "Epoch 432/100001\n",
      "[C1 valid: -0.984365, C2 fake: 0.058809], [G loss: -0.006936, mse: 0.004377]\n",
      "Epoch 433/100001\n",
      "[C1 valid: -0.983023, C2 fake: 0.057857], [G loss: -0.007103, mse: 0.004185]\n",
      "Epoch 434/100001\n",
      "[C1 valid: -0.985590, C2 fake: 0.059616], [G loss: -0.007280, mse: 0.004154]\n",
      "Epoch 435/100001\n",
      "[C1 valid: -0.984396, C2 fake: 0.059758], [G loss: -0.007057, mse: 0.004117]\n",
      "Epoch 436/100001\n",
      "[C1 valid: -0.984476, C2 fake: 0.052678], [G loss: -0.007116, mse: 0.004146]\n",
      "Epoch 437/100001\n",
      "[C1 valid: -0.985475, C2 fake: 0.055467], [G loss: -0.007139, mse: 0.004382]\n",
      "Epoch 438/100001\n",
      "[C1 valid: -0.984927, C2 fake: 0.055339], [G loss: -0.006976, mse: 0.004228]\n",
      "Epoch 439/100001\n",
      "[C1 valid: -0.984670, C2 fake: 0.053967], [G loss: -0.007314, mse: 0.004134]\n",
      "Epoch 440/100001\n",
      "[C1 valid: -0.982379, C2 fake: 0.053681], [G loss: -0.007148, mse: 0.004241]\n",
      "Epoch 441/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.985944, C2 fake: 0.052660], [G loss: -0.007225, mse: 0.004024]\n",
      "Epoch 442/100001\n",
      "[C1 valid: -0.985691, C2 fake: 0.052846], [G loss: -0.007037, mse: 0.004158]\n",
      "Epoch 443/100001\n",
      "[C1 valid: -0.985828, C2 fake: 0.057617], [G loss: -0.006888, mse: 0.004096]\n",
      "Epoch 444/100001\n",
      "[C1 valid: -0.981924, C2 fake: 0.054087], [G loss: -0.007173, mse: 0.003938]\n",
      "Epoch 445/100001\n",
      "[C1 valid: -0.984310, C2 fake: 0.052826], [G loss: -0.007207, mse: 0.004059]\n",
      "Epoch 446/100001\n",
      "[C1 valid: -0.984575, C2 fake: 0.054114], [G loss: -0.007000, mse: 0.004117]\n",
      "Epoch 447/100001\n",
      "[C1 valid: -0.984902, C2 fake: 0.052853], [G loss: -0.006846, mse: 0.004077]\n",
      "Epoch 448/100001\n",
      "[C1 valid: -0.986127, C2 fake: 0.049094], [G loss: -0.006484, mse: 0.003935]\n",
      "Epoch 449/100001\n",
      "[C1 valid: -0.984825, C2 fake: 0.064130], [G loss: -0.006770, mse: 0.004154]\n",
      "Epoch 450/100001\n",
      "[C1 valid: -0.983897, C2 fake: 0.055678], [G loss: -0.006754, mse: 0.003984]\n",
      "Epoch 451/100001\n",
      "[C1 valid: -0.982792, C2 fake: 0.051223], [G loss: -0.006963, mse: 0.003934]\n",
      "Epoch 452/100001\n",
      "[C1 valid: -0.982960, C2 fake: 0.052356], [G loss: -0.006138, mse: 0.004034]\n",
      "Epoch 453/100001\n",
      "[C1 valid: -0.982913, C2 fake: 0.051596], [G loss: -0.006469, mse: 0.003967]\n",
      "Epoch 454/100001\n",
      "[C1 valid: -0.985476, C2 fake: 0.050018], [G loss: -0.006884, mse: 0.003788]\n",
      "Epoch 455/100001\n",
      "[C1 valid: -0.984925, C2 fake: 0.051215], [G loss: -0.006636, mse: 0.004099]\n",
      "Epoch 456/100001\n",
      "[C1 valid: -0.986218, C2 fake: 0.048670], [G loss: -0.006812, mse: 0.004135]\n",
      "Epoch 457/100001\n",
      "[C1 valid: -0.985671, C2 fake: 0.048320], [G loss: -0.006892, mse: 0.003987]\n",
      "Epoch 458/100001\n",
      "[C1 valid: -0.983065, C2 fake: 0.047889], [G loss: -0.007002, mse: 0.004150]\n",
      "Epoch 459/100001\n",
      "[C1 valid: -0.986915, C2 fake: 0.049063], [G loss: -0.007394, mse: 0.003803]\n",
      "Epoch 460/100001\n",
      "[C1 valid: -0.985732, C2 fake: 0.052552], [G loss: -0.007116, mse: 0.003733]\n",
      "Epoch 461/100001\n",
      "[C1 valid: -0.985180, C2 fake: 0.050395], [G loss: -0.007591, mse: 0.003954]\n",
      "Epoch 462/100001\n",
      "[C1 valid: -0.986252, C2 fake: 0.050065], [G loss: -0.007737, mse: 0.003812]\n",
      "Epoch 463/100001\n",
      "[C1 valid: -0.985388, C2 fake: 0.052018], [G loss: -0.006931, mse: 0.003847]\n",
      "Epoch 464/100001\n",
      "[C1 valid: -0.984034, C2 fake: 0.063159], [G loss: -0.007295, mse: 0.003936]\n",
      "Epoch 465/100001\n",
      "[C1 valid: -0.986362, C2 fake: 0.048743], [G loss: -0.007437, mse: 0.003785]\n",
      "Epoch 466/100001\n",
      "[C1 valid: -0.986129, C2 fake: 0.050526], [G loss: -0.007012, mse: 0.003817]\n",
      "Epoch 467/100001\n",
      "[C1 valid: -0.983425, C2 fake: 0.050899], [G loss: -0.006249, mse: 0.004183]\n",
      "Epoch 468/100001\n",
      "[C1 valid: -0.985588, C2 fake: 0.050541], [G loss: -0.007157, mse: 0.003829]\n",
      "Epoch 469/100001\n",
      "[C1 valid: -0.986572, C2 fake: 0.049439], [G loss: -0.007381, mse: 0.003838]\n",
      "Epoch 470/100001\n",
      "[C1 valid: -0.986392, C2 fake: 0.051263], [G loss: -0.007420, mse: 0.003773]\n",
      "Epoch 471/100001\n",
      "[C1 valid: -0.986662, C2 fake: 0.049749], [G loss: -0.007544, mse: 0.003827]\n",
      "Epoch 472/100001\n",
      "[C1 valid: -0.984875, C2 fake: 0.047513], [G loss: -0.007390, mse: 0.004107]\n",
      "Epoch 473/100001\n",
      "[C1 valid: -0.983788, C2 fake: 0.048864], [G loss: -0.007645, mse: 0.003900]\n",
      "Epoch 474/100001\n",
      "[C1 valid: -0.985266, C2 fake: 0.051665], [G loss: -0.007871, mse: 0.003499]\n",
      "Epoch 475/100001\n",
      "[C1 valid: -0.984109, C2 fake: 0.045770], [G loss: -0.007494, mse: 0.004030]\n",
      "Epoch 476/100001\n",
      "[C1 valid: -0.984427, C2 fake: 0.048111], [G loss: -0.007593, mse: 0.003778]\n",
      "Epoch 477/100001\n",
      "[C1 valid: -0.986088, C2 fake: 0.049507], [G loss: -0.007631, mse: 0.003734]\n",
      "Epoch 478/100001\n",
      "[C1 valid: -0.981500, C2 fake: 0.046614], [G loss: -0.007710, mse: 0.003702]\n",
      "Epoch 479/100001\n",
      "[C1 valid: -0.988044, C2 fake: 0.047645], [G loss: -0.007490, mse: 0.003741]\n",
      "Epoch 480/100001\n",
      "[C1 valid: -0.971187, C2 fake: 0.112233], [G loss: -0.005847, mse: 0.003589]\n",
      "Epoch 481/100001\n",
      "[C1 valid: -0.979294, C2 fake: 0.088507], [G loss: -0.006725, mse: 0.004044]\n",
      "Epoch 482/100001\n",
      "[C1 valid: -0.984412, C2 fake: 0.085610], [G loss: -0.007501, mse: 0.003908]\n",
      "Epoch 483/100001\n",
      "[C1 valid: -0.982811, C2 fake: 0.074176], [G loss: -0.007691, mse: 0.003732]\n",
      "Epoch 484/100001\n",
      "[C1 valid: -0.986445, C2 fake: 0.066627], [G loss: -0.007648, mse: 0.003806]\n",
      "Epoch 485/100001\n",
      "[C1 valid: -0.985425, C2 fake: 0.066729], [G loss: -0.007553, mse: 0.003891]\n",
      "Epoch 486/100001\n",
      "[C1 valid: -0.985757, C2 fake: 0.058740], [G loss: -0.007685, mse: 0.003786]\n",
      "Epoch 487/100001\n",
      "[C1 valid: -0.984218, C2 fake: 0.056851], [G loss: -0.007707, mse: 0.003703]\n",
      "Epoch 488/100001\n",
      "[C1 valid: -0.985131, C2 fake: 0.054222], [G loss: -0.007623, mse: 0.003685]\n",
      "Epoch 489/100001\n",
      "[C1 valid: -0.985737, C2 fake: 0.053206], [G loss: -0.007618, mse: 0.003715]\n",
      "Epoch 490/100001\n",
      "[C1 valid: -0.985409, C2 fake: 0.051852], [G loss: -0.007311, mse: 0.003922]\n",
      "Epoch 491/100001\n",
      "[C1 valid: -0.988420, C2 fake: 0.050298], [G loss: -0.007387, mse: 0.003677]\n",
      "Epoch 492/100001\n",
      "[C1 valid: -0.988309, C2 fake: 0.051016], [G loss: -0.007155, mse: 0.003691]\n",
      "Epoch 493/100001\n",
      "[C1 valid: -0.987382, C2 fake: 0.049407], [G loss: -0.007313, mse: 0.003515]\n",
      "Epoch 494/100001\n",
      "[C1 valid: -0.986597, C2 fake: 0.048984], [G loss: -0.007280, mse: 0.003727]\n",
      "Epoch 495/100001\n",
      "[C1 valid: -0.984963, C2 fake: 0.051148], [G loss: -0.007175, mse: 0.003598]\n",
      "Epoch 496/100001\n",
      "[C1 valid: -0.988127, C2 fake: 0.051093], [G loss: -0.007384, mse: 0.003517]\n",
      "Epoch 497/100001\n",
      "[C1 valid: -0.986190, C2 fake: 0.049554], [G loss: -0.007481, mse: 0.003688]\n",
      "Epoch 498/100001\n",
      "[C1 valid: -0.987986, C2 fake: 0.048678], [G loss: -0.007337, mse: 0.003739]\n",
      "Epoch 499/100001\n",
      "[C1 valid: -0.987819, C2 fake: 0.045396], [G loss: -0.007489, mse: 0.003638]\n",
      "Epoch 500/100001\n",
      "[C1 valid: -0.988466, C2 fake: 0.048839], [G loss: -0.007626, mse: 0.003639]\n",
      "Epoch 501/100001\n",
      "[C1 valid: -0.987784, C2 fake: 0.048571], [G loss: -0.007640, mse: 0.003424]\n",
      "Epoch 502/100001\n",
      "[C1 valid: -0.989916, C2 fake: 0.047844], [G loss: -0.007739, mse: 0.003567]\n",
      "Epoch 503/100001\n",
      "[C1 valid: -0.989230, C2 fake: 0.049728], [G loss: -0.007679, mse: 0.003745]\n",
      "Epoch 504/100001\n",
      "[C1 valid: -0.990718, C2 fake: 0.047984], [G loss: -0.007660, mse: 0.003499]\n",
      "Epoch 505/100001\n",
      "[C1 valid: -0.989235, C2 fake: 0.049129], [G loss: -0.007155, mse: 0.003528]\n",
      "Epoch 506/100001\n",
      "[C1 valid: -0.958572, C2 fake: 0.091921], [G loss: -0.007656, mse: 0.003332]\n",
      "Epoch 507/100001\n",
      "[C1 valid: -0.981995, C2 fake: 0.061468], [G loss: -0.007967, mse: 0.003371]\n",
      "Epoch 508/100001\n",
      "[C1 valid: -0.986160, C2 fake: 0.057608], [G loss: -0.007989, mse: 0.003530]\n",
      "Epoch 509/100001\n",
      "[C1 valid: -0.988592, C2 fake: 0.060087], [G loss: -0.007919, mse: 0.003560]\n",
      "Epoch 510/100001\n",
      "[C1 valid: -0.988801, C2 fake: 0.052921], [G loss: -0.007966, mse: 0.003623]\n",
      "Epoch 511/100001\n",
      "[C1 valid: -0.990544, C2 fake: 0.052035], [G loss: -0.008083, mse: 0.003438]\n",
      "Epoch 512/100001\n",
      "[C1 valid: -0.989091, C2 fake: 0.052073], [G loss: -0.007861, mse: 0.003806]\n",
      "Epoch 513/100001\n",
      "[C1 valid: -0.984224, C2 fake: 0.049827], [G loss: -0.008088, mse: 0.003608]\n",
      "Epoch 514/100001\n",
      "[C1 valid: -0.990292, C2 fake: 0.061174], [G loss: -0.007724, mse: 0.003354]\n",
      "Epoch 515/100001\n",
      "[C1 valid: -0.988259, C2 fake: 0.060386], [G loss: -0.008023, mse: 0.003381]\n",
      "Epoch 516/100001\n",
      "[C1 valid: -0.987554, C2 fake: 0.056760], [G loss: -0.008030, mse: 0.003414]\n",
      "Epoch 517/100001\n",
      "[C1 valid: -0.989432, C2 fake: 0.054766], [G loss: -0.007938, mse: 0.003489]\n",
      "Epoch 518/100001\n",
      "[C1 valid: -0.989981, C2 fake: 0.054983], [G loss: -0.007962, mse: 0.003416]\n",
      "Epoch 519/100001\n",
      "[C1 valid: -0.990457, C2 fake: 0.053376], [G loss: -0.007930, mse: 0.003399]\n",
      "Epoch 520/100001\n",
      "[C1 valid: -0.989518, C2 fake: 0.053410], [G loss: -0.007923, mse: 0.003338]\n",
      "Epoch 521/100001\n",
      "[C1 valid: -0.989718, C2 fake: 0.050687], [G loss: -0.007863, mse: 0.003449]\n",
      "Epoch 522/100001\n",
      "[C1 valid: -0.988522, C2 fake: 0.048799], [G loss: -0.007776, mse: 0.003644]\n",
      "Epoch 523/100001\n",
      "[C1 valid: -0.988252, C2 fake: 0.052537], [G loss: -0.007679, mse: 0.003375]\n",
      "Epoch 524/100001\n",
      "[C1 valid: -0.990885, C2 fake: 0.050249], [G loss: -0.007798, mse: 0.003283]\n",
      "Epoch 525/100001\n",
      "[C1 valid: -0.989762, C2 fake: 0.051197], [G loss: -0.007624, mse: 0.003475]\n",
      "Epoch 526/100001\n",
      "[C1 valid: -0.989089, C2 fake: 0.048581], [G loss: -0.007556, mse: 0.003352]\n",
      "Epoch 527/100001\n",
      "[C1 valid: -0.989700, C2 fake: 0.049787], [G loss: -0.007017, mse: 0.003525]\n",
      "Epoch 528/100001\n",
      "[C1 valid: -0.988744, C2 fake: 0.052470], [G loss: -0.007054, mse: 0.003385]\n",
      "Epoch 529/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.991242, C2 fake: 0.047417], [G loss: -0.007509, mse: 0.003423]\n",
      "Epoch 530/100001\n",
      "[C1 valid: -0.987263, C2 fake: 0.046822], [G loss: -0.007787, mse: 0.003515]\n",
      "Epoch 531/100001\n",
      "[C1 valid: -0.990626, C2 fake: 0.049057], [G loss: -0.008042, mse: 0.003292]\n",
      "Epoch 532/100001\n",
      "[C1 valid: -0.990512, C2 fake: 0.049341], [G loss: -0.008001, mse: 0.003295]\n",
      "Epoch 533/100001\n",
      "[C1 valid: -0.991034, C2 fake: 0.045658], [G loss: -0.008106, mse: 0.003231]\n",
      "Epoch 534/100001\n",
      "[C1 valid: -0.990784, C2 fake: 0.049449], [G loss: -0.008058, mse: 0.003275]\n",
      "Epoch 535/100001\n",
      "[C1 valid: -0.988905, C2 fake: 0.050352], [G loss: -0.008064, mse: 0.003494]\n",
      "Epoch 536/100001\n",
      "[C1 valid: -0.990281, C2 fake: 0.045070], [G loss: -0.008084, mse: 0.003277]\n",
      "Epoch 537/100001\n",
      "[C1 valid: -0.990633, C2 fake: 0.046888], [G loss: -0.007928, mse: 0.003344]\n",
      "Epoch 538/100001\n",
      "[C1 valid: -0.990769, C2 fake: 0.053819], [G loss: -0.007716, mse: 0.003257]\n",
      "Epoch 539/100001\n",
      "[C1 valid: -0.991917, C2 fake: 0.045523], [G loss: -0.007440, mse: 0.003365]\n",
      "Epoch 540/100001\n",
      "[C1 valid: -0.990067, C2 fake: 0.047063], [G loss: -0.007313, mse: 0.003392]\n",
      "Epoch 541/100001\n",
      "[C1 valid: -0.990448, C2 fake: 0.046001], [G loss: -0.007543, mse: 0.003474]\n",
      "Epoch 542/100001\n",
      "[C1 valid: -0.990113, C2 fake: 0.045513], [G loss: -0.007178, mse: 0.003214]\n",
      "Epoch 543/100001\n",
      "[C1 valid: -0.989531, C2 fake: 0.044651], [G loss: -0.007232, mse: 0.003355]\n",
      "Epoch 544/100001\n",
      "[C1 valid: -0.992149, C2 fake: 0.043859], [G loss: -0.007794, mse: 0.003243]\n",
      "Epoch 545/100001\n",
      "[C1 valid: -0.990291, C2 fake: 0.049879], [G loss: -0.007190, mse: 0.003244]\n",
      "Epoch 546/100001\n",
      "[C1 valid: -0.979093, C2 fake: 0.052089], [G loss: -0.007521, mse: 0.003287]\n",
      "Epoch 547/100001\n",
      "[C1 valid: -0.988837, C2 fake: 0.046442], [G loss: -0.007128, mse: 0.003208]\n",
      "Epoch 548/100001\n",
      "[C1 valid: -0.991086, C2 fake: 0.049883], [G loss: -0.007747, mse: 0.003343]\n",
      "Epoch 549/100001\n",
      "[C1 valid: -0.989810, C2 fake: 0.046104], [G loss: -0.007929, mse: 0.003057]\n",
      "Epoch 550/100001\n",
      "[C1 valid: -0.992228, C2 fake: 0.048445], [G loss: -0.007941, mse: 0.003288]\n",
      "Epoch 551/100001\n",
      "[C1 valid: -0.990433, C2 fake: 0.046488], [G loss: -0.007814, mse: 0.003430]\n",
      "Epoch 552/100001\n",
      "[C1 valid: -0.991738, C2 fake: 0.046186], [G loss: -0.007641, mse: 0.003231]\n",
      "Epoch 553/100001\n",
      "[C1 valid: -0.990244, C2 fake: 0.044523], [G loss: -0.007812, mse: 0.003134]\n",
      "Epoch 554/100001\n",
      "[C1 valid: -0.990915, C2 fake: 0.042806], [G loss: -0.007767, mse: 0.003279]\n",
      "Epoch 555/100001\n",
      "[C1 valid: -0.990981, C2 fake: 0.044521], [G loss: -0.007803, mse: 0.003246]\n",
      "Epoch 556/100001\n",
      "[C1 valid: -0.989494, C2 fake: 0.044692], [G loss: -0.008034, mse: 0.003022]\n",
      "Epoch 557/100001\n",
      "[C1 valid: -0.991683, C2 fake: 0.043307], [G loss: -0.007900, mse: 0.003125]\n",
      "Epoch 558/100001\n",
      "[C1 valid: -0.991635, C2 fake: 0.049837], [G loss: -0.007022, mse: 0.003335]\n",
      "Epoch 559/100001\n",
      "[C1 valid: -0.979761, C2 fake: 0.113102], [G loss: -0.006243, mse: 0.003336]\n",
      "Epoch 560/100001\n",
      "[C1 valid: -0.989406, C2 fake: 0.051113], [G loss: -0.007230, mse: 0.003290]\n",
      "Epoch 561/100001\n",
      "[C1 valid: -0.988339, C2 fake: 0.054093], [G loss: -0.007465, mse: 0.003126]\n",
      "Epoch 562/100001\n",
      "[C1 valid: -0.989786, C2 fake: 0.051345], [G loss: -0.007418, mse: 0.003060]\n",
      "Epoch 563/100001\n",
      "[C1 valid: -0.989830, C2 fake: 0.048459], [G loss: -0.007375, mse: 0.003018]\n",
      "Epoch 564/100001\n",
      "[C1 valid: -0.990480, C2 fake: 0.047716], [G loss: -0.007334, mse: 0.003137]\n",
      "Epoch 565/100001\n",
      "[C1 valid: -0.992035, C2 fake: 0.048316], [G loss: -0.007433, mse: 0.003309]\n",
      "Epoch 566/100001\n",
      "[C1 valid: -0.990856, C2 fake: 0.044782], [G loss: -0.007389, mse: 0.003217]\n",
      "Epoch 567/100001\n",
      "[C1 valid: -0.992185, C2 fake: 0.044618], [G loss: -0.007320, mse: 0.003086]\n",
      "Epoch 568/100001\n",
      "[C1 valid: -0.989395, C2 fake: 0.043283], [G loss: -0.007385, mse: 0.003306]\n",
      "Epoch 569/100001\n",
      "[C1 valid: -0.991457, C2 fake: 0.045426], [G loss: -0.007763, mse: 0.003059]\n",
      "Epoch 570/100001\n",
      "[C1 valid: -0.991324, C2 fake: 0.039446], [G loss: -0.007450, mse: 0.003296]\n",
      "Epoch 571/100001\n",
      "[C1 valid: -0.987238, C2 fake: 0.042689], [G loss: -0.006990, mse: 0.003193]\n",
      "Epoch 572/100001\n",
      "[C1 valid: -0.991270, C2 fake: 0.040272], [G loss: -0.007421, mse: 0.003197]\n",
      "Epoch 573/100001\n",
      "[C1 valid: -0.988405, C2 fake: 0.041002], [G loss: -0.006359, mse: 0.003156]\n",
      "Epoch 574/100001\n",
      "[C1 valid: -0.990602, C2 fake: 0.044012], [G loss: -0.004975, mse: 0.003080]\n",
      "Epoch 575/100001\n",
      "[C1 valid: -0.989123, C2 fake: 0.058883], [G loss: -0.005549, mse: 0.003316]\n",
      "Epoch 576/100001\n",
      "[C1 valid: -0.988700, C2 fake: 0.044481], [G loss: -0.007361, mse: 0.003050]\n",
      "Epoch 577/100001\n",
      "[C1 valid: -0.989223, C2 fake: 0.044894], [G loss: -0.006423, mse: 0.003193]\n",
      "Epoch 578/100001\n",
      "[C1 valid: -0.990335, C2 fake: 0.040092], [G loss: -0.005046, mse: 0.003040]\n",
      "Epoch 579/100001\n",
      "[C1 valid: -0.990124, C2 fake: 0.043032], [G loss: -0.002708, mse: 0.003402]\n",
      "Epoch 580/100001\n",
      "[C1 valid: -0.990291, C2 fake: 0.043567], [G loss: -0.003210, mse: 0.003179]\n",
      "Epoch 581/100001\n",
      "[C1 valid: -0.987150, C2 fake: 0.042450], [G loss: -0.004165, mse: 0.003093]\n",
      "Epoch 582/100001\n",
      "[C1 valid: -0.987957, C2 fake: 0.037011], [G loss: -0.004765, mse: 0.003121]\n",
      "Epoch 583/100001\n",
      "[C1 valid: -0.988920, C2 fake: 0.039382], [G loss: -0.005521, mse: 0.003131]\n",
      "Epoch 584/100001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder.predict(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-3,-1,1,3]\n",
    "n_points = 400\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//4, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//4, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//4, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//4, n_features), x_input[3])\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder.predict(aae.encoder(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
