{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "export_excel(X_train, 'Dataset/X_train')\n",
    "export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu (ELU)                       (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         elu[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "elu_1 (ELU)                     (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 952\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Decoder:\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,242\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Discriminator:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "elu_4 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 80)                3280      \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 5,741\n",
      "Trainable params: 5,381\n",
      "Non-trainable params: 360\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network14.build_encoder(Z, nodes, n_features)\n",
    "print(\"Encoder:\\n\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network14.build_decoder(Z,nodes, n_features)\n",
    "print(\"Decoder:\\n\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator=network14.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model14\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 10001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model14.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10001\n",
      "[C1 valid: -0.502734, C2 fake: 0.506877], [G loss: 0.792295, mse: 0.885842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE0506/AAE_Model13.py:190: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/13encoder_40_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/13decoder_40_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/13discriminator_40_10001/assets\n",
      "Epoch 2/10001\n",
      "[C1 valid: -0.502518, C2 fake: 0.505304], [G loss: 0.744819, mse: 0.833191]\n",
      "Epoch 3/10001\n",
      "[C1 valid: -0.503577, C2 fake: 0.506323], [G loss: 0.721468, mse: 0.807230]\n",
      "Epoch 4/10001\n",
      "[C1 valid: -0.501825, C2 fake: 0.504368], [G loss: 0.693924, mse: 0.776625]\n",
      "Epoch 5/10001\n",
      "[C1 valid: -0.503381, C2 fake: 0.505647], [G loss: 0.621247, mse: 0.695778]\n",
      "Epoch 6/10001\n",
      "[C1 valid: -0.503175, C2 fake: 0.505239], [G loss: 0.577821, mse: 0.647520]\n",
      "Epoch 7/10001\n",
      "[C1 valid: -0.501668, C2 fake: 0.503867], [G loss: 0.515997, mse: 0.578817]\n",
      "Epoch 8/10001\n",
      "[C1 valid: -0.503505, C2 fake: 0.505641], [G loss: 0.469381, mse: 0.527096]\n",
      "Epoch 9/10001\n",
      "[C1 valid: -0.503163, C2 fake: 0.504541], [G loss: 0.461106, mse: 0.517768]\n",
      "Epoch 10/10001\n",
      "[C1 valid: -0.500572, C2 fake: 0.504452], [G loss: 0.402108, mse: 0.452282]\n",
      "Epoch 11/10001\n",
      "[C1 valid: -0.500431, C2 fake: 0.503208], [G loss: 0.365978, mse: 0.412079]\n",
      "Epoch 12/10001\n",
      "[C1 valid: -0.502168, C2 fake: 0.505556], [G loss: 0.338064, mse: 0.381134]\n",
      "Epoch 13/10001\n",
      "[C1 valid: -0.500596, C2 fake: 0.501600], [G loss: 0.300368, mse: 0.339185]\n",
      "Epoch 14/10001\n",
      "[C1 valid: -0.502459, C2 fake: 0.507155], [G loss: 0.279719, mse: 0.316296]\n",
      "Epoch 15/10001\n",
      "[C1 valid: -0.500619, C2 fake: 0.504367], [G loss: 0.264142, mse: 0.298949]\n",
      "Epoch 16/10001\n",
      "[C1 valid: -0.500338, C2 fake: 0.504458], [G loss: 0.254431, mse: 0.288183]\n",
      "Epoch 17/10001\n",
      "[C1 valid: -0.502440, C2 fake: 0.502588], [G loss: 0.227465, mse: 0.258310]\n",
      "Epoch 18/10001\n",
      "[C1 valid: -0.499797, C2 fake: 0.501584], [G loss: 0.202065, mse: 0.229930]\n",
      "Epoch 19/10001\n",
      "[C1 valid: -0.503034, C2 fake: 0.504115], [G loss: 0.197039, mse: 0.224508]\n",
      "Epoch 20/10001\n",
      "[C1 valid: -0.501405, C2 fake: 0.504390], [G loss: 0.174165, mse: 0.199115]\n",
      "Epoch 21/10001\n",
      "[C1 valid: -0.503884, C2 fake: 0.501842], [G loss: 0.185029, mse: 0.211158]\n",
      "Epoch 22/10001\n",
      "[C1 valid: -0.500344, C2 fake: 0.504802], [G loss: 0.161904, mse: 0.185363]\n",
      "Epoch 23/10001\n",
      "[C1 valid: -0.500386, C2 fake: 0.503082], [G loss: 0.157407, mse: 0.180423]\n",
      "Epoch 24/10001\n",
      "[C1 valid: -0.500381, C2 fake: 0.504052], [G loss: 0.143799, mse: 0.165239]\n",
      "Epoch 25/10001\n",
      "[C1 valid: -0.500717, C2 fake: 0.503339], [G loss: 0.141859, mse: 0.162996]\n",
      "Epoch 26/10001\n",
      "[C1 valid: -0.500161, C2 fake: 0.502971], [G loss: 0.135255, mse: 0.155541]\n",
      "Epoch 27/10001\n",
      "[C1 valid: -0.501785, C2 fake: 0.503373], [G loss: 0.131881, mse: 0.151875]\n",
      "Epoch 28/10001\n",
      "[C1 valid: -0.501587, C2 fake: 0.502732], [G loss: 0.127023, mse: 0.146605]\n",
      "Epoch 29/10001\n",
      "[C1 valid: -0.501559, C2 fake: 0.503058], [G loss: 0.120729, mse: 0.139583]\n",
      "Epoch 30/10001\n",
      "[C1 valid: -0.501333, C2 fake: 0.500954], [G loss: 0.121016, mse: 0.139972]\n",
      "Epoch 31/10001\n",
      "[C1 valid: -0.500072, C2 fake: 0.500144], [G loss: 0.120687, mse: 0.139596]\n",
      "Epoch 32/10001\n",
      "[C1 valid: -0.502925, C2 fake: 0.501735], [G loss: 0.108348, mse: 0.126041]\n",
      "Epoch 33/10001\n",
      "[C1 valid: -0.503932, C2 fake: 0.500151], [G loss: 0.108946, mse: 0.126461]\n",
      "Epoch 34/10001\n",
      "[C1 valid: -0.499615, C2 fake: 0.502226], [G loss: 0.106635, mse: 0.123982]\n",
      "Epoch 35/10001\n",
      "[C1 valid: -0.501035, C2 fake: 0.501880], [G loss: 0.099002, mse: 0.115488]\n",
      "Epoch 36/10001\n",
      "[C1 valid: -0.500237, C2 fake: 0.500425], [G loss: 0.094644, mse: 0.110613]\n",
      "Epoch 37/10001\n",
      "[C1 valid: -0.504111, C2 fake: 0.500222], [G loss: 0.096351, mse: 0.112447]\n",
      "Epoch 38/10001\n",
      "[C1 valid: -0.498374, C2 fake: 0.502330], [G loss: 0.096730, mse: 0.112962]\n",
      "Epoch 39/10001\n",
      "[C1 valid: -0.501712, C2 fake: 0.499479], [G loss: 0.094812, mse: 0.110808]\n",
      "Epoch 40/10001\n",
      "[C1 valid: -0.502612, C2 fake: 0.502162], [G loss: 0.091336, mse: 0.106947]\n",
      "Epoch 41/10001\n",
      "[C1 valid: -0.505498, C2 fake: 0.499107], [G loss: 0.088191, mse: 0.103454]\n",
      "Epoch 42/10001\n",
      "[C1 valid: -0.504185, C2 fake: 0.499753], [G loss: 0.092811, mse: 0.108688]\n",
      "Epoch 43/10001\n",
      "[C1 valid: -0.501702, C2 fake: 0.500669], [G loss: 0.083468, mse: 0.098146]\n",
      "Epoch 44/10001\n",
      "[C1 valid: -0.505401, C2 fake: 0.504600], [G loss: 0.085395, mse: 0.100380]\n",
      "Epoch 45/10001\n",
      "[C1 valid: -0.501176, C2 fake: 0.498764], [G loss: 0.078703, mse: 0.092918]\n",
      "Epoch 46/10001\n",
      "[C1 valid: -0.503001, C2 fake: 0.499600], [G loss: 0.079527, mse: 0.093898]\n",
      "Epoch 47/10001\n",
      "[C1 valid: -0.501006, C2 fake: 0.501328], [G loss: 0.077703, mse: 0.091822]\n",
      "Epoch 48/10001\n",
      "[C1 valid: -0.500067, C2 fake: 0.493871], [G loss: 0.076781, mse: 0.090857]\n",
      "Epoch 49/10001\n",
      "[C1 valid: -0.504685, C2 fake: 0.493084], [G loss: 0.077354, mse: 0.091354]\n",
      "Epoch 50/10001\n",
      "[C1 valid: -0.500284, C2 fake: 0.488992], [G loss: 0.072235, mse: 0.085317]\n",
      "Epoch 51/10001\n",
      "[C1 valid: -0.500114, C2 fake: 0.495585], [G loss: 0.070674, mse: 0.083905]\n",
      "Epoch 52/10001\n",
      "[C1 valid: -0.497032, C2 fake: 0.497371], [G loss: 0.075836, mse: 0.089472]\n",
      "Epoch 53/10001\n",
      "[C1 valid: -0.499407, C2 fake: 0.495405], [G loss: 0.071326, mse: 0.084470]\n",
      "Epoch 54/10001\n",
      "[C1 valid: -0.496330, C2 fake: 0.496365], [G loss: 0.072271, mse: 0.085480]\n",
      "Epoch 55/10001\n",
      "[C1 valid: -0.498392, C2 fake: 0.493884], [G loss: 0.067258, mse: 0.080187]\n",
      "Epoch 56/10001\n",
      "[C1 valid: -0.494736, C2 fake: 0.493156], [G loss: 0.067278, mse: 0.080002]\n",
      "Epoch 57/10001\n",
      "[C1 valid: -0.501701, C2 fake: 0.489867], [G loss: 0.062062, mse: 0.074293]\n",
      "Epoch 58/10001\n",
      "[C1 valid: -0.494513, C2 fake: 0.486437], [G loss: 0.064871, mse: 0.077438]\n",
      "Epoch 59/10001\n",
      "[C1 valid: -0.504490, C2 fake: 0.490968], [G loss: 0.062617, mse: 0.074925]\n",
      "Epoch 60/10001\n",
      "[C1 valid: -0.494104, C2 fake: 0.488637], [G loss: 0.058219, mse: 0.069996]\n",
      "Epoch 61/10001\n",
      "[C1 valid: -0.496049, C2 fake: 0.483485], [G loss: 0.062503, mse: 0.074842]\n",
      "Epoch 62/10001\n",
      "[C1 valid: -0.496921, C2 fake: 0.487447], [G loss: 0.063820, mse: 0.076255]\n",
      "Epoch 63/10001\n",
      "[C1 valid: -0.494596, C2 fake: 0.481027], [G loss: 0.060249, mse: 0.072283]\n",
      "Epoch 64/10001\n",
      "[C1 valid: -0.501967, C2 fake: 0.481931], [G loss: 0.054472, mse: 0.065972]\n",
      "Epoch 65/10001\n",
      "[C1 valid: -0.496058, C2 fake: 0.478893], [G loss: 0.057314, mse: 0.069168]\n",
      "Epoch 66/10001\n",
      "[C1 valid: -0.499313, C2 fake: 0.481164], [G loss: 0.059034, mse: 0.070947]\n",
      "Epoch 67/10001\n",
      "[C1 valid: -0.495971, C2 fake: 0.476272], [G loss: 0.052052, mse: 0.063200]\n",
      "Epoch 68/10001\n",
      "[C1 valid: -0.488281, C2 fake: 0.469917], [G loss: 0.051938, mse: 0.063060]\n",
      "Epoch 69/10001\n",
      "[C1 valid: -0.500186, C2 fake: 0.474204], [G loss: 0.053081, mse: 0.064461]\n",
      "Epoch 70/10001\n",
      "[C1 valid: -0.491633, C2 fake: 0.467942], [G loss: 0.049231, mse: 0.060101]\n",
      "Epoch 71/10001\n",
      "[C1 valid: -0.489743, C2 fake: 0.467358], [G loss: 0.058110, mse: 0.070124]\n",
      "Epoch 72/10001\n",
      "[C1 valid: -0.488529, C2 fake: 0.472769], [G loss: 0.051088, mse: 0.062070]\n",
      "Epoch 73/10001\n",
      "[C1 valid: -0.501544, C2 fake: 0.466654], [G loss: 0.051183, mse: 0.062127]\n",
      "Epoch 74/10001\n",
      "[C1 valid: -0.492862, C2 fake: 0.466506], [G loss: 0.052311, mse: 0.063385]\n",
      "Epoch 75/10001\n",
      "[C1 valid: -0.495918, C2 fake: 0.460551], [G loss: 0.050751, mse: 0.061864]\n",
      "Epoch 76/10001\n",
      "[C1 valid: -0.487362, C2 fake: 0.466707], [G loss: 0.046698, mse: 0.057088]\n",
      "Epoch 77/10001\n",
      "[C1 valid: -0.489187, C2 fake: 0.471216], [G loss: 0.046466, mse: 0.056851]\n",
      "Epoch 78/10001\n",
      "[C1 valid: -0.487146, C2 fake: 0.473908], [G loss: 0.046603, mse: 0.057359]\n",
      "Epoch 79/10001\n",
      "[C1 valid: -0.493835, C2 fake: 0.462889], [G loss: 0.048994, mse: 0.059790]\n",
      "Epoch 80/10001\n",
      "[C1 valid: -0.496228, C2 fake: 0.462017], [G loss: 0.044414, mse: 0.055000]\n",
      "Epoch 81/10001\n",
      "[C1 valid: -0.490851, C2 fake: 0.457230], [G loss: 0.046424, mse: 0.057117]\n",
      "Epoch 82/10001\n",
      "[C1 valid: -0.492162, C2 fake: 0.461756], [G loss: 0.045628, mse: 0.056302]\n",
      "Epoch 83/10001\n",
      "[C1 valid: -0.497894, C2 fake: 0.453471], [G loss: 0.044585, mse: 0.055077]\n",
      "Epoch 84/10001\n",
      "[C1 valid: -0.489197, C2 fake: 0.456750], [G loss: 0.050495, mse: 0.061591]\n",
      "Epoch 85/10001\n",
      "[C1 valid: -0.490145, C2 fake: 0.439811], [G loss: 0.045987, mse: 0.056605]\n",
      "Epoch 86/10001\n",
      "[C1 valid: -0.495394, C2 fake: 0.460680], [G loss: 0.044607, mse: 0.055003]\n",
      "Epoch 87/10001\n",
      "[C1 valid: -0.493561, C2 fake: 0.441634], [G loss: 0.042433, mse: 0.052654]\n",
      "Epoch 88/10001\n",
      "[C1 valid: -0.496837, C2 fake: 0.451671], [G loss: 0.039672, mse: 0.049425]\n",
      "Epoch 89/10001\n",
      "[C1 valid: -0.500568, C2 fake: 0.449513], [G loss: 0.042435, mse: 0.052684]\n",
      "Epoch 90/10001\n",
      "[C1 valid: -0.489780, C2 fake: 0.442000], [G loss: 0.041002, mse: 0.050932]\n",
      "Epoch 91/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.491132, C2 fake: 0.450948], [G loss: 0.043193, mse: 0.053413]\n",
      "Epoch 92/10001\n",
      "[C1 valid: -0.498930, C2 fake: 0.444478], [G loss: 0.041522, mse: 0.051657]\n",
      "Epoch 93/10001\n",
      "[C1 valid: -0.493006, C2 fake: 0.451893], [G loss: 0.038934, mse: 0.048762]\n",
      "Epoch 94/10001\n",
      "[C1 valid: -0.505559, C2 fake: 0.433477], [G loss: 0.041674, mse: 0.051788]\n",
      "Epoch 95/10001\n",
      "[C1 valid: -0.501973, C2 fake: 0.451354], [G loss: 0.038898, mse: 0.048524]\n",
      "Epoch 96/10001\n",
      "[C1 valid: -0.506116, C2 fake: 0.445983], [G loss: 0.041596, mse: 0.051877]\n",
      "Epoch 97/10001\n",
      "[C1 valid: -0.511911, C2 fake: 0.447222], [G loss: 0.041157, mse: 0.051050]\n",
      "Epoch 98/10001\n",
      "[C1 valid: -0.509365, C2 fake: 0.439368], [G loss: 0.039117, mse: 0.048843]\n",
      "Epoch 99/10001\n",
      "[C1 valid: -0.507833, C2 fake: 0.446276], [G loss: 0.036152, mse: 0.045523]\n",
      "Epoch 100/10001\n",
      "[C1 valid: -0.506238, C2 fake: 0.434601], [G loss: 0.037402, mse: 0.046900]\n",
      "Epoch 101/10001\n",
      "[C1 valid: -0.512646, C2 fake: 0.433832], [G loss: 0.038842, mse: 0.048680]\n",
      "Epoch 102/10001\n",
      "[C1 valid: -0.512717, C2 fake: 0.444263], [G loss: 0.035598, mse: 0.044887]\n",
      "Epoch 103/10001\n",
      "[C1 valid: -0.516828, C2 fake: 0.436799], [G loss: 0.037558, mse: 0.047234]\n",
      "Epoch 104/10001\n",
      "[C1 valid: -0.519840, C2 fake: 0.433331], [G loss: 0.037235, mse: 0.046966]\n",
      "Epoch 105/10001\n",
      "[C1 valid: -0.519281, C2 fake: 0.439767], [G loss: 0.035923, mse: 0.045606]\n",
      "Epoch 106/10001\n",
      "[C1 valid: -0.515217, C2 fake: 0.440891], [G loss: 0.037407, mse: 0.047209]\n",
      "Epoch 107/10001\n",
      "[C1 valid: -0.512606, C2 fake: 0.435692], [G loss: 0.034241, mse: 0.043735]\n",
      "Epoch 108/10001\n",
      "[C1 valid: -0.518410, C2 fake: 0.441233], [G loss: 0.039180, mse: 0.049140]\n",
      "Epoch 109/10001\n",
      "[C1 valid: -0.528952, C2 fake: 0.432772], [G loss: 0.036151, mse: 0.045903]\n",
      "Epoch 110/10001\n",
      "[C1 valid: -0.517006, C2 fake: 0.431866], [G loss: 0.034975, mse: 0.044580]\n",
      "Epoch 111/10001\n",
      "[C1 valid: -0.522828, C2 fake: 0.429188], [G loss: 0.037148, mse: 0.046757]\n",
      "Epoch 112/10001\n",
      "[C1 valid: -0.518218, C2 fake: 0.435917], [G loss: 0.037160, mse: 0.047007]\n",
      "Epoch 113/10001\n",
      "[C1 valid: -0.521772, C2 fake: 0.436204], [G loss: 0.033765, mse: 0.043103]\n",
      "Epoch 114/10001\n",
      "[C1 valid: -0.530204, C2 fake: 0.439217], [G loss: 0.034245, mse: 0.043696]\n",
      "Epoch 115/10001\n",
      "[C1 valid: -0.531685, C2 fake: 0.433071], [G loss: 0.032057, mse: 0.041376]\n",
      "Epoch 116/10001\n",
      "[C1 valid: -0.524927, C2 fake: 0.438026], [G loss: 0.034348, mse: 0.043944]\n",
      "Epoch 117/10001\n",
      "[C1 valid: -0.530300, C2 fake: 0.441247], [G loss: 0.036376, mse: 0.045988]\n",
      "Epoch 118/10001\n",
      "[C1 valid: -0.532023, C2 fake: 0.427844], [G loss: 0.032917, mse: 0.042364]\n",
      "Epoch 119/10001\n",
      "[C1 valid: -0.530803, C2 fake: 0.428559], [G loss: 0.034229, mse: 0.043638]\n",
      "Epoch 120/10001\n",
      "[C1 valid: -0.540861, C2 fake: 0.430398], [G loss: 0.035919, mse: 0.045580]\n",
      "Epoch 121/10001\n",
      "[C1 valid: -0.541190, C2 fake: 0.430756], [G loss: 0.033119, mse: 0.042462]\n",
      "Epoch 122/10001\n",
      "[C1 valid: -0.535321, C2 fake: 0.434707], [G loss: 0.031916, mse: 0.041340]\n",
      "Epoch 123/10001\n",
      "[C1 valid: -0.538852, C2 fake: 0.431139], [G loss: 0.032493, mse: 0.041754]\n",
      "Epoch 124/10001\n",
      "[C1 valid: -0.537510, C2 fake: 0.430524], [G loss: 0.032286, mse: 0.041461]\n",
      "Epoch 125/10001\n",
      "[C1 valid: -0.541016, C2 fake: 0.435881], [G loss: 0.032259, mse: 0.041520]\n",
      "Epoch 126/10001\n",
      "[C1 valid: -0.552172, C2 fake: 0.435320], [G loss: 0.033256, mse: 0.042740]\n",
      "Epoch 127/10001\n",
      "[C1 valid: -0.553342, C2 fake: 0.433091], [G loss: 0.032835, mse: 0.042177]\n",
      "Epoch 128/10001\n",
      "[C1 valid: -0.543984, C2 fake: 0.427141], [G loss: 0.032667, mse: 0.042168]\n",
      "Epoch 129/10001\n",
      "[C1 valid: -0.555482, C2 fake: 0.438695], [G loss: 0.032548, mse: 0.041946]\n",
      "Epoch 130/10001\n",
      "[C1 valid: -0.551285, C2 fake: 0.434566], [G loss: 0.031668, mse: 0.041121]\n",
      "Epoch 131/10001\n",
      "[C1 valid: -0.547738, C2 fake: 0.430684], [G loss: 0.033734, mse: 0.043268]\n",
      "Epoch 132/10001\n",
      "[C1 valid: -0.559453, C2 fake: 0.431427], [G loss: 0.033559, mse: 0.043029]\n",
      "Epoch 133/10001\n",
      "[C1 valid: -0.548949, C2 fake: 0.439035], [G loss: 0.030162, mse: 0.039436]\n",
      "Epoch 134/10001\n",
      "[C1 valid: -0.560597, C2 fake: 0.429630], [G loss: 0.031281, mse: 0.040613]\n",
      "Epoch 135/10001\n",
      "[C1 valid: -0.552410, C2 fake: 0.429482], [G loss: 0.032735, mse: 0.042202]\n",
      "Epoch 136/10001\n",
      "[C1 valid: -0.569170, C2 fake: 0.432289], [G loss: 0.030110, mse: 0.039593]\n",
      "Epoch 137/10001\n",
      "[C1 valid: -0.574396, C2 fake: 0.442535], [G loss: 0.029822, mse: 0.039163]\n",
      "Epoch 138/10001\n",
      "[C1 valid: -0.562111, C2 fake: 0.441430], [G loss: 0.031131, mse: 0.040500]\n",
      "Epoch 139/10001\n",
      "[C1 valid: -0.573184, C2 fake: 0.430738], [G loss: 0.032050, mse: 0.041710]\n",
      "Epoch 140/10001\n",
      "[C1 valid: -0.573040, C2 fake: 0.426940], [G loss: 0.031797, mse: 0.041617]\n",
      "Epoch 141/10001\n",
      "[C1 valid: -0.576644, C2 fake: 0.435176], [G loss: 0.030086, mse: 0.039641]\n",
      "Epoch 142/10001\n",
      "[C1 valid: -0.582810, C2 fake: 0.431154], [G loss: 0.030845, mse: 0.040944]\n",
      "Epoch 143/10001\n",
      "[C1 valid: -0.584457, C2 fake: 0.443767], [G loss: 0.029651, mse: 0.039238]\n",
      "Epoch 144/10001\n",
      "[C1 valid: -0.588700, C2 fake: 0.430069], [G loss: 0.028966, mse: 0.038634]\n",
      "Epoch 145/10001\n",
      "[C1 valid: -0.571520, C2 fake: 0.432737], [G loss: 0.030402, mse: 0.040354]\n",
      "Epoch 146/10001\n",
      "[C1 valid: -0.582475, C2 fake: 0.439976], [G loss: 0.028765, mse: 0.038629]\n",
      "Epoch 147/10001\n",
      "[C1 valid: -0.575768, C2 fake: 0.430637], [G loss: 0.027667, mse: 0.037253]\n",
      "Epoch 148/10001\n",
      "[C1 valid: -0.602867, C2 fake: 0.448413], [G loss: 0.027263, mse: 0.037057]\n",
      "Epoch 149/10001\n",
      "[C1 valid: -0.602152, C2 fake: 0.431761], [G loss: 0.028821, mse: 0.039275]\n",
      "Epoch 150/10001\n",
      "[C1 valid: -0.604927, C2 fake: 0.430712], [G loss: 0.027749, mse: 0.037748]\n",
      "Epoch 151/10001\n",
      "[C1 valid: -0.615173, C2 fake: 0.424824], [G loss: 0.027496, mse: 0.038280]\n",
      "Epoch 152/10001\n",
      "[C1 valid: -0.621136, C2 fake: 0.438908], [G loss: 0.026210, mse: 0.036387]\n",
      "Epoch 153/10001\n",
      "[C1 valid: -0.611322, C2 fake: 0.423917], [G loss: 0.026271, mse: 0.037107]\n",
      "Epoch 154/10001\n",
      "[C1 valid: -0.625400, C2 fake: 0.446087], [G loss: 0.027327, mse: 0.038102]\n",
      "Epoch 155/10001\n",
      "[C1 valid: -0.625342, C2 fake: 0.425012], [G loss: 0.025268, mse: 0.036304]\n",
      "Epoch 156/10001\n",
      "[C1 valid: -0.623975, C2 fake: 0.427521], [G loss: 0.026879, mse: 0.038271]\n",
      "Epoch 157/10001\n",
      "[C1 valid: -0.646155, C2 fake: 0.425543], [G loss: 0.027501, mse: 0.039197]\n",
      "Epoch 158/10001\n",
      "[C1 valid: -0.640383, C2 fake: 0.427896], [G loss: 0.024712, mse: 0.036353]\n",
      "Epoch 159/10001\n",
      "[C1 valid: -0.658309, C2 fake: 0.422044], [G loss: 0.026994, mse: 0.039077]\n",
      "Epoch 160/10001\n",
      "[C1 valid: -0.660669, C2 fake: 0.410530], [G loss: 0.026291, mse: 0.038357]\n",
      "Epoch 161/10001\n",
      "[C1 valid: -0.660011, C2 fake: 0.435971], [G loss: 0.023659, mse: 0.035128]\n",
      "Epoch 162/10001\n",
      "[C1 valid: -0.663248, C2 fake: 0.397752], [G loss: 0.024739, mse: 0.036849]\n",
      "Epoch 163/10001\n",
      "[C1 valid: -0.672551, C2 fake: 0.412440], [G loss: 0.025162, mse: 0.037590]\n",
      "Epoch 164/10001\n",
      "[C1 valid: -0.684878, C2 fake: 0.411957], [G loss: 0.023407, mse: 0.035585]\n",
      "Epoch 165/10001\n",
      "[C1 valid: -0.679004, C2 fake: 0.405662], [G loss: 0.023560, mse: 0.035873]\n",
      "Epoch 166/10001\n",
      "[C1 valid: -0.685520, C2 fake: 0.398234], [G loss: 0.023932, mse: 0.036136]\n",
      "Epoch 167/10001\n",
      "[C1 valid: -0.683713, C2 fake: 0.410311], [G loss: 0.024790, mse: 0.037163]\n",
      "Epoch 168/10001\n",
      "[C1 valid: -0.673510, C2 fake: 0.395990], [G loss: 0.023840, mse: 0.036214]\n",
      "Epoch 169/10001\n",
      "[C1 valid: -0.694908, C2 fake: 0.388721], [G loss: 0.023582, mse: 0.036028]\n",
      "Epoch 170/10001\n",
      "[C1 valid: -0.707557, C2 fake: 0.390612], [G loss: 0.025058, mse: 0.037583]\n",
      "Epoch 171/10001\n",
      "[C1 valid: -0.702363, C2 fake: 0.389520], [G loss: 0.023359, mse: 0.035688]\n",
      "Epoch 172/10001\n",
      "[C1 valid: -0.698416, C2 fake: 0.393279], [G loss: 0.022818, mse: 0.035110]\n",
      "Epoch 173/10001\n",
      "[C1 valid: -0.712613, C2 fake: 0.392664], [G loss: 0.022766, mse: 0.035158]\n",
      "Epoch 174/10001\n",
      "[C1 valid: -0.718060, C2 fake: 0.399232], [G loss: 0.022965, mse: 0.035366]\n",
      "Epoch 175/10001\n",
      "[C1 valid: -0.714935, C2 fake: 0.392226], [G loss: 0.023707, mse: 0.036248]\n",
      "Epoch 176/10001\n",
      "[C1 valid: -0.717701, C2 fake: 0.383242], [G loss: 0.022680, mse: 0.035124]\n",
      "Epoch 177/10001\n",
      "[C1 valid: -0.714880, C2 fake: 0.383988], [G loss: 0.023042, mse: 0.035371]\n",
      "Epoch 178/10001\n",
      "[C1 valid: -0.730033, C2 fake: 0.371984], [G loss: 0.022319, mse: 0.034745]\n",
      "Epoch 179/10001\n",
      "[C1 valid: -0.715427, C2 fake: 0.377126], [G loss: 0.022641, mse: 0.034901]\n",
      "Epoch 180/10001\n",
      "[C1 valid: -0.731983, C2 fake: 0.376922], [G loss: 0.022770, mse: 0.035237]\n",
      "Epoch 181/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.729213, C2 fake: 0.384293], [G loss: 0.022070, mse: 0.034381]\n",
      "Epoch 182/10001\n",
      "[C1 valid: -0.738520, C2 fake: 0.376922], [G loss: 0.023268, mse: 0.035730]\n",
      "Epoch 183/10001\n",
      "[C1 valid: -0.737165, C2 fake: 0.370671], [G loss: 0.022387, mse: 0.034793]\n",
      "Epoch 184/10001\n",
      "[C1 valid: -0.749654, C2 fake: 0.372192], [G loss: 0.023483, mse: 0.035867]\n",
      "Epoch 185/10001\n",
      "[C1 valid: -0.742958, C2 fake: 0.371920], [G loss: 0.022061, mse: 0.034174]\n",
      "Epoch 186/10001\n",
      "[C1 valid: -0.741516, C2 fake: 0.379931], [G loss: 0.022870, mse: 0.035231]\n",
      "Epoch 187/10001\n",
      "[C1 valid: -0.744644, C2 fake: 0.373665], [G loss: 0.021618, mse: 0.033873]\n",
      "Epoch 188/10001\n",
      "[C1 valid: -0.747299, C2 fake: 0.373750], [G loss: 0.021293, mse: 0.033431]\n",
      "Epoch 189/10001\n",
      "[C1 valid: -0.738760, C2 fake: 0.372550], [G loss: 0.021867, mse: 0.034004]\n",
      "Epoch 190/10001\n",
      "[C1 valid: -0.753884, C2 fake: 0.379299], [G loss: 0.021346, mse: 0.033562]\n",
      "Epoch 191/10001\n",
      "[C1 valid: -0.747106, C2 fake: 0.364745], [G loss: 0.021916, mse: 0.034208]\n",
      "Epoch 192/10001\n",
      "[C1 valid: -0.750200, C2 fake: 0.368742], [G loss: 0.022627, mse: 0.034712]\n",
      "Epoch 193/10001\n",
      "[C1 valid: -0.749533, C2 fake: 0.368481], [G loss: 0.020923, mse: 0.032876]\n",
      "Epoch 194/10001\n",
      "[C1 valid: -0.759844, C2 fake: 0.363684], [G loss: 0.021019, mse: 0.033246]\n",
      "Epoch 195/10001\n",
      "[C1 valid: -0.764261, C2 fake: 0.364464], [G loss: 0.021592, mse: 0.033562]\n",
      "Epoch 196/10001\n",
      "[C1 valid: -0.769640, C2 fake: 0.377220], [G loss: 0.023692, mse: 0.035897]\n",
      "Epoch 197/10001\n",
      "[C1 valid: -0.769475, C2 fake: 0.365845], [G loss: 0.020577, mse: 0.032619]\n",
      "Epoch 198/10001\n",
      "[C1 valid: -0.762052, C2 fake: 0.367405], [G loss: 0.020839, mse: 0.032730]\n",
      "Epoch 199/10001\n",
      "[C1 valid: -0.774109, C2 fake: 0.355671], [G loss: 0.024000, mse: 0.036361]\n",
      "Epoch 200/10001\n",
      "[C1 valid: -0.771985, C2 fake: 0.364315], [G loss: 0.019840, mse: 0.031768]\n",
      "Epoch 201/10001\n",
      "[C1 valid: -0.771088, C2 fake: 0.365570], [G loss: 0.022091, mse: 0.034273]\n",
      "Epoch 202/10001\n",
      "[C1 valid: -0.770661, C2 fake: 0.358413], [G loss: 0.020725, mse: 0.032697]\n",
      "Epoch 203/10001\n",
      "[C1 valid: -0.783299, C2 fake: 0.365673], [G loss: 0.020757, mse: 0.032863]\n",
      "Epoch 204/10001\n",
      "[C1 valid: -0.778912, C2 fake: 0.362186], [G loss: 0.020248, mse: 0.032127]\n",
      "Epoch 205/10001\n",
      "[C1 valid: -0.775260, C2 fake: 0.361794], [G loss: 0.020145, mse: 0.031875]\n",
      "Epoch 206/10001\n",
      "[C1 valid: -0.766337, C2 fake: 0.355518], [G loss: 0.021515, mse: 0.033519]\n",
      "Epoch 207/10001\n",
      "[C1 valid: -0.781311, C2 fake: 0.358537], [G loss: 0.020188, mse: 0.031913]\n",
      "Epoch 208/10001\n",
      "[C1 valid: -0.776278, C2 fake: 0.360006], [G loss: 0.020650, mse: 0.032466]\n",
      "Epoch 209/10001\n",
      "[C1 valid: -0.791053, C2 fake: 0.360613], [G loss: 0.020937, mse: 0.032383]\n",
      "Epoch 210/10001\n",
      "[C1 valid: -0.788468, C2 fake: 0.375420], [G loss: 0.021389, mse: 0.033021]\n",
      "Epoch 211/10001\n",
      "[C1 valid: -0.785736, C2 fake: 0.361182], [G loss: 0.019038, mse: 0.030543]\n",
      "Epoch 212/10001\n",
      "[C1 valid: -0.783560, C2 fake: 0.348579], [G loss: 0.020228, mse: 0.031689]\n",
      "Epoch 213/10001\n",
      "[C1 valid: -0.779508, C2 fake: 0.358657], [G loss: 0.019910, mse: 0.031540]\n",
      "Epoch 214/10001\n",
      "[C1 valid: -0.786747, C2 fake: 0.355008], [G loss: 0.020623, mse: 0.031915]\n",
      "Epoch 215/10001\n",
      "[C1 valid: -0.786558, C2 fake: 0.359140], [G loss: 0.019867, mse: 0.031430]\n",
      "Epoch 216/10001\n",
      "[C1 valid: -0.782083, C2 fake: 0.354258], [G loss: 0.021332, mse: 0.032453]\n",
      "Epoch 217/10001\n",
      "[C1 valid: -0.796260, C2 fake: 0.353603], [G loss: 0.019978, mse: 0.031495]\n",
      "Epoch 218/10001\n",
      "[C1 valid: -0.805660, C2 fake: 0.350662], [G loss: 0.019758, mse: 0.031324]\n",
      "Epoch 219/10001\n",
      "[C1 valid: -0.780899, C2 fake: 0.357868], [G loss: 0.019416, mse: 0.030833]\n",
      "Epoch 220/10001\n",
      "[C1 valid: -0.806068, C2 fake: 0.355318], [G loss: 0.020257, mse: 0.031814]\n",
      "Epoch 221/10001\n",
      "[C1 valid: -0.801662, C2 fake: 0.362254], [G loss: 0.021097, mse: 0.031470]\n",
      "Epoch 222/10001\n",
      "[C1 valid: -0.792955, C2 fake: 0.358416], [G loss: 0.020397, mse: 0.032114]\n",
      "Epoch 223/10001\n",
      "[C1 valid: -0.802939, C2 fake: 0.350937], [G loss: 0.019625, mse: 0.030782]\n",
      "Epoch 224/10001\n",
      "[C1 valid: -0.816157, C2 fake: 0.354556], [G loss: 0.019327, mse: 0.030635]\n",
      "Epoch 225/10001\n",
      "[C1 valid: -0.811130, C2 fake: 0.356289], [G loss: 0.020358, mse: 0.031416]\n",
      "Epoch 226/10001\n",
      "[C1 valid: -0.809809, C2 fake: 0.342651], [G loss: 0.018503, mse: 0.029934]\n",
      "Epoch 227/10001\n",
      "[C1 valid: -0.805260, C2 fake: 0.353676], [G loss: 0.019190, mse: 0.030406]\n",
      "Epoch 228/10001\n",
      "[C1 valid: -0.805100, C2 fake: 0.345136], [G loss: 0.021314, mse: 0.032036]\n",
      "Epoch 229/10001\n",
      "[C1 valid: -0.805720, C2 fake: 0.348755], [G loss: 0.018813, mse: 0.029853]\n",
      "Epoch 230/10001\n",
      "[C1 valid: -0.814016, C2 fake: 0.347475], [G loss: 0.018669, mse: 0.029708]\n",
      "Epoch 231/10001\n",
      "[C1 valid: -0.805864, C2 fake: 0.351717], [G loss: 0.019978, mse: 0.031204]\n",
      "Epoch 232/10001\n",
      "[C1 valid: -0.814216, C2 fake: 0.341914], [G loss: 0.017323, mse: 0.028622]\n",
      "Epoch 233/10001\n",
      "[C1 valid: -0.814271, C2 fake: 0.337982], [G loss: 0.020772, mse: 0.030653]\n",
      "Epoch 234/10001\n",
      "[C1 valid: -0.815722, C2 fake: 0.344038], [G loss: 0.020776, mse: 0.031610]\n",
      "Epoch 235/10001\n",
      "[C1 valid: -0.812956, C2 fake: 0.343721], [G loss: 0.018678, mse: 0.029795]\n",
      "Epoch 236/10001\n",
      "[C1 valid: -0.813809, C2 fake: 0.344616], [G loss: 0.018708, mse: 0.029619]\n",
      "Epoch 237/10001\n",
      "[C1 valid: -0.827591, C2 fake: 0.345020], [G loss: 0.020086, mse: 0.030352]\n",
      "Epoch 238/10001\n",
      "[C1 valid: -0.819278, C2 fake: 0.331565], [G loss: 0.018087, mse: 0.028526]\n",
      "Epoch 239/10001\n",
      "[C1 valid: -0.830685, C2 fake: 0.340113], [G loss: 0.019301, mse: 0.030099]\n",
      "Epoch 240/10001\n",
      "[C1 valid: -0.820697, C2 fake: 0.349137], [G loss: 0.020920, mse: 0.030068]\n",
      "Epoch 241/10001\n",
      "[C1 valid: -0.812286, C2 fake: 0.347734], [G loss: 0.020846, mse: 0.029682]\n",
      "Epoch 242/10001\n",
      "[C1 valid: -0.831732, C2 fake: 0.340403], [G loss: 0.017916, mse: 0.028213]\n",
      "Epoch 243/10001\n",
      "[C1 valid: -0.828181, C2 fake: 0.343110], [G loss: 0.017773, mse: 0.028580]\n",
      "Epoch 244/10001\n",
      "[C1 valid: -0.820351, C2 fake: 0.337441], [G loss: 0.018059, mse: 0.028727]\n",
      "Epoch 245/10001\n",
      "[C1 valid: -0.824929, C2 fake: 0.326337], [G loss: 0.018599, mse: 0.028636]\n",
      "Epoch 246/10001\n",
      "[C1 valid: -0.818900, C2 fake: 0.318835], [G loss: 0.017170, mse: 0.027648]\n",
      "Epoch 247/10001\n",
      "[C1 valid: -0.833856, C2 fake: 0.335627], [G loss: 0.019286, mse: 0.028987]\n",
      "Epoch 248/10001\n",
      "[C1 valid: -0.832080, C2 fake: 0.334520], [G loss: 0.020291, mse: 0.028861]\n",
      "Epoch 249/10001\n",
      "[C1 valid: -0.824410, C2 fake: 0.320063], [G loss: 0.020518, mse: 0.029137]\n",
      "Epoch 250/10001\n",
      "[C1 valid: -0.831811, C2 fake: 0.343775], [G loss: 0.018276, mse: 0.029088]\n",
      "Epoch 251/10001\n",
      "[C1 valid: -0.836019, C2 fake: 0.331611], [G loss: 0.018145, mse: 0.028512]\n",
      "Epoch 252/10001\n",
      "[C1 valid: -0.830617, C2 fake: 0.337975], [G loss: 0.019553, mse: 0.029636]\n",
      "Epoch 253/10001\n",
      "[C1 valid: -0.826523, C2 fake: 0.320903], [G loss: 0.019010, mse: 0.028484]\n",
      "Epoch 254/10001\n",
      "[C1 valid: -0.848602, C2 fake: 0.321184], [G loss: 0.019971, mse: 0.028770]\n",
      "Epoch 255/10001\n",
      "[C1 valid: -0.836865, C2 fake: 0.320691], [G loss: 0.020774, mse: 0.031006]\n",
      "Epoch 256/10001\n",
      "[C1 valid: -0.830172, C2 fake: 0.319582], [G loss: 0.020663, mse: 0.029061]\n",
      "Epoch 257/10001\n",
      "[C1 valid: -0.829047, C2 fake: 0.319564], [G loss: 0.019904, mse: 0.029490]\n",
      "Epoch 258/10001\n",
      "[C1 valid: -0.836461, C2 fake: 0.320592], [G loss: 0.020732, mse: 0.029091]\n",
      "Epoch 259/10001\n",
      "[C1 valid: -0.832125, C2 fake: 0.321203], [G loss: 0.019917, mse: 0.029039]\n",
      "Epoch 260/10001\n",
      "[C1 valid: -0.843589, C2 fake: 0.321769], [G loss: 0.017737, mse: 0.027210]\n",
      "Epoch 261/10001\n",
      "[C1 valid: -0.828494, C2 fake: 0.314403], [G loss: 0.021519, mse: 0.028930]\n",
      "Epoch 262/10001\n",
      "[C1 valid: -0.842263, C2 fake: 0.324346], [G loss: 0.021716, mse: 0.028606]\n",
      "Epoch 263/10001\n",
      "[C1 valid: -0.836333, C2 fake: 0.326508], [G loss: 0.015930, mse: 0.026797]\n",
      "Epoch 264/10001\n",
      "[C1 valid: -0.825105, C2 fake: 0.317435], [G loss: 0.017640, mse: 0.027488]\n",
      "Epoch 265/10001\n",
      "[C1 valid: -0.837139, C2 fake: 0.310649], [G loss: 0.020575, mse: 0.028625]\n",
      "Epoch 266/10001\n",
      "[C1 valid: -0.847828, C2 fake: 0.309833], [G loss: 0.016539, mse: 0.026330]\n",
      "Epoch 267/10001\n",
      "[C1 valid: -0.848155, C2 fake: 0.316759], [G loss: 0.020080, mse: 0.028028]\n",
      "Epoch 268/10001\n",
      "[C1 valid: -0.846816, C2 fake: 0.314726], [G loss: 0.021243, mse: 0.029667]\n",
      "Epoch 269/10001\n",
      "[C1 valid: -0.847735, C2 fake: 0.307159], [G loss: 0.018140, mse: 0.027252]\n",
      "Epoch 270/10001\n",
      "[C1 valid: -0.848724, C2 fake: 0.320160], [G loss: 0.017303, mse: 0.026903]\n",
      "Epoch 271/10001\n",
      "[C1 valid: -0.847322, C2 fake: 0.302426], [G loss: 0.019039, mse: 0.026525]\n",
      "Epoch 272/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.854433, C2 fake: 0.315326], [G loss: 0.020991, mse: 0.027677]\n",
      "Epoch 273/10001\n",
      "[C1 valid: -0.843508, C2 fake: 0.314913], [G loss: 0.018637, mse: 0.027335]\n",
      "Epoch 274/10001\n",
      "[C1 valid: -0.848775, C2 fake: 0.309356], [G loss: 0.017844, mse: 0.027181]\n",
      "Epoch 275/10001\n",
      "[C1 valid: -0.859071, C2 fake: 0.310163], [G loss: 0.019813, mse: 0.027978]\n",
      "Epoch 276/10001\n",
      "[C1 valid: -0.864932, C2 fake: 0.304082], [G loss: 0.020363, mse: 0.028064]\n",
      "Epoch 277/10001\n",
      "[C1 valid: -0.853567, C2 fake: 0.310373], [G loss: 0.018776, mse: 0.027007]\n",
      "Epoch 278/10001\n",
      "[C1 valid: -0.859050, C2 fake: 0.301166], [G loss: 0.019783, mse: 0.027928]\n",
      "Epoch 279/10001\n",
      "[C1 valid: -0.855562, C2 fake: 0.305069], [G loss: 0.022399, mse: 0.028984]\n",
      "Epoch 280/10001\n",
      "[C1 valid: -0.849041, C2 fake: 0.311651], [G loss: 0.021249, mse: 0.027506]\n",
      "Epoch 281/10001\n",
      "[C1 valid: -0.849526, C2 fake: 0.304572], [G loss: 0.018166, mse: 0.027863]\n",
      "Epoch 282/10001\n",
      "[C1 valid: -0.836415, C2 fake: 0.310137], [G loss: 0.018014, mse: 0.026326]\n",
      "Epoch 283/10001\n",
      "[C1 valid: -0.865209, C2 fake: 0.306536], [G loss: 0.018899, mse: 0.026828]\n",
      "Epoch 284/10001\n",
      "[C1 valid: -0.852528, C2 fake: 0.301581], [G loss: 0.018642, mse: 0.025843]\n",
      "Epoch 285/10001\n",
      "[C1 valid: -0.854782, C2 fake: 0.300486], [G loss: 0.019379, mse: 0.026358]\n",
      "Epoch 286/10001\n",
      "[C1 valid: -0.857260, C2 fake: 0.291134], [G loss: 0.017531, mse: 0.025367]\n",
      "Epoch 287/10001\n",
      "[C1 valid: -0.857911, C2 fake: 0.296422], [G loss: 0.018916, mse: 0.026450]\n",
      "Epoch 288/10001\n",
      "[C1 valid: -0.867125, C2 fake: 0.304844], [G loss: 0.020019, mse: 0.026274]\n",
      "Epoch 289/10001\n",
      "[C1 valid: -0.858830, C2 fake: 0.288900], [G loss: 0.019736, mse: 0.026758]\n",
      "Epoch 290/10001\n",
      "[C1 valid: -0.863773, C2 fake: 0.297435], [G loss: 0.018994, mse: 0.024963]\n",
      "Epoch 291/10001\n",
      "[C1 valid: -0.850466, C2 fake: 0.306861], [G loss: 0.019938, mse: 0.025889]\n",
      "Epoch 292/10001\n",
      "[C1 valid: -0.849657, C2 fake: 0.286918], [G loss: 0.016264, mse: 0.025925]\n",
      "Epoch 293/10001\n",
      "[C1 valid: -0.834023, C2 fake: 0.300823], [G loss: 0.017301, mse: 0.025399]\n",
      "Epoch 294/10001\n",
      "[C1 valid: -0.868429, C2 fake: 0.292798], [G loss: 0.017102, mse: 0.024315]\n",
      "Epoch 295/10001\n",
      "[C1 valid: -0.865113, C2 fake: 0.286341], [G loss: 0.017387, mse: 0.024045]\n",
      "Epoch 296/10001\n",
      "[C1 valid: -0.868793, C2 fake: 0.289061], [G loss: 0.017347, mse: 0.024758]\n",
      "Epoch 297/10001\n",
      "[C1 valid: -0.862401, C2 fake: 0.281744], [G loss: 0.020805, mse: 0.026997]\n",
      "Epoch 298/10001\n",
      "[C1 valid: -0.877611, C2 fake: 0.280036], [G loss: 0.017838, mse: 0.024044]\n",
      "Epoch 299/10001\n",
      "[C1 valid: -0.870740, C2 fake: 0.279765], [G loss: 0.020282, mse: 0.026349]\n",
      "Epoch 300/10001\n",
      "[C1 valid: -0.874150, C2 fake: 0.281017], [G loss: 0.017563, mse: 0.024431]\n",
      "Epoch 301/10001\n",
      "[C1 valid: -0.876829, C2 fake: 0.287019], [G loss: 0.019890, mse: 0.025571]\n",
      "Epoch 302/10001\n",
      "[C1 valid: -0.874062, C2 fake: 0.283925], [G loss: 0.017385, mse: 0.024456]\n",
      "Epoch 303/10001\n",
      "[C1 valid: -0.863027, C2 fake: 0.274866], [G loss: 0.018486, mse: 0.025246]\n",
      "Epoch 304/10001\n",
      "[C1 valid: -0.885783, C2 fake: 0.267525], [G loss: 0.018188, mse: 0.025156]\n",
      "Epoch 305/10001\n",
      "[C1 valid: -0.881169, C2 fake: 0.274666], [G loss: 0.020245, mse: 0.026479]\n",
      "Epoch 306/10001\n",
      "[C1 valid: -0.885378, C2 fake: 0.272562], [G loss: 0.019304, mse: 0.025828]\n",
      "Epoch 307/10001\n",
      "[C1 valid: -0.876255, C2 fake: 0.276565], [G loss: 0.020385, mse: 0.026282]\n",
      "Epoch 308/10001\n",
      "[C1 valid: -0.877749, C2 fake: 0.279211], [G loss: 0.018608, mse: 0.024108]\n",
      "Epoch 309/10001\n",
      "[C1 valid: -0.834184, C2 fake: 0.360056], [G loss: 0.018817, mse: 0.024443]\n",
      "Epoch 310/10001\n",
      "[C1 valid: -0.698628, C2 fake: 0.332805], [G loss: 0.019248, mse: 0.024943]\n",
      "Epoch 311/10001\n",
      "[C1 valid: -0.870405, C2 fake: 0.298535], [G loss: 0.017404, mse: 0.024984]\n",
      "Epoch 312/10001\n",
      "[C1 valid: -0.857426, C2 fake: 0.287539], [G loss: 0.018159, mse: 0.025116]\n",
      "Epoch 313/10001\n",
      "[C1 valid: -0.874470, C2 fake: 0.282288], [G loss: 0.015769, mse: 0.022255]\n",
      "Epoch 314/10001\n",
      "[C1 valid: -0.872638, C2 fake: 0.264994], [G loss: 0.017024, mse: 0.023977]\n",
      "Epoch 315/10001\n",
      "[C1 valid: -0.873358, C2 fake: 0.272292], [G loss: 0.016759, mse: 0.023160]\n",
      "Epoch 316/10001\n",
      "[C1 valid: -0.882983, C2 fake: 0.266262], [G loss: 0.017909, mse: 0.024194]\n",
      "Epoch 317/10001\n",
      "[C1 valid: -0.889543, C2 fake: 0.267121], [G loss: 0.016780, mse: 0.022474]\n",
      "Epoch 318/10001\n",
      "[C1 valid: -0.872205, C2 fake: 0.264223], [G loss: 0.017647, mse: 0.023213]\n",
      "Epoch 319/10001\n",
      "[C1 valid: -0.882420, C2 fake: 0.259726], [G loss: 0.018677, mse: 0.024555]\n",
      "Epoch 320/10001\n",
      "[C1 valid: -0.888525, C2 fake: 0.257155], [G loss: 0.017439, mse: 0.023359]\n",
      "Epoch 321/10001\n",
      "[C1 valid: -0.882528, C2 fake: 0.257610], [G loss: 0.017250, mse: 0.022997]\n",
      "Epoch 322/10001\n",
      "[C1 valid: -0.887055, C2 fake: 0.250373], [G loss: 0.017982, mse: 0.023499]\n",
      "Epoch 323/10001\n",
      "[C1 valid: -0.887102, C2 fake: 0.255274], [G loss: 0.018202, mse: 0.023412]\n",
      "Epoch 324/10001\n",
      "[C1 valid: -0.885646, C2 fake: 0.257855], [G loss: 0.017105, mse: 0.023038]\n",
      "Epoch 325/10001\n",
      "[C1 valid: -0.894206, C2 fake: 0.261617], [G loss: 0.018417, mse: 0.023928]\n",
      "Epoch 326/10001\n",
      "[C1 valid: -0.893903, C2 fake: 0.264653], [G loss: 0.017308, mse: 0.022826]\n",
      "Epoch 327/10001\n",
      "[C1 valid: -0.890316, C2 fake: 0.254072], [G loss: 0.016665, mse: 0.022099]\n",
      "Epoch 328/10001\n",
      "[C1 valid: -0.894583, C2 fake: 0.261702], [G loss: 0.018204, mse: 0.023756]\n",
      "Epoch 329/10001\n",
      "[C1 valid: -0.889125, C2 fake: 0.245700], [G loss: 0.018018, mse: 0.023332]\n",
      "Epoch 330/10001\n",
      "[C1 valid: -0.888512, C2 fake: 0.251629], [G loss: 0.017495, mse: 0.022857]\n",
      "Epoch 331/10001\n",
      "[C1 valid: -0.886069, C2 fake: 0.239495], [G loss: 0.016410, mse: 0.021372]\n",
      "Epoch 332/10001\n",
      "[C1 valid: -0.882642, C2 fake: 0.260191], [G loss: 0.017285, mse: 0.023285]\n",
      "Epoch 333/10001\n",
      "[C1 valid: -0.887295, C2 fake: 0.253920], [G loss: 0.016922, mse: 0.021913]\n",
      "Epoch 334/10001\n",
      "[C1 valid: -0.902845, C2 fake: 0.246404], [G loss: 0.017105, mse: 0.022314]\n",
      "Epoch 335/10001\n",
      "[C1 valid: -0.892704, C2 fake: 0.246483], [G loss: 0.017968, mse: 0.023267]\n",
      "Epoch 336/10001\n",
      "[C1 valid: -0.897797, C2 fake: 0.250428], [G loss: 0.015635, mse: 0.020662]\n",
      "Epoch 337/10001\n",
      "[C1 valid: -0.897771, C2 fake: 0.248459], [G loss: 0.018200, mse: 0.023066]\n",
      "Epoch 338/10001\n",
      "[C1 valid: -0.888913, C2 fake: 0.242943], [G loss: 0.016526, mse: 0.021243]\n",
      "Epoch 339/10001\n",
      "[C1 valid: -0.893352, C2 fake: 0.235799], [G loss: 0.017554, mse: 0.022491]\n",
      "Epoch 340/10001\n",
      "[C1 valid: -0.881953, C2 fake: 0.237507], [G loss: 0.017539, mse: 0.022259]\n",
      "Epoch 341/10001\n",
      "[C1 valid: -0.897329, C2 fake: 0.233543], [G loss: 0.015835, mse: 0.020719]\n",
      "Epoch 342/10001\n",
      "[C1 valid: -0.904338, C2 fake: 0.238435], [G loss: 0.016904, mse: 0.021544]\n",
      "Epoch 343/10001\n",
      "[C1 valid: -0.896858, C2 fake: 0.225746], [G loss: 0.016183, mse: 0.020839]\n",
      "Epoch 344/10001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#predicted_values = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "#sampling from the latent space without prediction\n",
    "latent_values = tf.random.normal([1000, Z])\n",
    "\n",
    "#predict the labels of the data values on the basis of the trained model.\n",
    "predicted_values = aae.decoder.predict(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values[:,:]=(predicted_values[:,:])\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-3,-1,1,3]\n",
    "n_points = 400\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//4, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//4, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//4, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//4, n_features), x_input[3])\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4))\n",
    "\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder(aae.encoder.predict(input_test))\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
