{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder=network16.build_encoder(Z, nodes, n_features)\n",
    "#print(\"Encoder:\\n\")\n",
    "#encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network16.build_decoder(Z,nodes, n_features)\n",
    "#print(\"Decoder:\\n\")\n",
    "#decoder.summary()\n",
    "\n",
    "discriminator=network16.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "#discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model16\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 10001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model16.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10001\n",
      "[D valid loss: 0.501108],[D fake loss: 0.000000], [G loss(mse): 1.320221, G loss(w): 0.824692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05019/AAE_Model16.py:199: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_40_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_40_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_40_10001/assets\n",
      "Epoch 2/10001\n",
      "[D valid loss: 0.498565],[D fake loss: 0.000000], [G loss(mse): 1.329681, G loss(w): 0.832070]\n",
      "Epoch 3/10001\n",
      "[D valid loss: 0.496521],[D fake loss: 0.000000], [G loss(mse): 1.330343, G loss(w): 0.833398]\n",
      "Epoch 4/10001\n",
      "[D valid loss: 0.493615],[D fake loss: 0.000000], [G loss(mse): 1.329697, G loss(w): 0.832947]\n",
      "Epoch 5/10001\n",
      "[D valid loss: 0.490532],[D fake loss: 0.000000], [G loss(mse): 1.277186, G loss(w): 0.782266]\n",
      "Epoch 6/10001\n",
      "[D valid loss: 0.487505],[D fake loss: 0.000000], [G loss(mse): 1.279055, G loss(w): 0.787337]\n",
      "Epoch 7/10001\n",
      "[D valid loss: 0.483341],[D fake loss: 0.000000], [G loss(mse): 1.245487, G loss(w): 0.757700]\n",
      "Epoch 8/10001\n",
      "[D valid loss: 0.480362],[D fake loss: 0.000000], [G loss(mse): 1.227970, G loss(w): 0.743310]\n",
      "Epoch 9/10001\n",
      "[D valid loss: 0.475742],[D fake loss: 0.000000], [G loss(mse): 1.223030, G loss(w): 0.740420]\n",
      "Epoch 10/10001\n",
      "[D valid loss: 0.470416],[D fake loss: 0.000000], [G loss(mse): 1.160216, G loss(w): 0.688894]\n",
      "Epoch 11/10001\n",
      "[D valid loss: 0.466558],[D fake loss: 0.000000], [G loss(mse): 1.169969, G loss(w): 0.707786]\n",
      "Epoch 12/10001\n",
      "[D valid loss: 0.462371],[D fake loss: 0.000000], [G loss(mse): 1.149447, G loss(w): 0.695934]\n",
      "Epoch 13/10001\n",
      "[D valid loss: 0.456127],[D fake loss: 0.000000], [G loss(mse): 1.092636, G loss(w): 0.646859]\n",
      "Epoch 14/10001\n",
      "[D valid loss: 0.450719],[D fake loss: 0.000000], [G loss(mse): 1.093573, G loss(w): 0.656551]\n",
      "Epoch 15/10001\n",
      "[D valid loss: 0.447124],[D fake loss: 0.000000], [G loss(mse): 1.066988, G loss(w): 0.641595]\n",
      "Epoch 16/10001\n",
      "[D valid loss: 0.440140],[D fake loss: 0.000000], [G loss(mse): 1.041575, G loss(w): 0.629565]\n",
      "Epoch 17/10001\n",
      "[D valid loss: 0.436161],[D fake loss: 0.000000], [G loss(mse): 1.028196, G loss(w): 0.622273]\n",
      "Epoch 18/10001\n",
      "[D valid loss: 0.431183],[D fake loss: 0.000000], [G loss(mse): 1.008963, G loss(w): 0.623376]\n",
      "Epoch 19/10001\n",
      "[D valid loss: 0.425645],[D fake loss: 0.000000], [G loss(mse): 0.934585, G loss(w): 0.555094]\n",
      "Epoch 20/10001\n",
      "[D valid loss: 0.417813],[D fake loss: 0.000000], [G loss(mse): 0.962027, G loss(w): 0.600380]\n",
      "Epoch 21/10001\n",
      "[D valid loss: 0.415959],[D fake loss: 0.000000], [G loss(mse): 0.925175, G loss(w): 0.576023]\n",
      "Epoch 22/10001\n",
      "[D valid loss: 0.410404],[D fake loss: 0.000000], [G loss(mse): 0.889160, G loss(w): 0.543360]\n",
      "Epoch 23/10001\n",
      "[D valid loss: 0.408847],[D fake loss: 0.000000], [G loss(mse): 0.877754, G loss(w): 0.548603]\n",
      "Epoch 24/10001\n",
      "[D valid loss: 0.402689],[D fake loss: 0.000000], [G loss(mse): 0.849661, G loss(w): 0.524239]\n",
      "Epoch 25/10001\n",
      "[D valid loss: 0.396834],[D fake loss: 0.000000], [G loss(mse): 0.854154, G loss(w): 0.541749]\n",
      "Epoch 26/10001\n",
      "[D valid loss: 0.394623],[D fake loss: 0.000000], [G loss(mse): 0.807121, G loss(w): 0.508124]\n",
      "Epoch 27/10001\n",
      "[D valid loss: 0.391934],[D fake loss: 0.000000], [G loss(mse): 0.780247, G loss(w): 0.495971]\n",
      "Epoch 28/10001\n",
      "[D valid loss: 0.385894],[D fake loss: 0.000000], [G loss(mse): 0.773195, G loss(w): 0.482984]\n",
      "Epoch 29/10001\n",
      "[D valid loss: 0.382071],[D fake loss: 0.000000], [G loss(mse): 0.767548, G loss(w): 0.486942]\n",
      "Epoch 30/10001\n",
      "[D valid loss: 0.375957],[D fake loss: 0.000000], [G loss(mse): 0.729656, G loss(w): 0.463040]\n",
      "Epoch 31/10001\n",
      "[D valid loss: 0.374200],[D fake loss: 0.000000], [G loss(mse): 0.716525, G loss(w): 0.465421]\n",
      "Epoch 32/10001\n",
      "[D valid loss: 0.366897],[D fake loss: 0.000000], [G loss(mse): 0.711121, G loss(w): 0.451908]\n",
      "Epoch 33/10001\n",
      "[D valid loss: 0.365540],[D fake loss: 0.000000], [G loss(mse): 0.690780, G loss(w): 0.449067]\n",
      "Epoch 34/10001\n",
      "[D valid loss: 0.361072],[D fake loss: 0.000000], [G loss(mse): 0.665659, G loss(w): 0.429635]\n",
      "Epoch 35/10001\n",
      "[D valid loss: 0.359568],[D fake loss: 0.000000], [G loss(mse): 0.662590, G loss(w): 0.416440]\n",
      "Epoch 36/10001\n",
      "[D valid loss: 0.353285],[D fake loss: 0.000000], [G loss(mse): 0.643899, G loss(w): 0.418466]\n",
      "Epoch 37/10001\n",
      "[D valid loss: 0.350022],[D fake loss: 0.000000], [G loss(mse): 0.640405, G loss(w): 0.409887]\n",
      "Epoch 38/10001\n",
      "[D valid loss: 0.348037],[D fake loss: 0.000000], [G loss(mse): 0.615797, G loss(w): 0.397165]\n",
      "Epoch 39/10001\n",
      "[D valid loss: 0.345165],[D fake loss: 0.000000], [G loss(mse): 0.607606, G loss(w): 0.393158]\n",
      "Epoch 40/10001\n",
      "[D valid loss: 0.338257],[D fake loss: 0.000000], [G loss(mse): 0.604951, G loss(w): 0.391384]\n",
      "Epoch 41/10001\n",
      "[D valid loss: 0.338248],[D fake loss: 0.000000], [G loss(mse): 0.600712, G loss(w): 0.397430]\n",
      "Epoch 42/10001\n",
      "[D valid loss: 0.332884],[D fake loss: 0.000000], [G loss(mse): 0.582019, G loss(w): 0.373141]\n",
      "Epoch 43/10001\n",
      "[D valid loss: 0.332801],[D fake loss: 0.000000], [G loss(mse): 0.580514, G loss(w): 0.387737]\n",
      "Epoch 44/10001\n",
      "[D valid loss: 0.326157],[D fake loss: 0.000000], [G loss(mse): 0.580678, G loss(w): 0.389687]\n",
      "Epoch 45/10001\n",
      "[D valid loss: 0.321450],[D fake loss: 0.000000], [G loss(mse): 0.561205, G loss(w): 0.365657]\n",
      "Epoch 46/10001\n",
      "[D valid loss: 0.320905],[D fake loss: 0.000000], [G loss(mse): 0.557074, G loss(w): 0.366678]\n",
      "Epoch 47/10001\n",
      "[D valid loss: 0.317499],[D fake loss: 0.000000], [G loss(mse): 0.529378, G loss(w): 0.344105]\n",
      "Epoch 48/10001\n",
      "[D valid loss: 0.316826],[D fake loss: 0.000000], [G loss(mse): 0.524510, G loss(w): 0.341589]\n",
      "Epoch 49/10001\n",
      "[D valid loss: 0.310540],[D fake loss: 0.000000], [G loss(mse): 0.520800, G loss(w): 0.345195]\n",
      "Epoch 50/10001\n",
      "[D valid loss: 0.308050],[D fake loss: 0.000000], [G loss(mse): 0.519671, G loss(w): 0.346081]\n",
      "Epoch 51/10001\n",
      "[D valid loss: 0.306863],[D fake loss: 0.000000], [G loss(mse): 0.499721, G loss(w): 0.332387]\n",
      "Epoch 52/10001\n",
      "[D valid loss: 0.300900],[D fake loss: 0.000000], [G loss(mse): 0.504336, G loss(w): 0.341080]\n",
      "Epoch 53/10001\n",
      "[D valid loss: 0.301734],[D fake loss: 0.000000], [G loss(mse): 0.482449, G loss(w): 0.316567]\n",
      "Epoch 54/10001\n",
      "[D valid loss: 0.297612],[D fake loss: 0.000000], [G loss(mse): 0.478321, G loss(w): 0.327572]\n",
      "Epoch 55/10001\n",
      "[D valid loss: 0.292651],[D fake loss: 0.000000], [G loss(mse): 0.488173, G loss(w): 0.326325]\n",
      "Epoch 56/10001\n",
      "[D valid loss: 0.290362],[D fake loss: 0.000000], [G loss(mse): 0.464789, G loss(w): 0.315527]\n",
      "Epoch 57/10001\n",
      "[D valid loss: 0.289536],[D fake loss: 0.000000], [G loss(mse): 0.470807, G loss(w): 0.326262]\n",
      "Epoch 58/10001\n",
      "[D valid loss: 0.285189],[D fake loss: 0.000000], [G loss(mse): 0.455729, G loss(w): 0.306296]\n",
      "Epoch 59/10001\n",
      "[D valid loss: 0.285757],[D fake loss: 0.000000], [G loss(mse): 0.430142, G loss(w): 0.296993]\n",
      "Epoch 60/10001\n",
      "[D valid loss: 0.281771],[D fake loss: 0.000000], [G loss(mse): 0.438614, G loss(w): 0.300314]\n",
      "Epoch 61/10001\n",
      "[D valid loss: 0.278988],[D fake loss: 0.000000], [G loss(mse): 0.422963, G loss(w): 0.291210]\n",
      "Epoch 62/10001\n",
      "[D valid loss: 0.276955],[D fake loss: 0.000000], [G loss(mse): 0.428963, G loss(w): 0.285532]\n",
      "Epoch 63/10001\n",
      "[D valid loss: 0.270833],[D fake loss: 0.000000], [G loss(mse): 0.413092, G loss(w): 0.282757]\n",
      "Epoch 64/10001\n",
      "[D valid loss: 0.269085],[D fake loss: 0.000000], [G loss(mse): 0.416800, G loss(w): 0.291397]\n",
      "Epoch 65/10001\n",
      "[D valid loss: 0.266831],[D fake loss: 0.000000], [G loss(mse): 0.412564, G loss(w): 0.283062]\n",
      "Epoch 66/10001\n",
      "[D valid loss: 0.267955],[D fake loss: 0.000000], [G loss(mse): 0.419837, G loss(w): 0.291970]\n",
      "Epoch 67/10001\n",
      "[D valid loss: 0.262823],[D fake loss: 0.000000], [G loss(mse): 0.396996, G loss(w): 0.273813]\n",
      "Epoch 68/10001\n",
      "[D valid loss: 0.261913],[D fake loss: 0.000000], [G loss(mse): 0.402067, G loss(w): 0.278418]\n",
      "Epoch 69/10001\n",
      "[D valid loss: 0.257377],[D fake loss: 0.000000], [G loss(mse): 0.383968, G loss(w): 0.266407]\n",
      "Epoch 70/10001\n",
      "[D valid loss: 0.257273],[D fake loss: 0.000000], [G loss(mse): 0.388030, G loss(w): 0.268856]\n",
      "Epoch 71/10001\n",
      "[D valid loss: 0.254726],[D fake loss: 0.000000], [G loss(mse): 0.391247, G loss(w): 0.271844]\n",
      "Epoch 72/10001\n",
      "[D valid loss: 0.256277],[D fake loss: 0.000000], [G loss(mse): 0.384965, G loss(w): 0.263226]\n",
      "Epoch 73/10001\n",
      "[D valid loss: 0.248843],[D fake loss: 0.000000], [G loss(mse): 0.384540, G loss(w): 0.275046]\n",
      "Epoch 74/10001\n",
      "[D valid loss: 0.249128],[D fake loss: 0.000000], [G loss(mse): 0.389737, G loss(w): 0.264718]\n",
      "Epoch 75/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.246957],[D fake loss: 0.000000], [G loss(mse): 0.364278, G loss(w): 0.257310]\n",
      "Epoch 76/10001\n",
      "[D valid loss: 0.245167],[D fake loss: 0.000000], [G loss(mse): 0.347775, G loss(w): 0.238699]\n",
      "Epoch 77/10001\n",
      "[D valid loss: 0.240104],[D fake loss: 0.000000], [G loss(mse): 0.356058, G loss(w): 0.256651]\n",
      "Epoch 78/10001\n",
      "[D valid loss: 0.242510],[D fake loss: 0.000000], [G loss(mse): 0.356002, G loss(w): 0.245779]\n",
      "Epoch 79/10001\n",
      "[D valid loss: 0.239087],[D fake loss: 0.000000], [G loss(mse): 0.341679, G loss(w): 0.247269]\n",
      "Epoch 80/10001\n",
      "[D valid loss: 0.236913],[D fake loss: 0.000000], [G loss(mse): 0.339043, G loss(w): 0.254154]\n",
      "Epoch 81/10001\n",
      "[D valid loss: 0.234783],[D fake loss: 0.000000], [G loss(mse): 0.341721, G loss(w): 0.244132]\n",
      "Epoch 82/10001\n",
      "[D valid loss: 0.232275],[D fake loss: 0.000000], [G loss(mse): 0.328754, G loss(w): 0.240773]\n",
      "Epoch 83/10001\n",
      "[D valid loss: 0.231786],[D fake loss: 0.000000], [G loss(mse): 0.345773, G loss(w): 0.247265]\n",
      "Epoch 84/10001\n",
      "[D valid loss: 0.227933],[D fake loss: 0.000000], [G loss(mse): 0.333337, G loss(w): 0.240920]\n",
      "Epoch 85/10001\n",
      "[D valid loss: 0.229294],[D fake loss: 0.000000], [G loss(mse): 0.322911, G loss(w): 0.227601]\n",
      "Epoch 86/10001\n",
      "[D valid loss: 0.224955],[D fake loss: 0.000000], [G loss(mse): 0.311301, G loss(w): 0.226956]\n",
      "Epoch 87/10001\n",
      "[D valid loss: 0.224687],[D fake loss: 0.000000], [G loss(mse): 0.309333, G loss(w): 0.222776]\n",
      "Epoch 88/10001\n",
      "[D valid loss: 0.221537],[D fake loss: 0.000000], [G loss(mse): 0.318034, G loss(w): 0.227503]\n",
      "Epoch 89/10001\n",
      "[D valid loss: 0.219003],[D fake loss: 0.000000], [G loss(mse): 0.298499, G loss(w): 0.219292]\n",
      "Epoch 90/10001\n",
      "[D valid loss: 0.220531],[D fake loss: 0.000000], [G loss(mse): 0.310691, G loss(w): 0.234070]\n",
      "Epoch 91/10001\n",
      "[D valid loss: 0.214701],[D fake loss: 0.000000], [G loss(mse): 0.313695, G loss(w): 0.229500]\n",
      "Epoch 92/10001\n",
      "[D valid loss: 0.215622],[D fake loss: 0.000000], [G loss(mse): 0.299276, G loss(w): 0.210575]\n",
      "Epoch 93/10001\n",
      "[D valid loss: 0.212582],[D fake loss: 0.000000], [G loss(mse): 0.296836, G loss(w): 0.217692]\n",
      "Epoch 94/10001\n",
      "[D valid loss: 0.210314],[D fake loss: 0.000000], [G loss(mse): 0.302465, G loss(w): 0.221758]\n",
      "Epoch 95/10001\n",
      "[D valid loss: 0.210718],[D fake loss: 0.000000], [G loss(mse): 0.294573, G loss(w): 0.217322]\n",
      "Epoch 96/10001\n",
      "[D valid loss: 0.209845],[D fake loss: 0.000000], [G loss(mse): 0.301489, G loss(w): 0.218535]\n",
      "Epoch 97/10001\n",
      "[D valid loss: 0.206232],[D fake loss: 0.000000], [G loss(mse): 0.274283, G loss(w): 0.196528]\n",
      "Epoch 98/10001\n",
      "[D valid loss: 0.203297],[D fake loss: 0.000000], [G loss(mse): 0.277869, G loss(w): 0.205980]\n",
      "Epoch 99/10001\n",
      "[D valid loss: 0.202163],[D fake loss: 0.000000], [G loss(mse): 0.281424, G loss(w): 0.210993]\n",
      "Epoch 100/10001\n",
      "[D valid loss: 0.203483],[D fake loss: 0.000000], [G loss(mse): 0.285277, G loss(w): 0.208643]\n",
      "Epoch 101/10001\n",
      "[D valid loss: 0.199644],[D fake loss: 0.000000], [G loss(mse): 0.285028, G loss(w): 0.210632]\n",
      "Epoch 102/10001\n",
      "[D valid loss: 0.194337],[D fake loss: 0.000000], [G loss(mse): 0.269568, G loss(w): 0.205038]\n",
      "Epoch 103/10001\n",
      "[D valid loss: 0.195894],[D fake loss: 0.000000], [G loss(mse): 0.284682, G loss(w): 0.211654]\n",
      "Epoch 104/10001\n",
      "[D valid loss: 0.198354],[D fake loss: 0.000000], [G loss(mse): 0.267773, G loss(w): 0.194128]\n",
      "Epoch 105/10001\n",
      "[D valid loss: 0.191640],[D fake loss: 0.000000], [G loss(mse): 0.279436, G loss(w): 0.205734]\n",
      "Epoch 106/10001\n",
      "[D valid loss: 0.191754],[D fake loss: 0.000000], [G loss(mse): 0.267194, G loss(w): 0.197854]\n",
      "Epoch 107/10001\n",
      "[D valid loss: 0.188759],[D fake loss: 0.000000], [G loss(mse): 0.273527, G loss(w): 0.199889]\n",
      "Epoch 108/10001\n",
      "[D valid loss: 0.188948],[D fake loss: 0.000000], [G loss(mse): 0.265651, G loss(w): 0.194444]\n",
      "Epoch 109/10001\n",
      "[D valid loss: 0.187987],[D fake loss: 0.000000], [G loss(mse): 0.268418, G loss(w): 0.191939]\n",
      "Epoch 110/10001\n",
      "[D valid loss: 0.184674],[D fake loss: 0.000000], [G loss(mse): 0.257372, G loss(w): 0.190752]\n",
      "Epoch 111/10001\n",
      "[D valid loss: 0.184848],[D fake loss: 0.000000], [G loss(mse): 0.273956, G loss(w): 0.205501]\n",
      "Epoch 112/10001\n",
      "[D valid loss: 0.183857],[D fake loss: 0.000000], [G loss(mse): 0.259563, G loss(w): 0.195313]\n",
      "Epoch 113/10001\n",
      "[D valid loss: 0.180940],[D fake loss: 0.000000], [G loss(mse): 0.250333, G loss(w): 0.180677]\n",
      "Epoch 114/10001\n",
      "[D valid loss: 0.181425],[D fake loss: 0.000000], [G loss(mse): 0.253211, G loss(w): 0.191709]\n",
      "Epoch 115/10001\n",
      "[D valid loss: 0.180480],[D fake loss: 0.000000], [G loss(mse): 0.241359, G loss(w): 0.177779]\n",
      "Epoch 116/10001\n",
      "[D valid loss: 0.181600],[D fake loss: 0.000000], [G loss(mse): 0.244400, G loss(w): 0.182828]\n",
      "Epoch 117/10001\n",
      "[D valid loss: 0.176341],[D fake loss: 0.000000], [G loss(mse): 0.242260, G loss(w): 0.181801]\n",
      "Epoch 118/10001\n",
      "[D valid loss: 0.175172],[D fake loss: 0.000000], [G loss(mse): 0.240116, G loss(w): 0.179840]\n",
      "Epoch 119/10001\n",
      "[D valid loss: 0.173953],[D fake loss: 0.000000], [G loss(mse): 0.237515, G loss(w): 0.173907]\n",
      "Epoch 120/10001\n",
      "[D valid loss: 0.176276],[D fake loss: 0.000000], [G loss(mse): 0.238129, G loss(w): 0.183350]\n",
      "Epoch 121/10001\n",
      "[D valid loss: 0.173706],[D fake loss: 0.000000], [G loss(mse): 0.244343, G loss(w): 0.180628]\n",
      "Epoch 122/10001\n",
      "[D valid loss: 0.170362],[D fake loss: 0.000000], [G loss(mse): 0.228496, G loss(w): 0.174333]\n",
      "Epoch 123/10001\n",
      "[D valid loss: 0.171858],[D fake loss: 0.000000], [G loss(mse): 0.227574, G loss(w): 0.167886]\n",
      "Epoch 124/10001\n",
      "[D valid loss: 0.169524],[D fake loss: 0.000000], [G loss(mse): 0.236037, G loss(w): 0.174156]\n",
      "Epoch 125/10001\n",
      "[D valid loss: 0.167587],[D fake loss: 0.000000], [G loss(mse): 0.234308, G loss(w): 0.173605]\n",
      "Epoch 126/10001\n",
      "[D valid loss: 0.165383],[D fake loss: 0.000000], [G loss(mse): 0.222011, G loss(w): 0.167840]\n",
      "Epoch 127/10001\n",
      "[D valid loss: 0.166688],[D fake loss: 0.000000], [G loss(mse): 0.220081, G loss(w): 0.164303]\n",
      "Epoch 128/10001\n",
      "[D valid loss: 0.165286],[D fake loss: 0.000000], [G loss(mse): 0.223948, G loss(w): 0.168792]\n",
      "Epoch 129/10001\n",
      "[D valid loss: 0.162266],[D fake loss: 0.000000], [G loss(mse): 0.228055, G loss(w): 0.173366]\n",
      "Epoch 130/10001\n",
      "[D valid loss: 0.162305],[D fake loss: 0.000000], [G loss(mse): 0.222134, G loss(w): 0.171686]\n",
      "Epoch 131/10001\n",
      "[D valid loss: 0.161429],[D fake loss: 0.000000], [G loss(mse): 0.220866, G loss(w): 0.164681]\n",
      "Epoch 132/10001\n",
      "[D valid loss: 0.159081],[D fake loss: 0.000000], [G loss(mse): 0.220444, G loss(w): 0.166859]\n",
      "Epoch 133/10001\n",
      "[D valid loss: 0.158600],[D fake loss: 0.000000], [G loss(mse): 0.223095, G loss(w): 0.174181]\n",
      "Epoch 134/10001\n",
      "[D valid loss: 0.158892],[D fake loss: 0.000000], [G loss(mse): 0.220189, G loss(w): 0.164488]\n",
      "Epoch 135/10001\n",
      "[D valid loss: 0.153847],[D fake loss: 0.000000], [G loss(mse): 0.219856, G loss(w): 0.164827]\n",
      "Epoch 136/10001\n",
      "[D valid loss: 0.155443],[D fake loss: 0.000000], [G loss(mse): 0.213918, G loss(w): 0.161184]\n",
      "Epoch 137/10001\n",
      "[D valid loss: 0.153752],[D fake loss: 0.000000], [G loss(mse): 0.211347, G loss(w): 0.162316]\n",
      "Epoch 138/10001\n",
      "[D valid loss: 0.154842],[D fake loss: 0.000000], [G loss(mse): 0.209856, G loss(w): 0.158193]\n",
      "Epoch 139/10001\n",
      "[D valid loss: 0.152279],[D fake loss: 0.000000], [G loss(mse): 0.206444, G loss(w): 0.159211]\n",
      "Epoch 140/10001\n",
      "[D valid loss: 0.150482],[D fake loss: 0.000000], [G loss(mse): 0.209392, G loss(w): 0.160909]\n",
      "Epoch 141/10001\n",
      "[D valid loss: 0.148998],[D fake loss: 0.000000], [G loss(mse): 0.209364, G loss(w): 0.156087]\n",
      "Epoch 142/10001\n",
      "[D valid loss: 0.149595],[D fake loss: 0.000000], [G loss(mse): 0.195617, G loss(w): 0.149549]\n",
      "Epoch 143/10001\n",
      "[D valid loss: 0.147900],[D fake loss: 0.000000], [G loss(mse): 0.208574, G loss(w): 0.160397]\n",
      "Epoch 144/10001\n",
      "[D valid loss: 0.147064],[D fake loss: 0.000000], [G loss(mse): 0.206913, G loss(w): 0.158364]\n",
      "Epoch 145/10001\n",
      "[D valid loss: 0.149528],[D fake loss: 0.000000], [G loss(mse): 0.201941, G loss(w): 0.151461]\n",
      "Epoch 146/10001\n",
      "[D valid loss: 0.144077],[D fake loss: 0.000000], [G loss(mse): 0.195438, G loss(w): 0.147466]\n",
      "Epoch 147/10001\n",
      "[D valid loss: 0.142141],[D fake loss: 0.000000], [G loss(mse): 0.202394, G loss(w): 0.151783]\n",
      "Epoch 148/10001\n",
      "[D valid loss: 0.142308],[D fake loss: 0.000000], [G loss(mse): 0.200639, G loss(w): 0.150706]\n",
      "Epoch 149/10001\n",
      "[D valid loss: 0.140088],[D fake loss: 0.000000], [G loss(mse): 0.194016, G loss(w): 0.145810]\n",
      "Epoch 150/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.143898],[D fake loss: 0.000000], [G loss(mse): 0.189583, G loss(w): 0.142850]\n",
      "Epoch 151/10001\n",
      "[D valid loss: 0.140325],[D fake loss: 0.000000], [G loss(mse): 0.191235, G loss(w): 0.142994]\n",
      "Epoch 152/10001\n",
      "[D valid loss: 0.138701],[D fake loss: 0.000000], [G loss(mse): 0.200485, G loss(w): 0.154086]\n",
      "Epoch 153/10001\n",
      "[D valid loss: 0.139696],[D fake loss: 0.000000], [G loss(mse): 0.199574, G loss(w): 0.152829]\n",
      "Epoch 154/10001\n",
      "[D valid loss: 0.138073],[D fake loss: 0.000000], [G loss(mse): 0.197779, G loss(w): 0.151078]\n",
      "Epoch 155/10001\n",
      "[D valid loss: 0.136845],[D fake loss: 0.000000], [G loss(mse): 0.189697, G loss(w): 0.145978]\n",
      "Epoch 156/10001\n",
      "[D valid loss: 0.134117],[D fake loss: 0.000000], [G loss(mse): 0.194317, G loss(w): 0.155733]\n",
      "Epoch 157/10001\n",
      "[D valid loss: 0.132284],[D fake loss: 0.000000], [G loss(mse): 0.184649, G loss(w): 0.145091]\n",
      "Epoch 158/10001\n",
      "[D valid loss: 0.130282],[D fake loss: 0.000000], [G loss(mse): 0.181387, G loss(w): 0.139903]\n",
      "Epoch 159/10001\n",
      "[D valid loss: 0.131421],[D fake loss: 0.000000], [G loss(mse): 0.187705, G loss(w): 0.143540]\n",
      "Epoch 160/10001\n",
      "[D valid loss: 0.129786],[D fake loss: 0.000000], [G loss(mse): 0.183686, G loss(w): 0.141674]\n",
      "Epoch 161/10001\n",
      "[D valid loss: 0.128141],[D fake loss: 0.000000], [G loss(mse): 0.178550, G loss(w): 0.140754]\n",
      "Epoch 162/10001\n",
      "[D valid loss: 0.128020],[D fake loss: 0.000000], [G loss(mse): 0.183136, G loss(w): 0.143955]\n",
      "Epoch 163/10001\n",
      "[D valid loss: 0.126507],[D fake loss: 0.000000], [G loss(mse): 0.178166, G loss(w): 0.138121]\n",
      "Epoch 164/10001\n",
      "[D valid loss: 0.127342],[D fake loss: 0.000000], [G loss(mse): 0.169597, G loss(w): 0.131831]\n",
      "Epoch 165/10001\n",
      "[D valid loss: 0.124392],[D fake loss: 0.000000], [G loss(mse): 0.172747, G loss(w): 0.130648]\n",
      "Epoch 166/10001\n",
      "[D valid loss: 0.126948],[D fake loss: 0.000000], [G loss(mse): 0.174852, G loss(w): 0.136442]\n",
      "Epoch 167/10001\n",
      "[D valid loss: 0.122228],[D fake loss: 0.000000], [G loss(mse): 0.175586, G loss(w): 0.135791]\n",
      "Epoch 168/10001\n",
      "[D valid loss: 0.122291],[D fake loss: 0.000000], [G loss(mse): 0.177350, G loss(w): 0.138788]\n",
      "Epoch 169/10001\n",
      "[D valid loss: 0.121730],[D fake loss: 0.000000], [G loss(mse): 0.177941, G loss(w): 0.139776]\n",
      "Epoch 170/10001\n",
      "[D valid loss: 0.122141],[D fake loss: 0.000000], [G loss(mse): 0.164477, G loss(w): 0.125876]\n",
      "Epoch 171/10001\n",
      "[D valid loss: 0.121461],[D fake loss: 0.000000], [G loss(mse): 0.176681, G loss(w): 0.139271]\n",
      "Epoch 172/10001\n",
      "[D valid loss: 0.119274],[D fake loss: 0.000000], [G loss(mse): 0.164964, G loss(w): 0.126163]\n",
      "Epoch 173/10001\n",
      "[D valid loss: 0.120367],[D fake loss: 0.000000], [G loss(mse): 0.173448, G loss(w): 0.136312]\n",
      "Epoch 174/10001\n",
      "[D valid loss: 0.118097],[D fake loss: 0.000000], [G loss(mse): 0.168708, G loss(w): 0.132602]\n",
      "Epoch 175/10001\n",
      "[D valid loss: 0.118269],[D fake loss: 0.000000], [G loss(mse): 0.164573, G loss(w): 0.128234]\n",
      "Epoch 176/10001\n",
      "[D valid loss: 0.115753],[D fake loss: 0.000000], [G loss(mse): 0.164264, G loss(w): 0.125096]\n",
      "Epoch 177/10001\n",
      "[D valid loss: 0.114961],[D fake loss: 0.000000], [G loss(mse): 0.161268, G loss(w): 0.121452]\n",
      "Epoch 178/10001\n",
      "[D valid loss: 0.111970],[D fake loss: 0.000000], [G loss(mse): 0.164559, G loss(w): 0.130829]\n",
      "Epoch 179/10001\n",
      "[D valid loss: 0.114026],[D fake loss: 0.000000], [G loss(mse): 0.172123, G loss(w): 0.136178]\n",
      "Epoch 180/10001\n",
      "[D valid loss: 0.113554],[D fake loss: 0.000000], [G loss(mse): 0.166205, G loss(w): 0.128336]\n",
      "Epoch 181/10001\n",
      "[D valid loss: 0.111055],[D fake loss: 0.000000], [G loss(mse): 0.163568, G loss(w): 0.130291]\n",
      "Epoch 182/10001\n",
      "[D valid loss: 0.113236],[D fake loss: 0.000000], [G loss(mse): 0.166183, G loss(w): 0.131088]\n",
      "Epoch 183/10001\n",
      "[D valid loss: 0.112694],[D fake loss: 0.000000], [G loss(mse): 0.163687, G loss(w): 0.125762]\n",
      "Epoch 184/10001\n",
      "[D valid loss: 0.110665],[D fake loss: 0.000000], [G loss(mse): 0.159523, G loss(w): 0.125156]\n",
      "Epoch 185/10001\n",
      "[D valid loss: 0.111409],[D fake loss: 0.000000], [G loss(mse): 0.153380, G loss(w): 0.123373]\n",
      "Epoch 186/10001\n",
      "[D valid loss: 0.109052],[D fake loss: 0.000000], [G loss(mse): 0.157316, G loss(w): 0.123767]\n",
      "Epoch 187/10001\n",
      "[D valid loss: 0.109005],[D fake loss: 0.000000], [G loss(mse): 0.160162, G loss(w): 0.125520]\n",
      "Epoch 188/10001\n",
      "[D valid loss: 0.107042],[D fake loss: 0.000000], [G loss(mse): 0.165127, G loss(w): 0.125654]\n",
      "Epoch 189/10001\n",
      "[D valid loss: 0.109878],[D fake loss: 0.000000], [G loss(mse): 0.162751, G loss(w): 0.128824]\n",
      "Epoch 190/10001\n",
      "[D valid loss: 0.105945],[D fake loss: 0.000000], [G loss(mse): 0.155527, G loss(w): 0.120400]\n",
      "Epoch 191/10001\n",
      "[D valid loss: 0.106278],[D fake loss: 0.000000], [G loss(mse): 0.156283, G loss(w): 0.123145]\n",
      "Epoch 192/10001\n",
      "[D valid loss: 0.104556],[D fake loss: 0.000000], [G loss(mse): 0.152961, G loss(w): 0.122073]\n",
      "Epoch 193/10001\n",
      "[D valid loss: 0.106747],[D fake loss: 0.000000], [G loss(mse): 0.153597, G loss(w): 0.120967]\n",
      "Epoch 194/10001\n",
      "[D valid loss: 0.102700],[D fake loss: 0.000000], [G loss(mse): 0.143762, G loss(w): 0.114637]\n",
      "Epoch 195/10001\n",
      "[D valid loss: 0.103308],[D fake loss: 0.000000], [G loss(mse): 0.154527, G loss(w): 0.124181]\n",
      "Epoch 196/10001\n",
      "[D valid loss: 0.103631],[D fake loss: 0.000000], [G loss(mse): 0.151604, G loss(w): 0.117858]\n",
      "Epoch 197/10001\n",
      "[D valid loss: 0.103409],[D fake loss: 0.000000], [G loss(mse): 0.152108, G loss(w): 0.121469]\n",
      "Epoch 198/10001\n",
      "[D valid loss: 0.102015],[D fake loss: 0.000000], [G loss(mse): 0.144830, G loss(w): 0.113139]\n",
      "Epoch 199/10001\n",
      "[D valid loss: 0.100567],[D fake loss: 0.000000], [G loss(mse): 0.152938, G loss(w): 0.121179]\n",
      "Epoch 200/10001\n",
      "[D valid loss: 0.103014],[D fake loss: 0.000000], [G loss(mse): 0.144845, G loss(w): 0.114702]\n",
      "Epoch 201/10001\n",
      "[D valid loss: 0.100104],[D fake loss: 0.000000], [G loss(mse): 0.152494, G loss(w): 0.121763]\n",
      "Epoch 202/10001\n",
      "[D valid loss: 0.098174],[D fake loss: 0.000000], [G loss(mse): 0.143229, G loss(w): 0.116335]\n",
      "Epoch 203/10001\n",
      "[D valid loss: 0.098322],[D fake loss: 0.000000], [G loss(mse): 0.145089, G loss(w): 0.116751]\n",
      "Epoch 204/10001\n",
      "[D valid loss: 0.097323],[D fake loss: 0.000000], [G loss(mse): 0.147525, G loss(w): 0.116688]\n",
      "Epoch 205/10001\n",
      "[D valid loss: 0.096137],[D fake loss: 0.000000], [G loss(mse): 0.146015, G loss(w): 0.116019]\n",
      "Epoch 206/10001\n",
      "[D valid loss: 0.096551],[D fake loss: 0.000000], [G loss(mse): 0.146278, G loss(w): 0.114804]\n",
      "Epoch 207/10001\n",
      "[D valid loss: 0.098433],[D fake loss: 0.000000], [G loss(mse): 0.140514, G loss(w): 0.111904]\n",
      "Epoch 208/10001\n",
      "[D valid loss: 0.097153],[D fake loss: 0.000000], [G loss(mse): 0.144451, G loss(w): 0.114985]\n",
      "Epoch 209/10001\n",
      "[D valid loss: 0.094561],[D fake loss: 0.000000], [G loss(mse): 0.146650, G loss(w): 0.115465]\n",
      "Epoch 210/10001\n",
      "[D valid loss: 0.093821],[D fake loss: 0.000000], [G loss(mse): 0.137682, G loss(w): 0.109822]\n",
      "Epoch 211/10001\n",
      "[D valid loss: 0.094813],[D fake loss: 0.000000], [G loss(mse): 0.133359, G loss(w): 0.101923]\n",
      "Epoch 212/10001\n",
      "[D valid loss: 0.093522],[D fake loss: 0.000000], [G loss(mse): 0.138579, G loss(w): 0.110872]\n",
      "Epoch 213/10001\n",
      "[D valid loss: 0.093910],[D fake loss: 0.000000], [G loss(mse): 0.142475, G loss(w): 0.112729]\n",
      "Epoch 214/10001\n",
      "[D valid loss: 0.092020],[D fake loss: 0.000000], [G loss(mse): 0.130744, G loss(w): 0.105555]\n",
      "Epoch 215/10001\n",
      "[D valid loss: 0.091241],[D fake loss: 0.000000], [G loss(mse): 0.135956, G loss(w): 0.106868]\n",
      "Epoch 216/10001\n",
      "[D valid loss: 0.091508],[D fake loss: 0.000000], [G loss(mse): 0.130852, G loss(w): 0.101369]\n",
      "Epoch 217/10001\n",
      "[D valid loss: 0.089625],[D fake loss: 0.000000], [G loss(mse): 0.132045, G loss(w): 0.106827]\n",
      "Epoch 218/10001\n",
      "[D valid loss: 0.091562],[D fake loss: 0.000000], [G loss(mse): 0.136886, G loss(w): 0.109387]\n",
      "Epoch 219/10001\n",
      "[D valid loss: 0.087426],[D fake loss: 0.000000], [G loss(mse): 0.141579, G loss(w): 0.110457]\n",
      "Epoch 220/10001\n",
      "[D valid loss: 0.088226],[D fake loss: 0.000000], [G loss(mse): 0.136492, G loss(w): 0.110210]\n",
      "Epoch 221/10001\n",
      "[D valid loss: 0.088747],[D fake loss: 0.000000], [G loss(mse): 0.126213, G loss(w): 0.101819]\n",
      "Epoch 222/10001\n",
      "[D valid loss: 0.087096],[D fake loss: 0.000000], [G loss(mse): 0.134505, G loss(w): 0.111311]\n",
      "Epoch 223/10001\n",
      "[D valid loss: 0.090422],[D fake loss: 0.000000], [G loss(mse): 0.135013, G loss(w): 0.106259]\n",
      "Epoch 224/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.084549],[D fake loss: 0.000000], [G loss(mse): 0.126928, G loss(w): 0.102080]\n",
      "Epoch 225/10001\n",
      "[D valid loss: 0.085999],[D fake loss: 0.000000], [G loss(mse): 0.136404, G loss(w): 0.107965]\n",
      "Epoch 226/10001\n",
      "[D valid loss: 0.084297],[D fake loss: 0.000000], [G loss(mse): 0.131673, G loss(w): 0.104944]\n",
      "Epoch 227/10001\n",
      "[D valid loss: 0.085971],[D fake loss: 0.000000], [G loss(mse): 0.132989, G loss(w): 0.104885]\n",
      "Epoch 228/10001\n",
      "[D valid loss: 0.084189],[D fake loss: 0.000000], [G loss(mse): 0.132760, G loss(w): 0.104916]\n",
      "Epoch 229/10001\n",
      "[D valid loss: 0.083480],[D fake loss: 0.000000], [G loss(mse): 0.137370, G loss(w): 0.108779]\n",
      "Epoch 230/10001\n",
      "[D valid loss: 0.082876],[D fake loss: 0.000000], [G loss(mse): 0.123457, G loss(w): 0.100048]\n",
      "Epoch 231/10001\n",
      "[D valid loss: 0.083936],[D fake loss: 0.000000], [G loss(mse): 0.126917, G loss(w): 0.101745]\n",
      "Epoch 232/10001\n",
      "[D valid loss: 0.082972],[D fake loss: 0.000000], [G loss(mse): 0.126626, G loss(w): 0.100710]\n",
      "Epoch 233/10001\n",
      "[D valid loss: 0.082267],[D fake loss: 0.000000], [G loss(mse): 0.124974, G loss(w): 0.098615]\n",
      "Epoch 234/10001\n",
      "[D valid loss: 0.081963],[D fake loss: 0.000000], [G loss(mse): 0.133267, G loss(w): 0.106453]\n",
      "Epoch 235/10001\n",
      "[D valid loss: 0.079399],[D fake loss: 0.000000], [G loss(mse): 0.123822, G loss(w): 0.100454]\n",
      "Epoch 236/10001\n",
      "[D valid loss: 0.079625],[D fake loss: 0.000000], [G loss(mse): 0.125169, G loss(w): 0.097009]\n",
      "Epoch 237/10001\n",
      "[D valid loss: 0.078793],[D fake loss: 0.000000], [G loss(mse): 0.124616, G loss(w): 0.099361]\n",
      "Epoch 238/10001\n",
      "[D valid loss: 0.079473],[D fake loss: 0.000000], [G loss(mse): 0.121791, G loss(w): 0.097658]\n",
      "Epoch 239/10001\n",
      "[D valid loss: 0.078310],[D fake loss: 0.000000], [G loss(mse): 0.119572, G loss(w): 0.094847]\n",
      "Epoch 240/10001\n",
      "[D valid loss: 0.079114],[D fake loss: 0.000000], [G loss(mse): 0.118632, G loss(w): 0.094373]\n",
      "Epoch 241/10001\n",
      "[D valid loss: 0.077363],[D fake loss: 0.000000], [G loss(mse): 0.119874, G loss(w): 0.096317]\n",
      "Epoch 242/10001\n",
      "[D valid loss: 0.076323],[D fake loss: 0.000000], [G loss(mse): 0.123139, G loss(w): 0.102640]\n",
      "Epoch 243/10001\n",
      "[D valid loss: 0.078876],[D fake loss: 0.000000], [G loss(mse): 0.124541, G loss(w): 0.098298]\n",
      "Epoch 244/10001\n",
      "[D valid loss: 0.075557],[D fake loss: 0.000000], [G loss(mse): 0.117155, G loss(w): 0.093839]\n",
      "Epoch 245/10001\n",
      "[D valid loss: 0.075043],[D fake loss: 0.000000], [G loss(mse): 0.121328, G loss(w): 0.096981]\n",
      "Epoch 246/10001\n",
      "[D valid loss: 0.074847],[D fake loss: 0.000000], [G loss(mse): 0.120611, G loss(w): 0.100185]\n",
      "Epoch 247/10001\n",
      "[D valid loss: 0.075810],[D fake loss: 0.000000], [G loss(mse): 0.122174, G loss(w): 0.097086]\n",
      "Epoch 248/10001\n",
      "[D valid loss: 0.075386],[D fake loss: 0.000000], [G loss(mse): 0.115839, G loss(w): 0.093360]\n",
      "Epoch 249/10001\n",
      "[D valid loss: 0.073449],[D fake loss: 0.000000], [G loss(mse): 0.114024, G loss(w): 0.092092]\n",
      "Epoch 250/10001\n",
      "[D valid loss: 0.073094],[D fake loss: 0.000000], [G loss(mse): 0.111804, G loss(w): 0.091683]\n",
      "Epoch 251/10001\n",
      "[D valid loss: 0.074803],[D fake loss: 0.000000], [G loss(mse): 0.114491, G loss(w): 0.092786]\n",
      "Epoch 252/10001\n",
      "[D valid loss: 0.072921],[D fake loss: 0.000000], [G loss(mse): 0.121051, G loss(w): 0.098087]\n",
      "Epoch 253/10001\n",
      "[D valid loss: 0.071941],[D fake loss: 0.000000], [G loss(mse): 0.119172, G loss(w): 0.096969]\n",
      "Epoch 254/10001\n",
      "[D valid loss: 0.073603],[D fake loss: 0.000000], [G loss(mse): 0.118197, G loss(w): 0.096209]\n",
      "Epoch 255/10001\n",
      "[D valid loss: 0.069469],[D fake loss: 0.000000], [G loss(mse): 0.116241, G loss(w): 0.092835]\n",
      "Epoch 256/10001\n",
      "[D valid loss: 0.071862],[D fake loss: 0.000000], [G loss(mse): 0.112836, G loss(w): 0.090534]\n",
      "Epoch 257/10001\n",
      "[D valid loss: 0.074826],[D fake loss: 0.000000], [G loss(mse): 0.114091, G loss(w): 0.090816]\n",
      "Epoch 258/10001\n",
      "[D valid loss: 0.068494],[D fake loss: 0.000000], [G loss(mse): 0.112787, G loss(w): 0.091313]\n",
      "Epoch 259/10001\n",
      "[D valid loss: 0.068748],[D fake loss: 0.000000], [G loss(mse): 0.109250, G loss(w): 0.089529]\n",
      "Epoch 260/10001\n",
      "[D valid loss: 0.068536],[D fake loss: 0.000000], [G loss(mse): 0.115810, G loss(w): 0.090996]\n",
      "Epoch 261/10001\n",
      "[D valid loss: 0.068468],[D fake loss: 0.000000], [G loss(mse): 0.105416, G loss(w): 0.084631]\n",
      "Epoch 262/10001\n",
      "[D valid loss: 0.069516],[D fake loss: 0.000000], [G loss(mse): 0.109255, G loss(w): 0.089018]\n",
      "Epoch 263/10001\n",
      "[D valid loss: 0.068834],[D fake loss: 0.000000], [G loss(mse): 0.112263, G loss(w): 0.092183]\n",
      "Epoch 264/10001\n",
      "[D valid loss: 0.066860],[D fake loss: 0.000000], [G loss(mse): 0.108326, G loss(w): 0.088087]\n",
      "Epoch 265/10001\n",
      "[D valid loss: 0.067627],[D fake loss: 0.000000], [G loss(mse): 0.106787, G loss(w): 0.085983]\n",
      "Epoch 266/10001\n",
      "[D valid loss: 0.067158],[D fake loss: 0.000000], [G loss(mse): 0.108320, G loss(w): 0.089579]\n",
      "Epoch 267/10001\n",
      "[D valid loss: 0.065773],[D fake loss: 0.000000], [G loss(mse): 0.103480, G loss(w): 0.085336]\n",
      "Epoch 268/10001\n",
      "[D valid loss: 0.066969],[D fake loss: 0.000000], [G loss(mse): 0.107431, G loss(w): 0.088658]\n",
      "Epoch 269/10001\n",
      "[D valid loss: 0.067902],[D fake loss: 0.000000], [G loss(mse): 0.110265, G loss(w): 0.088273]\n",
      "Epoch 270/10001\n",
      "[D valid loss: 0.065286],[D fake loss: 0.000000], [G loss(mse): 0.104638, G loss(w): 0.083966]\n",
      "Epoch 271/10001\n",
      "[D valid loss: 0.067494],[D fake loss: 0.000000], [G loss(mse): 0.106887, G loss(w): 0.086395]\n",
      "Epoch 272/10001\n",
      "[D valid loss: 0.064130],[D fake loss: 0.000000], [G loss(mse): 0.103666, G loss(w): 0.084693]\n",
      "Epoch 273/10001\n",
      "[D valid loss: 0.063806],[D fake loss: 0.000000], [G loss(mse): 0.112130, G loss(w): 0.090362]\n",
      "Epoch 274/10001\n",
      "[D valid loss: 0.065485],[D fake loss: 0.000000], [G loss(mse): 0.110681, G loss(w): 0.091117]\n",
      "Epoch 275/10001\n",
      "[D valid loss: 0.061704],[D fake loss: 0.000000], [G loss(mse): 0.103917, G loss(w): 0.082979]\n",
      "Epoch 276/10001\n",
      "[D valid loss: 0.063506],[D fake loss: 0.000000], [G loss(mse): 0.103780, G loss(w): 0.084786]\n",
      "Epoch 277/10001\n",
      "[D valid loss: 0.063107],[D fake loss: 0.000000], [G loss(mse): 0.105815, G loss(w): 0.087856]\n",
      "Epoch 278/10001\n",
      "[D valid loss: 0.062916],[D fake loss: 0.000000], [G loss(mse): 0.106352, G loss(w): 0.084653]\n",
      "Epoch 279/10001\n",
      "[D valid loss: 0.062752],[D fake loss: 0.000000], [G loss(mse): 0.106709, G loss(w): 0.088552]\n",
      "Epoch 280/10001\n",
      "[D valid loss: 0.062170],[D fake loss: 0.000000], [G loss(mse): 0.111674, G loss(w): 0.091721]\n",
      "Epoch 281/10001\n",
      "[D valid loss: 0.062462],[D fake loss: 0.000000], [G loss(mse): 0.100149, G loss(w): 0.080951]\n",
      "Epoch 282/10001\n",
      "[D valid loss: 0.061315],[D fake loss: 0.000000], [G loss(mse): 0.104254, G loss(w): 0.082895]\n",
      "Epoch 283/10001\n",
      "[D valid loss: 0.060347],[D fake loss: 0.000000], [G loss(mse): 0.102124, G loss(w): 0.083348]\n",
      "Epoch 284/10001\n",
      "[D valid loss: 0.059845],[D fake loss: 0.000000], [G loss(mse): 0.098758, G loss(w): 0.082545]\n",
      "Epoch 285/10001\n",
      "[D valid loss: 0.060255],[D fake loss: 0.000000], [G loss(mse): 0.098398, G loss(w): 0.078180]\n",
      "Epoch 286/10001\n",
      "[D valid loss: 0.061782],[D fake loss: 0.000000], [G loss(mse): 0.103575, G loss(w): 0.085610]\n",
      "Epoch 287/10001\n",
      "[D valid loss: 0.060365],[D fake loss: 0.000000], [G loss(mse): 0.101508, G loss(w): 0.083117]\n",
      "Epoch 288/10001\n",
      "[D valid loss: 0.056433],[D fake loss: 0.000000], [G loss(mse): 0.104858, G loss(w): 0.084810]\n",
      "Epoch 289/10001\n",
      "[D valid loss: 0.060142],[D fake loss: 0.000000], [G loss(mse): 0.099810, G loss(w): 0.082095]\n",
      "Epoch 290/10001\n",
      "[D valid loss: 0.057260],[D fake loss: 0.000000], [G loss(mse): 0.099501, G loss(w): 0.083840]\n",
      "Epoch 291/10001\n",
      "[D valid loss: 0.058995],[D fake loss: 0.000000], [G loss(mse): 0.094142, G loss(w): 0.076835]\n",
      "Epoch 292/10001\n",
      "[D valid loss: 0.057539],[D fake loss: 0.000000], [G loss(mse): 0.096436, G loss(w): 0.079089]\n",
      "Epoch 293/10001\n",
      "[D valid loss: 0.056903],[D fake loss: 0.000000], [G loss(mse): 0.096937, G loss(w): 0.078634]\n",
      "Epoch 294/10001\n",
      "[D valid loss: 0.056681],[D fake loss: 0.000000], [G loss(mse): 0.105465, G loss(w): 0.085935]\n",
      "Epoch 295/10001\n",
      "[D valid loss: 0.056264],[D fake loss: 0.000000], [G loss(mse): 0.094171, G loss(w): 0.076814]\n",
      "Epoch 296/10001\n",
      "[D valid loss: 0.056800],[D fake loss: 0.000000], [G loss(mse): 0.093493, G loss(w): 0.074310]\n",
      "Epoch 297/10001\n",
      "[D valid loss: 0.056185],[D fake loss: 0.000000], [G loss(mse): 0.101803, G loss(w): 0.080115]\n",
      "Epoch 298/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.055713],[D fake loss: 0.000000], [G loss(mse): 0.098363, G loss(w): 0.080717]\n",
      "Epoch 299/10001\n",
      "[D valid loss: 0.055420],[D fake loss: 0.000000], [G loss(mse): 0.093404, G loss(w): 0.076125]\n",
      "Epoch 300/10001\n",
      "[D valid loss: 0.055420],[D fake loss: 0.000000], [G loss(mse): 0.097954, G loss(w): 0.078008]\n",
      "Epoch 301/10001\n",
      "[D valid loss: 0.055193],[D fake loss: 0.000000], [G loss(mse): 0.098667, G loss(w): 0.078650]\n",
      "Epoch 302/10001\n",
      "[D valid loss: 0.053447],[D fake loss: 0.000000], [G loss(mse): 0.095366, G loss(w): 0.078797]\n",
      "Epoch 303/10001\n",
      "[D valid loss: 0.054822],[D fake loss: 0.000000], [G loss(mse): 0.087876, G loss(w): 0.072997]\n",
      "Epoch 304/10001\n",
      "[D valid loss: 0.052398],[D fake loss: 0.000000], [G loss(mse): 0.094674, G loss(w): 0.077546]\n",
      "Epoch 305/10001\n",
      "[D valid loss: 0.054167],[D fake loss: 0.000000], [G loss(mse): 0.094633, G loss(w): 0.075121]\n",
      "Epoch 306/10001\n",
      "[D valid loss: 0.054996],[D fake loss: 0.000000], [G loss(mse): 0.098600, G loss(w): 0.078720]\n",
      "Epoch 307/10001\n",
      "[D valid loss: 0.052413],[D fake loss: 0.000000], [G loss(mse): 0.089981, G loss(w): 0.074183]\n",
      "Epoch 308/10001\n",
      "[D valid loss: 0.051981],[D fake loss: 0.000000], [G loss(mse): 0.092360, G loss(w): 0.077232]\n",
      "Epoch 309/10001\n",
      "[D valid loss: 0.052729],[D fake loss: 0.000000], [G loss(mse): 0.090522, G loss(w): 0.074127]\n",
      "Epoch 310/10001\n",
      "[D valid loss: 0.052399],[D fake loss: 0.000000], [G loss(mse): 0.091318, G loss(w): 0.072850]\n",
      "Epoch 311/10001\n",
      "[D valid loss: 0.051335],[D fake loss: 0.000000], [G loss(mse): 0.092700, G loss(w): 0.076524]\n",
      "Epoch 312/10001\n",
      "[D valid loss: 0.052191],[D fake loss: 0.000000], [G loss(mse): 0.093026, G loss(w): 0.076147]\n",
      "Epoch 313/10001\n",
      "[D valid loss: 0.051751],[D fake loss: 0.000000], [G loss(mse): 0.090828, G loss(w): 0.075146]\n",
      "Epoch 314/10001\n",
      "[D valid loss: 0.050971],[D fake loss: 0.000000], [G loss(mse): 0.095304, G loss(w): 0.079319]\n",
      "Epoch 315/10001\n",
      "[D valid loss: 0.050436],[D fake loss: 0.000000], [G loss(mse): 0.090934, G loss(w): 0.073540]\n",
      "Epoch 316/10001\n",
      "[D valid loss: 0.049792],[D fake loss: 0.000000], [G loss(mse): 0.087915, G loss(w): 0.073501]\n",
      "Epoch 317/10001\n",
      "[D valid loss: 0.051343],[D fake loss: 0.000000], [G loss(mse): 0.085698, G loss(w): 0.070016]\n",
      "Epoch 318/10001\n",
      "[D valid loss: 0.048979],[D fake loss: 0.000000], [G loss(mse): 0.089914, G loss(w): 0.074896]\n",
      "Epoch 319/10001\n",
      "[D valid loss: 0.050000],[D fake loss: 0.000000], [G loss(mse): 0.086009, G loss(w): 0.071923]\n",
      "Epoch 320/10001\n",
      "[D valid loss: 0.050072],[D fake loss: 0.000000], [G loss(mse): 0.085127, G loss(w): 0.070560]\n",
      "Epoch 321/10001\n",
      "[D valid loss: 0.050748],[D fake loss: 0.000000], [G loss(mse): 0.088305, G loss(w): 0.072659]\n",
      "Epoch 322/10001\n",
      "[D valid loss: 0.048264],[D fake loss: 0.000000], [G loss(mse): 0.088916, G loss(w): 0.073079]\n",
      "Epoch 323/10001\n",
      "[D valid loss: 0.049856],[D fake loss: 0.000000], [G loss(mse): 0.087714, G loss(w): 0.072887]\n",
      "Epoch 324/10001\n",
      "[D valid loss: 0.049155],[D fake loss: 0.000000], [G loss(mse): 0.087158, G loss(w): 0.071981]\n",
      "Epoch 325/10001\n",
      "[D valid loss: 0.048051],[D fake loss: 0.000000], [G loss(mse): 0.088603, G loss(w): 0.072293]\n",
      "Epoch 326/10001\n",
      "[D valid loss: 0.048004],[D fake loss: 0.000000], [G loss(mse): 0.084006, G loss(w): 0.067777]\n",
      "Epoch 327/10001\n",
      "[D valid loss: 0.048411],[D fake loss: 0.000000], [G loss(mse): 0.081512, G loss(w): 0.067728]\n",
      "Epoch 328/10001\n",
      "[D valid loss: 0.049243],[D fake loss: 0.000000], [G loss(mse): 0.085629, G loss(w): 0.070450]\n",
      "Epoch 329/10001\n",
      "[D valid loss: 0.046280],[D fake loss: 0.000000], [G loss(mse): 0.085086, G loss(w): 0.070393]\n",
      "Epoch 330/10001\n",
      "[D valid loss: 0.045329],[D fake loss: 0.000000], [G loss(mse): 0.083719, G loss(w): 0.069050]\n",
      "Epoch 331/10001\n",
      "[D valid loss: 0.048296],[D fake loss: 0.000000], [G loss(mse): 0.079363, G loss(w): 0.066077]\n",
      "Epoch 332/10001\n",
      "[D valid loss: 0.046116],[D fake loss: 0.000000], [G loss(mse): 0.082056, G loss(w): 0.068813]\n",
      "Epoch 333/10001\n",
      "[D valid loss: 0.046325],[D fake loss: 0.000000], [G loss(mse): 0.082967, G loss(w): 0.069250]\n",
      "Epoch 334/10001\n",
      "[D valid loss: 0.046702],[D fake loss: 0.000000], [G loss(mse): 0.083601, G loss(w): 0.067257]\n",
      "Epoch 335/10001\n",
      "[D valid loss: 0.045539],[D fake loss: 0.000000], [G loss(mse): 0.091621, G loss(w): 0.074240]\n",
      "Epoch 336/10001\n",
      "[D valid loss: 0.044255],[D fake loss: 0.000000], [G loss(mse): 0.083806, G loss(w): 0.069814]\n",
      "Epoch 337/10001\n",
      "[D valid loss: 0.044994],[D fake loss: 0.000000], [G loss(mse): 0.081599, G loss(w): 0.068046]\n",
      "Epoch 338/10001\n",
      "[D valid loss: 0.046313],[D fake loss: 0.000000], [G loss(mse): 0.084909, G loss(w): 0.069811]\n",
      "Epoch 339/10001\n",
      "[D valid loss: 0.044252],[D fake loss: 0.000000], [G loss(mse): 0.078345, G loss(w): 0.065768]\n",
      "Epoch 340/10001\n",
      "[D valid loss: 0.044805],[D fake loss: 0.000000], [G loss(mse): 0.083932, G loss(w): 0.069860]\n",
      "Epoch 341/10001\n",
      "[D valid loss: 0.043395],[D fake loss: 0.000000], [G loss(mse): 0.079105, G loss(w): 0.067718]\n",
      "Epoch 342/10001\n",
      "[D valid loss: 0.045573],[D fake loss: 0.000000], [G loss(mse): 0.083272, G loss(w): 0.070405]\n",
      "Epoch 343/10001\n",
      "[D valid loss: 0.044911],[D fake loss: 0.000000], [G loss(mse): 0.081869, G loss(w): 0.068148]\n",
      "Epoch 344/10001\n",
      "[D valid loss: 0.043448],[D fake loss: 0.000000], [G loss(mse): 0.078151, G loss(w): 0.065129]\n",
      "Epoch 345/10001\n",
      "[D valid loss: 0.042164],[D fake loss: 0.000000], [G loss(mse): 0.077554, G loss(w): 0.065025]\n",
      "Epoch 346/10001\n",
      "[D valid loss: 0.044177],[D fake loss: 0.000000], [G loss(mse): 0.081426, G loss(w): 0.066851]\n",
      "Epoch 347/10001\n",
      "[D valid loss: 0.044882],[D fake loss: 0.000000], [G loss(mse): 0.083554, G loss(w): 0.068296]\n",
      "Epoch 348/10001\n",
      "[D valid loss: 0.043009],[D fake loss: 0.000000], [G loss(mse): 0.081266, G loss(w): 0.067524]\n",
      "Epoch 349/10001\n",
      "[D valid loss: 0.042159],[D fake loss: 0.000000], [G loss(mse): 0.079915, G loss(w): 0.066456]\n",
      "Epoch 350/10001\n",
      "[D valid loss: 0.042085],[D fake loss: 0.000000], [G loss(mse): 0.080593, G loss(w): 0.066528]\n",
      "Epoch 351/10001\n",
      "[D valid loss: 0.042832],[D fake loss: 0.000000], [G loss(mse): 0.078985, G loss(w): 0.065646]\n",
      "Epoch 352/10001\n",
      "[D valid loss: 0.041137],[D fake loss: 0.000000], [G loss(mse): 0.079877, G loss(w): 0.068096]\n",
      "Epoch 353/10001\n",
      "[D valid loss: 0.043350],[D fake loss: 0.000000], [G loss(mse): 0.077196, G loss(w): 0.064919]\n",
      "Epoch 354/10001\n",
      "[D valid loss: 0.042265],[D fake loss: 0.000000], [G loss(mse): 0.077884, G loss(w): 0.065254]\n",
      "Epoch 355/10001\n",
      "[D valid loss: 0.041378],[D fake loss: 0.000000], [G loss(mse): 0.076190, G loss(w): 0.064346]\n",
      "Epoch 356/10001\n",
      "[D valid loss: 0.040925],[D fake loss: 0.000000], [G loss(mse): 0.077490, G loss(w): 0.064806]\n",
      "Epoch 357/10001\n",
      "[D valid loss: 0.042027],[D fake loss: 0.000000], [G loss(mse): 0.076362, G loss(w): 0.064578]\n",
      "Epoch 358/10001\n",
      "[D valid loss: 0.040526],[D fake loss: 0.000000], [G loss(mse): 0.079240, G loss(w): 0.067648]\n",
      "Epoch 359/10001\n",
      "[D valid loss: 0.040500],[D fake loss: 0.000000], [G loss(mse): 0.075664, G loss(w): 0.063568]\n",
      "Epoch 360/10001\n",
      "[D valid loss: 0.040043],[D fake loss: 0.000000], [G loss(mse): 0.073717, G loss(w): 0.063431]\n",
      "Epoch 361/10001\n",
      "[D valid loss: 0.041057],[D fake loss: 0.000000], [G loss(mse): 0.076815, G loss(w): 0.065312]\n",
      "Epoch 362/10001\n",
      "[D valid loss: 0.042858],[D fake loss: 0.000000], [G loss(mse): 0.072903, G loss(w): 0.062242]\n",
      "Epoch 363/10001\n",
      "[D valid loss: 0.038682],[D fake loss: 0.000000], [G loss(mse): 0.074849, G loss(w): 0.062497]\n",
      "Epoch 364/10001\n",
      "[D valid loss: 0.039665],[D fake loss: 0.000000], [G loss(mse): 0.078353, G loss(w): 0.065393]\n",
      "Epoch 365/10001\n",
      "[D valid loss: 0.038788],[D fake loss: 0.000000], [G loss(mse): 0.077166, G loss(w): 0.065860]\n",
      "Epoch 366/10001\n",
      "[D valid loss: 0.039832],[D fake loss: 0.000000], [G loss(mse): 0.075163, G loss(w): 0.064906]\n",
      "Epoch 367/10001\n",
      "[D valid loss: 0.039629],[D fake loss: 0.000000], [G loss(mse): 0.070473, G loss(w): 0.057932]\n",
      "Epoch 368/10001\n",
      "[D valid loss: 0.039595],[D fake loss: 0.000000], [G loss(mse): 0.077812, G loss(w): 0.064309]\n",
      "Epoch 369/10001\n",
      "[D valid loss: 0.037189],[D fake loss: 0.000000], [G loss(mse): 0.070436, G loss(w): 0.059897]\n",
      "Epoch 370/10001\n",
      "[D valid loss: 0.038357],[D fake loss: 0.000000], [G loss(mse): 0.076578, G loss(w): 0.065433]\n",
      "Epoch 371/10001\n",
      "[D valid loss: 0.038531],[D fake loss: 0.000000], [G loss(mse): 0.076174, G loss(w): 0.062976]\n",
      "Epoch 372/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.040744],[D fake loss: 0.000000], [G loss(mse): 0.071102, G loss(w): 0.061018]\n",
      "Epoch 373/10001\n",
      "[D valid loss: 0.037967],[D fake loss: 0.000000], [G loss(mse): 0.073880, G loss(w): 0.062362]\n",
      "Epoch 374/10001\n",
      "[D valid loss: 0.037230],[D fake loss: 0.000000], [G loss(mse): 0.073660, G loss(w): 0.061847]\n",
      "Epoch 375/10001\n",
      "[D valid loss: 0.037847],[D fake loss: 0.000000], [G loss(mse): 0.073756, G loss(w): 0.060539]\n",
      "Epoch 376/10001\n",
      "[D valid loss: 0.037795],[D fake loss: 0.000000], [G loss(mse): 0.074664, G loss(w): 0.061656]\n",
      "Epoch 377/10001\n",
      "[D valid loss: 0.038248],[D fake loss: 0.000000], [G loss(mse): 0.072687, G loss(w): 0.060560]\n",
      "Epoch 378/10001\n",
      "[D valid loss: 0.036534],[D fake loss: 0.000000], [G loss(mse): 0.070863, G loss(w): 0.059573]\n",
      "Epoch 379/10001\n",
      "[D valid loss: 0.035228],[D fake loss: 0.000000], [G loss(mse): 0.067923, G loss(w): 0.058213]\n",
      "Epoch 380/10001\n",
      "[D valid loss: 0.037903],[D fake loss: 0.000000], [G loss(mse): 0.073247, G loss(w): 0.061999]\n",
      "Epoch 381/10001\n",
      "[D valid loss: 0.036693],[D fake loss: 0.000000], [G loss(mse): 0.064497, G loss(w): 0.054472]\n",
      "Epoch 382/10001\n",
      "[D valid loss: 0.036381],[D fake loss: 0.000000], [G loss(mse): 0.069013, G loss(w): 0.059196]\n",
      "Epoch 383/10001\n",
      "[D valid loss: 0.035645],[D fake loss: 0.000000], [G loss(mse): 0.069216, G loss(w): 0.058901]\n",
      "Epoch 384/10001\n",
      "[D valid loss: 0.036209],[D fake loss: 0.000000], [G loss(mse): 0.069741, G loss(w): 0.058421]\n",
      "Epoch 385/10001\n",
      "[D valid loss: 0.036363],[D fake loss: 0.000000], [G loss(mse): 0.068040, G loss(w): 0.056603]\n",
      "Epoch 386/10001\n",
      "[D valid loss: 0.035368],[D fake loss: 0.000000], [G loss(mse): 0.068656, G loss(w): 0.058165]\n",
      "Epoch 387/10001\n",
      "[D valid loss: 0.034491],[D fake loss: 0.000000], [G loss(mse): 0.066227, G loss(w): 0.057496]\n",
      "Epoch 388/10001\n",
      "[D valid loss: 0.035335],[D fake loss: 0.000000], [G loss(mse): 0.067739, G loss(w): 0.057531]\n",
      "Epoch 389/10001\n",
      "[D valid loss: 0.035319],[D fake loss: 0.000000], [G loss(mse): 0.065615, G loss(w): 0.055967]\n",
      "Epoch 390/10001\n",
      "[D valid loss: 0.035949],[D fake loss: 0.000000], [G loss(mse): 0.068654, G loss(w): 0.056878]\n",
      "Epoch 391/10001\n",
      "[D valid loss: 0.036502],[D fake loss: 0.000000], [G loss(mse): 0.066069, G loss(w): 0.057283]\n",
      "Epoch 392/10001\n",
      "[D valid loss: 0.035112],[D fake loss: 0.000000], [G loss(mse): 0.068542, G loss(w): 0.058855]\n",
      "Epoch 393/10001\n",
      "[D valid loss: 0.032508],[D fake loss: 0.000000], [G loss(mse): 0.070210, G loss(w): 0.058813]\n",
      "Epoch 394/10001\n",
      "[D valid loss: 0.034538],[D fake loss: 0.000000], [G loss(mse): 0.065947, G loss(w): 0.055523]\n",
      "Epoch 395/10001\n",
      "[D valid loss: 0.035117],[D fake loss: 0.000000], [G loss(mse): 0.072049, G loss(w): 0.060568]\n",
      "Epoch 396/10001\n",
      "[D valid loss: 0.032325],[D fake loss: 0.000000], [G loss(mse): 0.066379, G loss(w): 0.057875]\n",
      "Epoch 397/10001\n",
      "[D valid loss: 0.034748],[D fake loss: 0.000000], [G loss(mse): 0.066298, G loss(w): 0.056285]\n",
      "Epoch 398/10001\n",
      "[D valid loss: 0.033665],[D fake loss: 0.000000], [G loss(mse): 0.065672, G loss(w): 0.056710]\n",
      "Epoch 399/10001\n",
      "[D valid loss: 0.032828],[D fake loss: 0.000000], [G loss(mse): 0.069062, G loss(w): 0.058512]\n",
      "Epoch 400/10001\n",
      "[D valid loss: 0.033066],[D fake loss: 0.000000], [G loss(mse): 0.065597, G loss(w): 0.054963]\n",
      "Epoch 401/10001\n",
      "[D valid loss: 0.031548],[D fake loss: 0.000000], [G loss(mse): 0.067023, G loss(w): 0.057283]\n",
      "Epoch 402/10001\n",
      "[D valid loss: 0.032363],[D fake loss: 0.000000], [G loss(mse): 0.063316, G loss(w): 0.054891]\n",
      "Epoch 403/10001\n",
      "[D valid loss: 0.031819],[D fake loss: 0.000000], [G loss(mse): 0.068245, G loss(w): 0.058211]\n",
      "Epoch 404/10001\n",
      "[D valid loss: 0.031709],[D fake loss: 0.000000], [G loss(mse): 0.067755, G loss(w): 0.058643]\n",
      "Epoch 405/10001\n",
      "[D valid loss: 0.031904],[D fake loss: 0.000000], [G loss(mse): 0.062065, G loss(w): 0.053133]\n",
      "Epoch 406/10001\n",
      "[D valid loss: 0.031876],[D fake loss: 0.000000], [G loss(mse): 0.063016, G loss(w): 0.053350]\n",
      "Epoch 407/10001\n",
      "[D valid loss: 0.033877],[D fake loss: 0.000000], [G loss(mse): 0.061138, G loss(w): 0.053124]\n",
      "Epoch 408/10001\n",
      "[D valid loss: 0.032783],[D fake loss: 0.000000], [G loss(mse): 0.069810, G loss(w): 0.058359]\n",
      "Epoch 409/10001\n",
      "[D valid loss: 0.032126],[D fake loss: 0.000000], [G loss(mse): 0.064362, G loss(w): 0.054246]\n",
      "Epoch 410/10001\n",
      "[D valid loss: 0.033586],[D fake loss: 0.000000], [G loss(mse): 0.064619, G loss(w): 0.055252]\n",
      "Epoch 411/10001\n",
      "[D valid loss: 0.030708],[D fake loss: 0.000000], [G loss(mse): 0.063455, G loss(w): 0.052945]\n",
      "Epoch 412/10001\n",
      "[D valid loss: 0.031350],[D fake loss: 0.000000], [G loss(mse): 0.063892, G loss(w): 0.055014]\n",
      "Epoch 413/10001\n",
      "[D valid loss: 0.029259],[D fake loss: 0.000000], [G loss(mse): 0.064139, G loss(w): 0.056303]\n",
      "Epoch 414/10001\n",
      "[D valid loss: 0.030722],[D fake loss: 0.000000], [G loss(mse): 0.065941, G loss(w): 0.055673]\n",
      "Epoch 415/10001\n",
      "[D valid loss: 0.031652],[D fake loss: 0.000000], [G loss(mse): 0.063338, G loss(w): 0.054347]\n",
      "Epoch 416/10001\n",
      "[D valid loss: 0.030879],[D fake loss: 0.000000], [G loss(mse): 0.060269, G loss(w): 0.051614]\n",
      "Epoch 417/10001\n",
      "[D valid loss: 0.030733],[D fake loss: 0.000000], [G loss(mse): 0.060187, G loss(w): 0.051030]\n",
      "Epoch 418/10001\n",
      "[D valid loss: 0.030226],[D fake loss: 0.000000], [G loss(mse): 0.062921, G loss(w): 0.053272]\n",
      "Epoch 419/10001\n",
      "[D valid loss: 0.031483],[D fake loss: 0.000000], [G loss(mse): 0.062115, G loss(w): 0.054091]\n",
      "Epoch 420/10001\n",
      "[D valid loss: 0.031380],[D fake loss: 0.000000], [G loss(mse): 0.060323, G loss(w): 0.052836]\n",
      "Epoch 421/10001\n",
      "[D valid loss: 0.028613],[D fake loss: 0.000000], [G loss(mse): 0.062009, G loss(w): 0.054338]\n",
      "Epoch 422/10001\n",
      "[D valid loss: 0.029302],[D fake loss: 0.000000], [G loss(mse): 0.061388, G loss(w): 0.052862]\n",
      "Epoch 423/10001\n",
      "[D valid loss: 0.028715],[D fake loss: 0.000000], [G loss(mse): 0.061048, G loss(w): 0.052107]\n",
      "Epoch 424/10001\n",
      "[D valid loss: 0.029590],[D fake loss: 0.000000], [G loss(mse): 0.060906, G loss(w): 0.052999]\n",
      "Epoch 425/10001\n",
      "[D valid loss: 0.029968],[D fake loss: 0.000000], [G loss(mse): 0.058145, G loss(w): 0.050248]\n",
      "Epoch 426/10001\n",
      "[D valid loss: 0.030038],[D fake loss: 0.000000], [G loss(mse): 0.055795, G loss(w): 0.048750]\n",
      "Epoch 427/10001\n",
      "[D valid loss: 0.027974],[D fake loss: 0.000000], [G loss(mse): 0.056468, G loss(w): 0.049240]\n",
      "Epoch 428/10001\n",
      "[D valid loss: 0.027826],[D fake loss: 0.000000], [G loss(mse): 0.056091, G loss(w): 0.048708]\n",
      "Epoch 429/10001\n",
      "[D valid loss: 0.028303],[D fake loss: 0.000000], [G loss(mse): 0.061095, G loss(w): 0.052843]\n",
      "Epoch 430/10001\n",
      "[D valid loss: 0.028318],[D fake loss: 0.000000], [G loss(mse): 0.060053, G loss(w): 0.052397]\n",
      "Epoch 431/10001\n",
      "[D valid loss: 0.027841],[D fake loss: 0.000000], [G loss(mse): 0.059867, G loss(w): 0.052720]\n",
      "Epoch 432/10001\n",
      "[D valid loss: 0.027157],[D fake loss: 0.000000], [G loss(mse): 0.063686, G loss(w): 0.053371]\n",
      "Epoch 433/10001\n",
      "[D valid loss: 0.027647],[D fake loss: 0.000000], [G loss(mse): 0.059637, G loss(w): 0.051415]\n",
      "Epoch 434/10001\n",
      "[D valid loss: 0.026444],[D fake loss: 0.000000], [G loss(mse): 0.063197, G loss(w): 0.056465]\n",
      "Epoch 435/10001\n",
      "[D valid loss: 0.027161],[D fake loss: 0.000000], [G loss(mse): 0.058131, G loss(w): 0.050749]\n",
      "Epoch 436/10001\n",
      "[D valid loss: 0.027541],[D fake loss: 0.000000], [G loss(mse): 0.056675, G loss(w): 0.048699]\n",
      "Epoch 437/10001\n",
      "[D valid loss: 0.026874],[D fake loss: 0.000000], [G loss(mse): 0.059137, G loss(w): 0.051447]\n",
      "Epoch 438/10001\n",
      "[D valid loss: 0.026443],[D fake loss: 0.000000], [G loss(mse): 0.058863, G loss(w): 0.051060]\n",
      "Epoch 439/10001\n",
      "[D valid loss: 0.027133],[D fake loss: 0.000000], [G loss(mse): 0.055333, G loss(w): 0.048974]\n",
      "Epoch 440/10001\n",
      "[D valid loss: 0.027062],[D fake loss: 0.000000], [G loss(mse): 0.055935, G loss(w): 0.049045]\n",
      "Epoch 441/10001\n",
      "[D valid loss: 0.027218],[D fake loss: 0.000000], [G loss(mse): 0.057167, G loss(w): 0.050253]\n",
      "Epoch 442/10001\n",
      "[D valid loss: 0.027257],[D fake loss: 0.000000], [G loss(mse): 0.055612, G loss(w): 0.047619]\n",
      "Epoch 443/10001\n",
      "[D valid loss: 0.026026],[D fake loss: 0.000000], [G loss(mse): 0.056577, G loss(w): 0.050189]\n",
      "Epoch 444/10001\n",
      "[D valid loss: 0.026449],[D fake loss: 0.000000], [G loss(mse): 0.056049, G loss(w): 0.048842]\n",
      "Epoch 445/10001\n",
      "[D valid loss: 0.026339],[D fake loss: 0.000000], [G loss(mse): 0.056012, G loss(w): 0.048789]\n",
      "Epoch 446/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.025079],[D fake loss: 0.000000], [G loss(mse): 0.056194, G loss(w): 0.049639]\n",
      "Epoch 447/10001\n",
      "[D valid loss: 0.026788],[D fake loss: 0.000000], [G loss(mse): 0.055313, G loss(w): 0.048069]\n",
      "Epoch 448/10001\n",
      "[D valid loss: 0.025463],[D fake loss: 0.000000], [G loss(mse): 0.054218, G loss(w): 0.048319]\n",
      "Epoch 449/10001\n",
      "[D valid loss: 0.026499],[D fake loss: 0.000000], [G loss(mse): 0.057930, G loss(w): 0.051881]\n",
      "Epoch 450/10001\n",
      "[D valid loss: 0.023170],[D fake loss: 0.000000], [G loss(mse): 0.058964, G loss(w): 0.052813]\n",
      "Epoch 451/10001\n",
      "[D valid loss: 0.025052],[D fake loss: 0.000000], [G loss(mse): 0.057051, G loss(w): 0.048916]\n",
      "Epoch 452/10001\n",
      "[D valid loss: 0.026308],[D fake loss: 0.000000], [G loss(mse): 0.056681, G loss(w): 0.049637]\n",
      "Epoch 453/10001\n",
      "[D valid loss: 0.024370],[D fake loss: 0.000000], [G loss(mse): 0.053515, G loss(w): 0.047302]\n",
      "Epoch 454/10001\n",
      "[D valid loss: 0.024335],[D fake loss: 0.000000], [G loss(mse): 0.055152, G loss(w): 0.049021]\n",
      "Epoch 455/10001\n",
      "[D valid loss: 0.028284],[D fake loss: 0.000000], [G loss(mse): 0.054232, G loss(w): 0.048216]\n",
      "Epoch 456/10001\n",
      "[D valid loss: 0.024226],[D fake loss: 0.000000], [G loss(mse): 0.053358, G loss(w): 0.047381]\n",
      "Epoch 457/10001\n",
      "[D valid loss: 0.024763],[D fake loss: 0.000000], [G loss(mse): 0.057565, G loss(w): 0.048277]\n",
      "Epoch 458/10001\n",
      "[D valid loss: 0.026503],[D fake loss: 0.000000], [G loss(mse): 0.054536, G loss(w): 0.047487]\n",
      "Epoch 459/10001\n",
      "[D valid loss: 0.023047],[D fake loss: 0.000000], [G loss(mse): 0.052768, G loss(w): 0.046751]\n",
      "Epoch 460/10001\n",
      "[D valid loss: 0.023065],[D fake loss: 0.000000], [G loss(mse): 0.053897, G loss(w): 0.048096]\n",
      "Epoch 461/10001\n",
      "[D valid loss: 0.024030],[D fake loss: 0.000000], [G loss(mse): 0.055327, G loss(w): 0.049210]\n",
      "Epoch 462/10001\n",
      "[D valid loss: 0.023592],[D fake loss: 0.000000], [G loss(mse): 0.054567, G loss(w): 0.045757]\n",
      "Epoch 463/10001\n",
      "[D valid loss: 0.023457],[D fake loss: 0.000000], [G loss(mse): 0.052028, G loss(w): 0.046129]\n",
      "Epoch 464/10001\n",
      "[D valid loss: 0.023288],[D fake loss: 0.000000], [G loss(mse): 0.055065, G loss(w): 0.049172]\n",
      "Epoch 465/10001\n",
      "[D valid loss: 0.022623],[D fake loss: 0.000000], [G loss(mse): 0.053873, G loss(w): 0.048035]\n",
      "Epoch 466/10001\n",
      "[D valid loss: 0.025489],[D fake loss: 0.000000], [G loss(mse): 0.050782, G loss(w): 0.045332]\n",
      "Epoch 467/10001\n",
      "[D valid loss: 0.024343],[D fake loss: 0.000000], [G loss(mse): 0.052632, G loss(w): 0.047287]\n",
      "Epoch 468/10001\n",
      "[D valid loss: 0.023177],[D fake loss: 0.000000], [G loss(mse): 0.052066, G loss(w): 0.046378]\n",
      "Epoch 469/10001\n",
      "[D valid loss: 0.025190],[D fake loss: 0.000000], [G loss(mse): 0.052792, G loss(w): 0.046844]\n",
      "Epoch 470/10001\n",
      "[D valid loss: 0.023278],[D fake loss: 0.000000], [G loss(mse): 0.050761, G loss(w): 0.045098]\n",
      "Epoch 471/10001\n",
      "[D valid loss: 0.023934],[D fake loss: 0.000000], [G loss(mse): 0.055583, G loss(w): 0.048561]\n",
      "Epoch 472/10001\n",
      "[D valid loss: 0.024078],[D fake loss: 0.000000], [G loss(mse): 0.054056, G loss(w): 0.046801]\n",
      "Epoch 473/10001\n",
      "[D valid loss: 0.022016],[D fake loss: 0.000000], [G loss(mse): 0.052536, G loss(w): 0.046870]\n",
      "Epoch 474/10001\n",
      "[D valid loss: 0.025025],[D fake loss: 0.000000], [G loss(mse): 0.052005, G loss(w): 0.046456]\n",
      "Epoch 475/10001\n",
      "[D valid loss: 0.023288],[D fake loss: 0.000000], [G loss(mse): 0.053749, G loss(w): 0.047512]\n",
      "Epoch 476/10001\n",
      "[D valid loss: 0.021832],[D fake loss: 0.000000], [G loss(mse): 0.053059, G loss(w): 0.047364]\n",
      "Epoch 477/10001\n",
      "[D valid loss: 0.021139],[D fake loss: 0.000000], [G loss(mse): 0.048408, G loss(w): 0.042719]\n",
      "Epoch 478/10001\n",
      "[D valid loss: 0.023202],[D fake loss: 0.000000], [G loss(mse): 0.050278, G loss(w): 0.044831]\n",
      "Epoch 479/10001\n",
      "[D valid loss: 0.022060],[D fake loss: 0.000000], [G loss(mse): 0.053630, G loss(w): 0.046519]\n",
      "Epoch 480/10001\n",
      "[D valid loss: 0.021261],[D fake loss: 0.000000], [G loss(mse): 0.052634, G loss(w): 0.045822]\n",
      "Epoch 481/10001\n",
      "[D valid loss: 0.021477],[D fake loss: 0.000000], [G loss(mse): 0.052692, G loss(w): 0.047131]\n",
      "Epoch 482/10001\n",
      "[D valid loss: 0.020465],[D fake loss: 0.000000], [G loss(mse): 0.051373, G loss(w): 0.045548]\n",
      "Epoch 483/10001\n",
      "[D valid loss: 0.022094],[D fake loss: 0.000000], [G loss(mse): 0.052182, G loss(w): 0.045983]\n",
      "Epoch 484/10001\n",
      "[D valid loss: 0.021156],[D fake loss: 0.000000], [G loss(mse): 0.050879, G loss(w): 0.045038]\n",
      "Epoch 485/10001\n",
      "[D valid loss: 0.020975],[D fake loss: 0.000000], [G loss(mse): 0.048290, G loss(w): 0.043308]\n",
      "Epoch 486/10001\n",
      "[D valid loss: 0.020864],[D fake loss: 0.000000], [G loss(mse): 0.051431, G loss(w): 0.045108]\n",
      "Epoch 487/10001\n",
      "[D valid loss: 0.022397],[D fake loss: 0.000000], [G loss(mse): 0.049418, G loss(w): 0.042818]\n",
      "Epoch 488/10001\n",
      "[D valid loss: 0.021191],[D fake loss: 0.000000], [G loss(mse): 0.051977, G loss(w): 0.046474]\n",
      "Epoch 489/10001\n",
      "[D valid loss: 0.020509],[D fake loss: 0.000000], [G loss(mse): 0.049191, G loss(w): 0.042664]\n",
      "Epoch 490/10001\n",
      "[D valid loss: 0.021639],[D fake loss: 0.000000], [G loss(mse): 0.052408, G loss(w): 0.046120]\n",
      "Epoch 491/10001\n",
      "[D valid loss: 0.021757],[D fake loss: 0.000000], [G loss(mse): 0.050135, G loss(w): 0.044204]\n",
      "Epoch 492/10001\n",
      "[D valid loss: 0.021513],[D fake loss: 0.000000], [G loss(mse): 0.051054, G loss(w): 0.046104]\n",
      "Epoch 493/10001\n",
      "[D valid loss: 0.021131],[D fake loss: 0.000000], [G loss(mse): 0.050130, G loss(w): 0.045182]\n",
      "Epoch 494/10001\n",
      "[D valid loss: 0.020501],[D fake loss: 0.000000], [G loss(mse): 0.049436, G loss(w): 0.044478]\n",
      "Epoch 495/10001\n",
      "[D valid loss: 0.019485],[D fake loss: 0.000000], [G loss(mse): 0.049429, G loss(w): 0.043644]\n",
      "Epoch 496/10001\n",
      "[D valid loss: 0.019700],[D fake loss: 0.000000], [G loss(mse): 0.046549, G loss(w): 0.041576]\n",
      "Epoch 497/10001\n",
      "[D valid loss: 0.022406],[D fake loss: 0.000000], [G loss(mse): 0.049416, G loss(w): 0.044944]\n",
      "Epoch 498/10001\n",
      "[D valid loss: 0.021925],[D fake loss: 0.000000], [G loss(mse): 0.046724, G loss(w): 0.042047]\n",
      "Epoch 499/10001\n",
      "[D valid loss: 0.021162],[D fake loss: 0.000000], [G loss(mse): 0.048052, G loss(w): 0.043432]\n",
      "Epoch 500/10001\n",
      "[D valid loss: 0.021050],[D fake loss: 0.000000], [G loss(mse): 0.049323, G loss(w): 0.044017]\n",
      "Epoch 501/10001\n",
      "[D valid loss: 0.021546],[D fake loss: 0.000000], [G loss(mse): 0.050006, G loss(w): 0.044097]\n",
      "Epoch 502/10001\n",
      "[D valid loss: 0.019886],[D fake loss: 0.000000], [G loss(mse): 0.050943, G loss(w): 0.045738]\n",
      "Epoch 503/10001\n",
      "[D valid loss: 0.020233],[D fake loss: 0.000000], [G loss(mse): 0.048946, G loss(w): 0.043299]\n",
      "Epoch 504/10001\n",
      "[D valid loss: 0.019834],[D fake loss: 0.000000], [G loss(mse): 0.045510, G loss(w): 0.039700]\n",
      "Epoch 505/10001\n",
      "[D valid loss: 0.017579],[D fake loss: 0.000000], [G loss(mse): 0.048443, G loss(w): 0.042258]\n",
      "Epoch 506/10001\n",
      "[D valid loss: 0.020628],[D fake loss: 0.000000], [G loss(mse): 0.048470, G loss(w): 0.044196]\n",
      "Epoch 507/10001\n",
      "[D valid loss: 0.020085],[D fake loss: 0.000000], [G loss(mse): 0.048197, G loss(w): 0.044037]\n",
      "Epoch 508/10001\n",
      "[D valid loss: 0.019778],[D fake loss: 0.000000], [G loss(mse): 0.052019, G loss(w): 0.045480]\n",
      "Epoch 509/10001\n",
      "[D valid loss: 0.017879],[D fake loss: 0.000000], [G loss(mse): 0.046173, G loss(w): 0.041859]\n",
      "Epoch 510/10001\n",
      "[D valid loss: 0.019927],[D fake loss: 0.000000], [G loss(mse): 0.049093, G loss(w): 0.043883]\n",
      "Epoch 511/10001\n",
      "[D valid loss: 0.020349],[D fake loss: 0.000000], [G loss(mse): 0.044789, G loss(w): 0.039533]\n",
      "Epoch 512/10001\n",
      "[D valid loss: 0.019836],[D fake loss: 0.000000], [G loss(mse): 0.048841, G loss(w): 0.043730]\n",
      "Epoch 513/10001\n",
      "[D valid loss: 0.019352],[D fake loss: 0.000000], [G loss(mse): 0.046733, G loss(w): 0.041268]\n",
      "Epoch 514/10001\n",
      "[D valid loss: 0.019115],[D fake loss: 0.000000], [G loss(mse): 0.047898, G loss(w): 0.043161]\n",
      "Epoch 515/10001\n",
      "[D valid loss: 0.019896],[D fake loss: 0.000000], [G loss(mse): 0.044930, G loss(w): 0.040766]\n",
      "Epoch 516/10001\n",
      "[D valid loss: 0.018703],[D fake loss: 0.000000], [G loss(mse): 0.047768, G loss(w): 0.042911]\n",
      "Epoch 517/10001\n",
      "[D valid loss: 0.017795],[D fake loss: 0.000000], [G loss(mse): 0.044518, G loss(w): 0.040727]\n",
      "Epoch 518/10001\n",
      "[D valid loss: 0.020215],[D fake loss: 0.000000], [G loss(mse): 0.046113, G loss(w): 0.040410]\n",
      "Epoch 519/10001\n",
      "[D valid loss: 0.017246],[D fake loss: 0.000000], [G loss(mse): 0.046039, G loss(w): 0.041795]\n",
      "Epoch 520/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D valid loss: 0.018437],[D fake loss: 0.000000], [G loss(mse): 0.046087, G loss(w): 0.040893]\n",
      "Epoch 521/10001\n",
      "[D valid loss: 0.018577],[D fake loss: 0.000000], [G loss(mse): 0.045768, G loss(w): 0.040529]\n",
      "Epoch 522/10001\n",
      "[D valid loss: 0.019921],[D fake loss: 0.000000], [G loss(mse): 0.044474, G loss(w): 0.040465]\n",
      "Epoch 523/10001\n",
      "[D valid loss: 0.018165],[D fake loss: 0.000000], [G loss(mse): 0.045986, G loss(w): 0.042098]\n",
      "Epoch 524/10001\n",
      "[D valid loss: 0.018689],[D fake loss: 0.000000], [G loss(mse): 0.046183, G loss(w): 0.042453]\n",
      "Epoch 525/10001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "n_points = 900\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//9, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//9, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//9, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//9, n_features), x_input[3])\n",
    "predict5 = np.full((n_points//9, n_features), x_input[4])\n",
    "predict6 = np.full((n_points//9, n_features), x_input[5])\n",
    "predict7 = np.full((n_points//9, n_features), x_input[6])\n",
    "predict8 = np.full((n_points//9, n_features), x_input[7])\n",
    "predict9 = np.full((n_points//9, n_features), x_input[8])\n",
    "\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4, predict5, predict6, predict7, predict8, predict9))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder(aae.encoder.predict(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
