{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           360         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           360         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 952\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Decoder:\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,242\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Discriminator:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                1680      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 5,801\n",
      "Trainable params: 5,401\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network16.build_encoder(Z, nodes, n_features)\n",
    "print(\"Encoder:\\n\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network16.build_decoder(Z,nodes, n_features)\n",
    "print(\"Decoder:\\n\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator=network16.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model16\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 10001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model16.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10001\n",
      "[C1 valid: 0.471421, C2 fake: 0.000000], [G loss: 0.378111, mse: 0.746776]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05019/AAE_Model16.py:197: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_40_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_40_10001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_40_10001/assets\n",
      "Epoch 2/10001\n",
      "[C1 valid: 0.417364, C2 fake: 0.000000], [G loss: 0.291340, mse: 0.573952]\n",
      "Epoch 3/10001\n",
      "[C1 valid: 0.383513, C2 fake: 0.000000], [G loss: 0.256761, mse: 0.505584]\n",
      "Epoch 4/10001\n",
      "[C1 valid: 0.358562, C2 fake: 0.000000], [G loss: 0.220994, mse: 0.434830]\n",
      "Epoch 5/10001\n",
      "[C1 valid: 0.340608, C2 fake: 0.000000], [G loss: 0.204129, mse: 0.401898]\n",
      "Epoch 6/10001\n",
      "[C1 valid: 0.325468, C2 fake: 0.000000], [G loss: 0.181440, mse: 0.357360]\n",
      "Epoch 7/10001\n",
      "[C1 valid: 0.304280, C2 fake: 0.000000], [G loss: 0.156418, mse: 0.307904]\n",
      "Epoch 8/10001\n",
      "[C1 valid: 0.290735, C2 fake: 0.000000], [G loss: 0.150603, mse: 0.296889]\n",
      "Epoch 9/10001\n",
      "[C1 valid: 0.278952, C2 fake: 0.000000], [G loss: 0.142126, mse: 0.280490]\n",
      "Epoch 10/10001\n",
      "[C1 valid: 0.268646, C2 fake: 0.000000], [G loss: 0.143371, mse: 0.283405]\n",
      "Epoch 11/10001\n",
      "[C1 valid: 0.258913, C2 fake: 0.000000], [G loss: 0.124903, mse: 0.246899]\n",
      "Epoch 12/10001\n",
      "[C1 valid: 0.245257, C2 fake: 0.000000], [G loss: 0.122660, mse: 0.242690]\n",
      "Epoch 13/10001\n",
      "[C1 valid: 0.231714, C2 fake: 0.000000], [G loss: 0.121475, mse: 0.240607]\n",
      "Epoch 14/10001\n",
      "[C1 valid: 0.215705, C2 fake: 0.000000], [G loss: 0.113235, mse: 0.224467]\n",
      "Epoch 15/10001\n",
      "[C1 valid: 0.212960, C2 fake: 0.000000], [G loss: 0.107750, mse: 0.213745]\n",
      "Epoch 16/10001\n",
      "[C1 valid: 0.204275, C2 fake: 0.000000], [G loss: 0.096249, mse: 0.190914]\n",
      "Epoch 17/10001\n",
      "[C1 valid: 0.192347, C2 fake: 0.000000], [G loss: 0.098926, mse: 0.196407]\n",
      "Epoch 18/10001\n",
      "[C1 valid: 0.186902, C2 fake: 0.000000], [G loss: 0.093234, mse: 0.185242]\n",
      "Epoch 19/10001\n",
      "[C1 valid: 0.179140, C2 fake: 0.000000], [G loss: 0.087563, mse: 0.173980]\n",
      "Epoch 20/10001\n",
      "[C1 valid: 0.171056, C2 fake: 0.000000], [G loss: 0.089913, mse: 0.178820]\n",
      "Epoch 21/10001\n",
      "[C1 valid: 0.157848, C2 fake: 0.000000], [G loss: 0.079120, mse: 0.157346]\n",
      "Epoch 22/10001\n",
      "[C1 valid: 0.149754, C2 fake: 0.000000], [G loss: 0.078402, mse: 0.156039]\n",
      "Epoch 23/10001\n",
      "[C1 valid: 0.146176, C2 fake: 0.000000], [G loss: 0.082041, mse: 0.163335]\n",
      "Epoch 24/10001\n",
      "[C1 valid: 0.139930, C2 fake: 0.000000], [G loss: 0.078558, mse: 0.156450]\n",
      "Epoch 25/10001\n",
      "[C1 valid: 0.134963, C2 fake: 0.000000], [G loss: 0.074340, mse: 0.148175]\n",
      "Epoch 26/10001\n",
      "[C1 valid: 0.128716, C2 fake: 0.000000], [G loss: 0.072608, mse: 0.144687]\n",
      "Epoch 27/10001\n",
      "[C1 valid: 0.120957, C2 fake: 0.000000], [G loss: 0.064747, mse: 0.129091]\n",
      "Epoch 28/10001\n",
      "[C1 valid: 0.118782, C2 fake: 0.000000], [G loss: 0.065847, mse: 0.131305]\n",
      "Epoch 29/10001\n",
      "[C1 valid: 0.115260, C2 fake: 0.000000], [G loss: 0.062645, mse: 0.124963]\n",
      "Epoch 30/10001\n",
      "[C1 valid: 0.105244, C2 fake: 0.000000], [G loss: 0.065526, mse: 0.130742]\n",
      "Epoch 31/10001\n",
      "[C1 valid: 0.101380, C2 fake: 0.000000], [G loss: 0.059690, mse: 0.119091]\n",
      "Epoch 32/10001\n",
      "[C1 valid: 0.097697, C2 fake: 0.000000], [G loss: 0.057541, mse: 0.114851]\n",
      "Epoch 33/10001\n",
      "[C1 valid: 0.092289, C2 fake: 0.000000], [G loss: 0.055283, mse: 0.110382]\n",
      "Epoch 34/10001\n",
      "[C1 valid: 0.091280, C2 fake: 0.000000], [G loss: 0.052396, mse: 0.104548]\n",
      "Epoch 35/10001\n",
      "[C1 valid: 0.083944, C2 fake: 0.000000], [G loss: 0.056719, mse: 0.113243]\n",
      "Epoch 36/10001\n",
      "[C1 valid: 0.079713, C2 fake: 0.000000], [G loss: 0.052425, mse: 0.104683]\n",
      "Epoch 37/10001\n",
      "[C1 valid: 0.076413, C2 fake: 0.000000], [G loss: 0.048045, mse: 0.095954]\n",
      "Epoch 38/10001\n",
      "[C1 valid: 0.074023, C2 fake: 0.000000], [G loss: 0.050041, mse: 0.099943]\n",
      "Epoch 39/10001\n",
      "[C1 valid: 0.068103, C2 fake: 0.000000], [G loss: 0.050557, mse: 0.101003]\n",
      "Epoch 40/10001\n",
      "[C1 valid: 0.065654, C2 fake: 0.000000], [G loss: 0.047213, mse: 0.094323]\n",
      "Epoch 41/10001\n",
      "[C1 valid: 0.066708, C2 fake: 0.000000], [G loss: 0.043862, mse: 0.087640]\n",
      "Epoch 42/10001\n",
      "[C1 valid: 0.061193, C2 fake: 0.000000], [G loss: 0.043892, mse: 0.087698]\n",
      "Epoch 43/10001\n",
      "[C1 valid: 0.055672, C2 fake: 0.000000], [G loss: 0.040829, mse: 0.081589]\n",
      "Epoch 44/10001\n",
      "[C1 valid: 0.054192, C2 fake: 0.000000], [G loss: 0.038342, mse: 0.076617]\n",
      "Epoch 45/10001\n",
      "[C1 valid: 0.049970, C2 fake: 0.000000], [G loss: 0.036379, mse: 0.072691]\n",
      "Epoch 46/10001\n",
      "[C1 valid: 0.048362, C2 fake: 0.000000], [G loss: 0.036310, mse: 0.072559]\n",
      "Epoch 47/10001\n",
      "[C1 valid: 0.046764, C2 fake: 0.000000], [G loss: 0.033164, mse: 0.066261]\n",
      "Epoch 48/10001\n",
      "[C1 valid: 0.044911, C2 fake: 0.000000], [G loss: 0.032815, mse: 0.065575]\n",
      "Epoch 49/10001\n",
      "[C1 valid: 0.043530, C2 fake: 0.000000], [G loss: 0.031408, mse: 0.062777]\n",
      "Epoch 50/10001\n",
      "[C1 valid: 0.043589, C2 fake: 0.000000], [G loss: 0.030272, mse: 0.060503]\n",
      "Epoch 51/10001\n",
      "[C1 valid: 0.038330, C2 fake: 0.000000], [G loss: 0.029999, mse: 0.059959]\n",
      "Epoch 52/10001\n",
      "[C1 valid: 0.039867, C2 fake: 0.000000], [G loss: 0.027202, mse: 0.054366]\n",
      "Epoch 53/10001\n",
      "[C1 valid: 0.036459, C2 fake: 0.000000], [G loss: 0.027441, mse: 0.054843]\n",
      "Epoch 54/10001\n",
      "[C1 valid: 0.033834, C2 fake: 0.000000], [G loss: 0.027219, mse: 0.054406]\n",
      "Epoch 55/10001\n",
      "[C1 valid: 0.035043, C2 fake: 0.000000], [G loss: 0.026125, mse: 0.052201]\n",
      "Epoch 56/10001\n",
      "[C1 valid: 0.030729, C2 fake: 0.000000], [G loss: 0.024733, mse: 0.049432]\n",
      "Epoch 57/10001\n",
      "[C1 valid: 0.028988, C2 fake: 0.000000], [G loss: 0.025265, mse: 0.050503]\n",
      "Epoch 58/10001\n",
      "[C1 valid: 0.025602, C2 fake: 0.000000], [G loss: 0.023937, mse: 0.047853]\n",
      "Epoch 59/10001\n",
      "[C1 valid: 0.030232, C2 fake: 0.000000], [G loss: 0.023541, mse: 0.047063]\n",
      "Epoch 60/10001\n",
      "[C1 valid: 0.025678, C2 fake: 0.000000], [G loss: 0.021638, mse: 0.043254]\n",
      "Epoch 61/10001\n",
      "[C1 valid: 0.026959, C2 fake: 0.000000], [G loss: 0.021328, mse: 0.042635]\n",
      "Epoch 62/10001\n",
      "[C1 valid: 0.021350, C2 fake: 0.000000], [G loss: 0.023164, mse: 0.046303]\n",
      "Epoch 63/10001\n",
      "[C1 valid: 0.023692, C2 fake: 0.000000], [G loss: 0.021437, mse: 0.042854]\n",
      "Epoch 64/10001\n",
      "[C1 valid: 0.023179, C2 fake: 0.000000], [G loss: 0.021162, mse: 0.042309]\n",
      "Epoch 65/10001\n",
      "[C1 valid: 0.019491, C2 fake: 0.000000], [G loss: 0.020759, mse: 0.041497]\n",
      "Epoch 66/10001\n",
      "[C1 valid: 0.020848, C2 fake: 0.000000], [G loss: 0.020840, mse: 0.041660]\n",
      "Epoch 67/10001\n",
      "[C1 valid: 0.018485, C2 fake: 0.000000], [G loss: 0.018107, mse: 0.036190]\n",
      "Epoch 68/10001\n",
      "[C1 valid: 0.020257, C2 fake: 0.000000], [G loss: 0.017732, mse: 0.035450]\n",
      "Epoch 69/10001\n",
      "[C1 valid: 0.016953, C2 fake: 0.000000], [G loss: 0.018552, mse: 0.037088]\n",
      "Epoch 70/10001\n",
      "[C1 valid: 0.017822, C2 fake: 0.000000], [G loss: 0.017934, mse: 0.035832]\n",
      "Epoch 71/10001\n",
      "[C1 valid: 0.015660, C2 fake: 0.000000], [G loss: 0.017396, mse: 0.034778]\n",
      "Epoch 72/10001\n",
      "[C1 valid: 0.017011, C2 fake: 0.000000], [G loss: 0.016319, mse: 0.032624]\n",
      "Epoch 73/10001\n",
      "[C1 valid: 0.013881, C2 fake: 0.000000], [G loss: 0.016706, mse: 0.033397]\n",
      "Epoch 74/10001\n",
      "[C1 valid: 0.014921, C2 fake: 0.000000], [G loss: 0.016443, mse: 0.032876]\n",
      "Epoch 75/10001\n",
      "[C1 valid: 0.012653, C2 fake: 0.000000], [G loss: 0.014620, mse: 0.029228]\n",
      "Epoch 76/10001\n",
      "[C1 valid: 0.010202, C2 fake: 0.000000], [G loss: 0.015912, mse: 0.031808]\n",
      "Epoch 77/10001\n",
      "[C1 valid: 0.008850, C2 fake: 0.000000], [G loss: 0.015640, mse: 0.031261]\n",
      "Epoch 78/10001\n",
      "[C1 valid: 0.015754, C2 fake: 0.000000], [G loss: 0.014689, mse: 0.029369]\n",
      "Epoch 79/10001\n",
      "[C1 valid: 0.008967, C2 fake: 0.000000], [G loss: 0.014271, mse: 0.028531]\n",
      "Epoch 80/10001\n",
      "[C1 valid: 0.009133, C2 fake: 0.000000], [G loss: 0.014512, mse: 0.029015]\n",
      "Epoch 81/10001\n",
      "[C1 valid: 0.010996, C2 fake: 0.000000], [G loss: 0.015598, mse: 0.031187]\n",
      "Epoch 82/10001\n",
      "[C1 valid: 0.010010, C2 fake: 0.000000], [G loss: 0.015341, mse: 0.030675]\n",
      "Epoch 83/10001\n",
      "[C1 valid: 0.008066, C2 fake: 0.000000], [G loss: 0.014675, mse: 0.029340]\n",
      "Epoch 84/10001\n",
      "[C1 valid: 0.009381, C2 fake: 0.000000], [G loss: 0.013299, mse: 0.026591]\n",
      "Epoch 85/10001\n",
      "[C1 valid: 0.009825, C2 fake: 0.000000], [G loss: 0.012927, mse: 0.025847]\n",
      "Epoch 86/10001\n",
      "[C1 valid: 0.005901, C2 fake: 0.000000], [G loss: 0.013973, mse: 0.027937]\n",
      "Epoch 87/10001\n",
      "[C1 valid: 0.005235, C2 fake: 0.000000], [G loss: 0.013330, mse: 0.026653]\n",
      "Epoch 88/10001\n",
      "[C1 valid: 0.008372, C2 fake: 0.000000], [G loss: 0.014134, mse: 0.028263]\n",
      "Epoch 89/10001\n",
      "[C1 valid: 0.009221, C2 fake: 0.000000], [G loss: 0.012662, mse: 0.025317]\n",
      "Epoch 90/10001\n",
      "[C1 valid: 0.009500, C2 fake: 0.000000], [G loss: 0.012794, mse: 0.025582]\n",
      "Epoch 91/10001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: 0.006481, C2 fake: 0.000000], [G loss: 0.012712, mse: 0.025417]\n",
      "Epoch 92/10001\n",
      "[C1 valid: 0.013916, C2 fake: 0.000000], [G loss: 0.012859, mse: 0.025715]\n",
      "Epoch 93/10001\n",
      "[C1 valid: 0.008387, C2 fake: 0.000000], [G loss: 0.012450, mse: 0.024895]\n",
      "Epoch 94/10001\n",
      "[C1 valid: 0.008368, C2 fake: 0.000000], [G loss: 0.012297, mse: 0.024589]\n",
      "Epoch 95/10001\n",
      "[C1 valid: 0.004739, C2 fake: 0.000000], [G loss: 0.012136, mse: 0.024263]\n",
      "Epoch 96/10001\n",
      "[C1 valid: 0.006807, C2 fake: 0.000000], [G loss: 0.011292, mse: 0.022578]\n",
      "Epoch 97/10001\n",
      "[C1 valid: 0.006370, C2 fake: 0.000000], [G loss: 0.011099, mse: 0.022192]\n",
      "Epoch 98/10001\n",
      "[C1 valid: 0.006324, C2 fake: 0.000000], [G loss: 0.010642, mse: 0.021277]\n",
      "Epoch 99/10001\n",
      "[C1 valid: 0.007238, C2 fake: 0.000000], [G loss: 0.010925, mse: 0.021847]\n",
      "Epoch 100/10001\n",
      "[C1 valid: 0.006771, C2 fake: 0.000000], [G loss: 0.010637, mse: 0.021269]\n",
      "Epoch 101/10001\n",
      "[C1 valid: 0.004832, C2 fake: 0.000000], [G loss: 0.011081, mse: 0.022157]\n",
      "Epoch 102/10001\n",
      "[C1 valid: 0.005272, C2 fake: 0.000000], [G loss: 0.009769, mse: 0.019533]\n",
      "Epoch 103/10001\n",
      "[C1 valid: 0.005822, C2 fake: 0.000000], [G loss: 0.009668, mse: 0.019332]\n",
      "Epoch 104/10001\n",
      "[C1 valid: 0.005492, C2 fake: 0.000000], [G loss: 0.009884, mse: 0.019763]\n",
      "Epoch 105/10001\n",
      "[C1 valid: 0.005517, C2 fake: 0.000000], [G loss: 0.010381, mse: 0.020757]\n",
      "Epoch 106/10001\n",
      "[C1 valid: 0.003885, C2 fake: 0.000000], [G loss: 0.010509, mse: 0.021013]\n",
      "Epoch 107/10001\n",
      "[C1 valid: 0.004959, C2 fake: 0.000000], [G loss: 0.010391, mse: 0.020779]\n",
      "Epoch 108/10001\n",
      "[C1 valid: 0.005190, C2 fake: 0.000000], [G loss: 0.010306, mse: 0.020609]\n",
      "Epoch 109/10001\n",
      "[C1 valid: 0.003742, C2 fake: 0.000000], [G loss: 0.010049, mse: 0.020095]\n",
      "Epoch 110/10001\n",
      "[C1 valid: 0.003120, C2 fake: 0.000000], [G loss: 0.009424, mse: 0.018845]\n",
      "Epoch 111/10001\n",
      "[C1 valid: 0.003393, C2 fake: 0.000000], [G loss: 0.009710, mse: 0.019416]\n",
      "Epoch 112/10001\n",
      "[C1 valid: 0.021432, C2 fake: 0.000000], [G loss: 0.009823, mse: 0.019643]\n",
      "Epoch 113/10001\n",
      "[C1 valid: 0.006732, C2 fake: 0.000000], [G loss: 0.008673, mse: 0.017343]\n",
      "Epoch 114/10001\n",
      "[C1 valid: 0.005671, C2 fake: 0.000000], [G loss: 0.009049, mse: 0.018096]\n",
      "Epoch 115/10001\n",
      "[C1 valid: 0.003484, C2 fake: 0.000000], [G loss: 0.009243, mse: 0.018484]\n",
      "Epoch 116/10001\n",
      "[C1 valid: 0.003288, C2 fake: 0.000000], [G loss: 0.008707, mse: 0.017412]\n",
      "Epoch 117/10001\n",
      "[C1 valid: 0.002850, C2 fake: 0.000000], [G loss: 0.009459, mse: 0.018918]\n",
      "Epoch 118/10001\n",
      "[C1 valid: 0.001982, C2 fake: 0.000000], [G loss: 0.008844, mse: 0.017686]\n",
      "Epoch 119/10001\n",
      "[C1 valid: 0.003365, C2 fake: 0.000000], [G loss: 0.008144, mse: 0.016287]\n",
      "Epoch 120/10001\n",
      "[C1 valid: 0.004386, C2 fake: 0.000000], [G loss: 0.008090, mse: 0.016177]\n",
      "Epoch 121/10001\n",
      "[C1 valid: 0.003126, C2 fake: 0.000000], [G loss: 0.008461, mse: 0.016920]\n",
      "Epoch 122/10001\n",
      "[C1 valid: 0.002263, C2 fake: 0.000000], [G loss: 0.007975, mse: 0.015948]\n",
      "Epoch 123/10001\n",
      "[C1 valid: 0.003445, C2 fake: 0.000000], [G loss: 0.007791, mse: 0.015578]\n",
      "Epoch 124/10001\n",
      "[C1 valid: 0.001739, C2 fake: 0.000000], [G loss: 0.007540, mse: 0.015078]\n",
      "Epoch 125/10001\n",
      "[C1 valid: 0.002404, C2 fake: 0.000000], [G loss: 0.007480, mse: 0.014958]\n",
      "Epoch 126/10001\n",
      "[C1 valid: 0.001260, C2 fake: 0.000000], [G loss: 0.006975, mse: 0.013947]\n",
      "Epoch 127/10001\n",
      "[C1 valid: 0.001226, C2 fake: 0.000000], [G loss: 0.007274, mse: 0.014546]\n",
      "Epoch 128/10001\n",
      "[C1 valid: 0.001875, C2 fake: 0.000000], [G loss: 0.007537, mse: 0.015074]\n",
      "Epoch 129/10001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder.predict(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "n_points = 900\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//9, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//9, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//9, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//9, n_features), x_input[3])\n",
    "predict5 = np.full((n_points//9, n_features), x_input[4])\n",
    "predict6 = np.full((n_points//9, n_features), x_input[5])\n",
    "predict7 = np.full((n_points//9, n_features), x_input[6])\n",
    "predict8 = np.full((n_points//9, n_features), x_input[7])\n",
    "predict9 = np.full((n_points//9, n_features), x_input[8])\n",
    "\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4, predict5, predict6, predict7, predict8, predict9))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder.predict(aae.encoder(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
