{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from backend import import_excel, export_excel\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "# style.use('bmh')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import dataset,network20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scenario= \"sinus\" #sinus, helix\n",
    "n_instance = 1000\n",
    "n_features = 2\n",
    "Z=40\n",
    "scales = ['-1-1','0-1']\n",
    "scaled = '-1-1'\n",
    "nodes=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    print(\"X_train= x,y\",X_train.shape)\n",
    "    print(\"y_train= z\",y_train.shape)\n",
    "\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_train[:,0], X_train[:,1], y_train, c='orange')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = dataset.get_dataset(n_instance, scenario)\n",
    "    plt.scatter(X_train,y_train, c='orange', label='Sample Data')\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made dataset\n"
     ]
    }
   ],
   "source": [
    "#storage data\n",
    "os.system('mkdir Dataset')\n",
    "os.system('mkdir AAE')\n",
    "os.system('mkdir AAE/Models')\n",
    "os.system('mkdir AAE/Losses')\n",
    "os.system('mkdir AAE/Random_test')\n",
    "#export_excel(X_train, 'Dataset/X_train')\n",
    "#export_excel(y_train, 'Dataset/y_train')\n",
    "\n",
    "# print(X_train.shape,y_train.shape)\n",
    "X_train = import_excel('Dataset/X_train')\n",
    "y_train = import_excel('Dataset/y_train')\n",
    "print('made dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16)           64          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "elu (ELU)                       (None, 16)           0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            136         elu[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8)            32          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "elu_1 (ELU)                     (None, 8)            0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40)           360         elu_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,000\n",
      "Trainable params: 952\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Decoder:\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 328       \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 2,242\n",
      "Trainable params: 2,194\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Discriminator:\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "elu_4 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                1680      \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 80)                320       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                840       \n",
      "_________________________________________________________________\n",
      "elu_8 (ELU)                  (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 5,801\n",
      "Trainable params: 5,401\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder=network20.build_encoder(Z, nodes, n_features)\n",
    "print(\"Encoder:\\n\")\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "decoder=network20.build_decoder(Z,nodes, n_features)\n",
    "print(\"Decoder:\\n\")\n",
    "decoder.summary()\n",
    "\n",
    "discriminator=network20.build_discriminator(Z)\n",
    "print(\"Discriminator:\\n\")\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import AAE_Model20\n",
    "\n",
    "GANorWGAN='WGAN'\n",
    "epochs = 100001\n",
    "BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aae = AAE_Model20.AAE(Z, n_features, BATCH_SIZE,GANorWGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape_1 (1000, 2)\n",
      "Cycles:  1\n",
      "X_train (1000, 1)\n",
      "y_train (1000, 1)\n",
      "X_train_scaled (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, scaler, X_train_scaled = aae.preproc(X_train, y_train, scaled)\n",
    "\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_train_scaled\",X_train_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100001\n",
      "[C1 valid: -0.505835, C2 fake: 0.509894], [G loss: 0.356548, mse: 0.723112]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilyhua/OneDrive - Imperial College London/INHALE Code/Lily/AAE/AAE05012/AAE_Model16.py:190: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AAE/Models/encoder_40_100001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/decoder_40_100001/assets\n",
      "INFO:tensorflow:Assets written to: AAE/Models/discriminator_40_100001/assets\n",
      "Epoch 2/100001\n",
      "[C1 valid: -0.507384, C2 fake: 0.508698], [G loss: 0.317172, mse: 0.644420]\n",
      "Epoch 3/100001\n",
      "[C1 valid: -0.508771, C2 fake: 0.509670], [G loss: 0.262577, mse: 0.535270]\n",
      "Epoch 4/100001\n",
      "[C1 valid: -0.507721, C2 fake: 0.509111], [G loss: 0.232258, mse: 0.474725]\n",
      "Epoch 5/100001\n",
      "[C1 valid: -0.506220, C2 fake: 0.510144], [G loss: 0.190366, mse: 0.390988]\n",
      "Epoch 6/100001\n",
      "[C1 valid: -0.505802, C2 fake: 0.509767], [G loss: 0.159488, mse: 0.329236]\n",
      "Epoch 7/100001\n",
      "[C1 valid: -0.506004, C2 fake: 0.508038], [G loss: 0.143255, mse: 0.296944]\n",
      "Epoch 8/100001\n",
      "[C1 valid: -0.506648, C2 fake: 0.506728], [G loss: 0.116382, mse: 0.243235]\n",
      "Epoch 9/100001\n",
      "[C1 valid: -0.506868, C2 fake: 0.508961], [G loss: 0.102741, mse: 0.216186]\n",
      "Epoch 10/100001\n",
      "[C1 valid: -0.504435, C2 fake: 0.508889], [G loss: 0.093509, mse: 0.197728]\n",
      "Epoch 11/100001\n",
      "[C1 valid: -0.507328, C2 fake: 0.508511], [G loss: 0.080247, mse: 0.171034]\n",
      "Epoch 12/100001\n",
      "[C1 valid: -0.507566, C2 fake: 0.507516], [G loss: 0.072956, mse: 0.156289]\n",
      "Epoch 13/100001\n",
      "[C1 valid: -0.507784, C2 fake: 0.506412], [G loss: 0.070003, mse: 0.150504]\n",
      "Epoch 14/100001\n",
      "[C1 valid: -0.508980, C2 fake: 0.508905], [G loss: 0.059982, mse: 0.130399]\n",
      "Epoch 15/100001\n",
      "[C1 valid: -0.509695, C2 fake: 0.508972], [G loss: 0.055876, mse: 0.122180]\n",
      "Epoch 16/100001\n",
      "[C1 valid: -0.508144, C2 fake: 0.510613], [G loss: 0.053773, mse: 0.117771]\n",
      "Epoch 17/100001\n",
      "[C1 valid: -0.507155, C2 fake: 0.509720], [G loss: 0.047123, mse: 0.104610]\n",
      "Epoch 18/100001\n",
      "[C1 valid: -0.505871, C2 fake: 0.508478], [G loss: 0.045502, mse: 0.101563]\n",
      "Epoch 19/100001\n",
      "[C1 valid: -0.508273, C2 fake: 0.508898], [G loss: 0.040431, mse: 0.091270]\n",
      "Epoch 20/100001\n",
      "[C1 valid: -0.509098, C2 fake: 0.508515], [G loss: 0.037898, mse: 0.086017]\n",
      "Epoch 21/100001\n",
      "[C1 valid: -0.510152, C2 fake: 0.508925], [G loss: 0.034667, mse: 0.079699]\n",
      "Epoch 22/100001\n",
      "[C1 valid: -0.509931, C2 fake: 0.509556], [G loss: 0.035246, mse: 0.080766]\n",
      "Epoch 23/100001\n",
      "[C1 valid: -0.508550, C2 fake: 0.510004], [G loss: 0.031791, mse: 0.073781]\n",
      "Epoch 24/100001\n",
      "[C1 valid: -0.510447, C2 fake: 0.508409], [G loss: 0.028568, mse: 0.067736]\n",
      "Epoch 25/100001\n",
      "[C1 valid: -0.512826, C2 fake: 0.508637], [G loss: 0.025409, mse: 0.061474]\n",
      "Epoch 26/100001\n",
      "[C1 valid: -0.513088, C2 fake: 0.508806], [G loss: 0.028172, mse: 0.067175]\n",
      "Epoch 27/100001\n",
      "[C1 valid: -0.508910, C2 fake: 0.507752], [G loss: 0.024342, mse: 0.059527]\n",
      "Epoch 28/100001\n",
      "[C1 valid: -0.507937, C2 fake: 0.504863], [G loss: 0.025504, mse: 0.061425]\n",
      "Epoch 29/100001\n",
      "[C1 valid: -0.509588, C2 fake: 0.504871], [G loss: 0.024963, mse: 0.060434]\n",
      "Epoch 30/100001\n",
      "[C1 valid: -0.511062, C2 fake: 0.504461], [G loss: 0.023210, mse: 0.056971]\n",
      "Epoch 31/100001\n",
      "[C1 valid: -0.507972, C2 fake: 0.501073], [G loss: 0.021243, mse: 0.052893]\n",
      "Epoch 32/100001\n",
      "[C1 valid: -0.509095, C2 fake: 0.503447], [G loss: 0.020569, mse: 0.051416]\n",
      "Epoch 33/100001\n",
      "[C1 valid: -0.510859, C2 fake: 0.502304], [G loss: 0.022438, mse: 0.055065]\n",
      "Epoch 34/100001\n",
      "[C1 valid: -0.506614, C2 fake: 0.500900], [G loss: 0.019965, mse: 0.050105]\n",
      "Epoch 35/100001\n",
      "[C1 valid: -0.507866, C2 fake: 0.498256], [G loss: 0.019852, mse: 0.049250]\n",
      "Epoch 36/100001\n",
      "[C1 valid: -0.509408, C2 fake: 0.497379], [G loss: 0.019846, mse: 0.049331]\n",
      "Epoch 37/100001\n",
      "[C1 valid: -0.509298, C2 fake: 0.494791], [G loss: 0.019209, mse: 0.047948]\n",
      "Epoch 38/100001\n",
      "[C1 valid: -0.510646, C2 fake: 0.492510], [G loss: 0.017118, mse: 0.044170]\n",
      "Epoch 39/100001\n",
      "[C1 valid: -0.504103, C2 fake: 0.495778], [G loss: 0.017036, mse: 0.043699]\n",
      "Epoch 40/100001\n",
      "[C1 valid: -0.503885, C2 fake: 0.485111], [G loss: 0.016470, mse: 0.042595]\n",
      "Epoch 41/100001\n",
      "[C1 valid: -0.507739, C2 fake: 0.481490], [G loss: 0.017379, mse: 0.044535]\n",
      "Epoch 42/100001\n",
      "[C1 valid: -0.504448, C2 fake: 0.485181], [G loss: 0.016250, mse: 0.041846]\n",
      "Epoch 43/100001\n",
      "[C1 valid: -0.507580, C2 fake: 0.480533], [G loss: 0.016037, mse: 0.041786]\n",
      "Epoch 44/100001\n",
      "[C1 valid: -0.504005, C2 fake: 0.478260], [G loss: 0.015589, mse: 0.041071]\n",
      "Epoch 45/100001\n",
      "[C1 valid: -0.505363, C2 fake: 0.476451], [G loss: 0.015924, mse: 0.041463]\n",
      "Epoch 46/100001\n",
      "[C1 valid: -0.511490, C2 fake: 0.478039], [G loss: 0.015295, mse: 0.040299]\n",
      "Epoch 47/100001\n",
      "[C1 valid: -0.513052, C2 fake: 0.475412], [G loss: 0.015109, mse: 0.039821]\n",
      "Epoch 48/100001\n",
      "[C1 valid: -0.519145, C2 fake: 0.485684], [G loss: 0.014358, mse: 0.038329]\n",
      "Epoch 49/100001\n",
      "[C1 valid: -0.516262, C2 fake: 0.478866], [G loss: 0.014893, mse: 0.039442]\n",
      "Epoch 50/100001\n",
      "[C1 valid: -0.518450, C2 fake: 0.470717], [G loss: 0.013081, mse: 0.035450]\n",
      "Epoch 51/100001\n",
      "[C1 valid: -0.516520, C2 fake: 0.472804], [G loss: 0.012998, mse: 0.035362]\n",
      "Epoch 52/100001\n",
      "[C1 valid: -0.515304, C2 fake: 0.459889], [G loss: 0.014014, mse: 0.037526]\n",
      "Epoch 53/100001\n",
      "[C1 valid: -0.518128, C2 fake: 0.467161], [G loss: 0.012713, mse: 0.034592]\n",
      "Epoch 54/100001\n",
      "[C1 valid: -0.520924, C2 fake: 0.464055], [G loss: 0.011863, mse: 0.032865]\n",
      "Epoch 55/100001\n",
      "[C1 valid: -0.515394, C2 fake: 0.468816], [G loss: 0.012656, mse: 0.034677]\n",
      "Epoch 56/100001\n",
      "[C1 valid: -0.523211, C2 fake: 0.455861], [G loss: 0.012307, mse: 0.033870]\n",
      "Epoch 57/100001\n",
      "[C1 valid: -0.520308, C2 fake: 0.452444], [G loss: 0.011325, mse: 0.031785]\n",
      "Epoch 58/100001\n",
      "[C1 valid: -0.528354, C2 fake: 0.453177], [G loss: 0.012836, mse: 0.034931]\n",
      "Epoch 59/100001\n",
      "[C1 valid: -0.533969, C2 fake: 0.451965], [G loss: 0.011941, mse: 0.033096]\n",
      "Epoch 60/100001\n",
      "[C1 valid: -0.535044, C2 fake: 0.453231], [G loss: 0.011166, mse: 0.031459]\n",
      "Epoch 61/100001\n",
      "[C1 valid: -0.536510, C2 fake: 0.450513], [G loss: 0.010397, mse: 0.029853]\n",
      "Epoch 62/100001\n",
      "[C1 valid: -0.536691, C2 fake: 0.449989], [G loss: 0.010053, mse: 0.029385]\n",
      "Epoch 63/100001\n",
      "[C1 valid: -0.538850, C2 fake: 0.446797], [G loss: 0.009749, mse: 0.028825]\n",
      "Epoch 64/100001\n",
      "[C1 valid: -0.548404, C2 fake: 0.437264], [G loss: 0.010582, mse: 0.030287]\n",
      "Epoch 65/100001\n",
      "[C1 valid: -0.550302, C2 fake: 0.437022], [G loss: 0.009978, mse: 0.029461]\n",
      "Epoch 66/100001\n",
      "[C1 valid: -0.569023, C2 fake: 0.438959], [G loss: 0.010047, mse: 0.029937]\n",
      "Epoch 67/100001\n",
      "[C1 valid: -0.568346, C2 fake: 0.436187], [G loss: 0.009770, mse: 0.029501]\n",
      "Epoch 68/100001\n",
      "[C1 valid: -0.583733, C2 fake: 0.436343], [G loss: 0.008656, mse: 0.028005]\n",
      "Epoch 69/100001\n",
      "[C1 valid: -0.615892, C2 fake: 0.424781], [G loss: 0.008957, mse: 0.028746]\n",
      "Epoch 70/100001\n",
      "[C1 valid: -0.606089, C2 fake: 0.396721], [G loss: 0.008137, mse: 0.028486]\n",
      "Epoch 71/100001\n",
      "[C1 valid: -0.625375, C2 fake: 0.385816], [G loss: 0.007712, mse: 0.027084]\n",
      "Epoch 72/100001\n",
      "[C1 valid: -0.628114, C2 fake: 0.372083], [G loss: 0.006406, mse: 0.026457]\n",
      "Epoch 73/100001\n",
      "[C1 valid: -0.634809, C2 fake: 0.367128], [G loss: 0.007094, mse: 0.026592]\n",
      "Epoch 74/100001\n",
      "[C1 valid: -0.649845, C2 fake: 0.352712], [G loss: 0.006298, mse: 0.026094]\n",
      "Epoch 75/100001\n",
      "[C1 valid: -0.643801, C2 fake: 0.352152], [G loss: 0.006908, mse: 0.027108]\n",
      "Epoch 76/100001\n",
      "[C1 valid: -0.661801, C2 fake: 0.344441], [G loss: 0.006142, mse: 0.026691]\n",
      "Epoch 77/100001\n",
      "[C1 valid: -0.680803, C2 fake: 0.330689], [G loss: 0.005505, mse: 0.025612]\n",
      "Epoch 78/100001\n",
      "[C1 valid: -0.676506, C2 fake: 0.330787], [G loss: 0.005539, mse: 0.025513]\n",
      "Epoch 79/100001\n",
      "[C1 valid: -0.692118, C2 fake: 0.317510], [G loss: 0.007307, mse: 0.024857]\n",
      "Epoch 80/100001\n",
      "[C1 valid: -0.683385, C2 fake: 0.318428], [G loss: 0.005781, mse: 0.025837]\n",
      "Epoch 81/100001\n",
      "[C1 valid: -0.702356, C2 fake: 0.309555], [G loss: 0.005407, mse: 0.024853]\n",
      "Epoch 82/100001\n",
      "[C1 valid: -0.713237, C2 fake: 0.309935], [G loss: 0.006361, mse: 0.024411]\n",
      "Epoch 83/100001\n",
      "[C1 valid: -0.716332, C2 fake: 0.296417], [G loss: 0.005288, mse: 0.024264]\n",
      "Epoch 84/100001\n",
      "[C1 valid: -0.723743, C2 fake: 0.299592], [G loss: 0.006097, mse: 0.022984]\n",
      "Epoch 85/100001\n",
      "[C1 valid: -0.715183, C2 fake: 0.302051], [G loss: 0.005497, mse: 0.024105]\n",
      "Epoch 86/100001\n",
      "[C1 valid: -0.722074, C2 fake: 0.305343], [G loss: 0.004472, mse: 0.024672]\n",
      "Epoch 87/100001\n",
      "[C1 valid: -0.737032, C2 fake: 0.291063], [G loss: 0.003286, mse: 0.021473]\n",
      "Epoch 88/100001\n",
      "[C1 valid: -0.743046, C2 fake: 0.291303], [G loss: 0.005250, mse: 0.023547]\n",
      "Epoch 89/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.751434, C2 fake: 0.281719], [G loss: 0.002848, mse: 0.021478]\n",
      "Epoch 90/100001\n",
      "[C1 valid: -0.747093, C2 fake: 0.296773], [G loss: 0.003492, mse: 0.021127]\n",
      "Epoch 91/100001\n",
      "[C1 valid: -0.765528, C2 fake: 0.282459], [G loss: 0.005320, mse: 0.022726]\n",
      "Epoch 92/100001\n",
      "[C1 valid: -0.765838, C2 fake: 0.282756], [G loss: 0.004074, mse: 0.021302]\n",
      "Epoch 93/100001\n",
      "[C1 valid: -0.769650, C2 fake: 0.278087], [G loss: 0.004114, mse: 0.022025]\n",
      "Epoch 94/100001\n",
      "[C1 valid: -0.782323, C2 fake: 0.267045], [G loss: 0.004838, mse: 0.020895]\n",
      "Epoch 95/100001\n",
      "[C1 valid: -0.777868, C2 fake: 0.270474], [G loss: 0.003969, mse: 0.021725]\n",
      "Epoch 96/100001\n",
      "[C1 valid: -0.784867, C2 fake: 0.263514], [G loss: 0.002528, mse: 0.020127]\n",
      "Epoch 97/100001\n",
      "[C1 valid: -0.792526, C2 fake: 0.269430], [G loss: 0.002630, mse: 0.020420]\n",
      "Epoch 98/100001\n",
      "[C1 valid: -0.768445, C2 fake: 0.278992], [G loss: 0.003414, mse: 0.020180]\n",
      "Epoch 99/100001\n",
      "[C1 valid: -0.801089, C2 fake: 0.260167], [G loss: 0.003648, mse: 0.020844]\n",
      "Epoch 100/100001\n",
      "[C1 valid: -0.802954, C2 fake: 0.267708], [G loss: 0.003461, mse: 0.020605]\n",
      "Epoch 101/100001\n",
      "[C1 valid: -0.809349, C2 fake: 0.257314], [G loss: 0.002797, mse: 0.020306]\n",
      "Epoch 102/100001\n",
      "[C1 valid: -0.785166, C2 fake: 0.266783], [G loss: 0.001885, mse: 0.018606]\n",
      "Epoch 103/100001\n",
      "[C1 valid: -0.811008, C2 fake: 0.271284], [G loss: 0.005623, mse: 0.018973]\n",
      "Epoch 104/100001\n",
      "[C1 valid: -0.760994, C2 fake: 0.275398], [G loss: 0.001685, mse: 0.018816]\n",
      "Epoch 105/100001\n",
      "[C1 valid: -0.823889, C2 fake: 0.245651], [G loss: 0.003492, mse: 0.020436]\n",
      "Epoch 106/100001\n",
      "[C1 valid: -0.816980, C2 fake: 0.247918], [G loss: 0.002530, mse: 0.019767]\n",
      "Epoch 107/100001\n",
      "[C1 valid: -0.829042, C2 fake: 0.248301], [G loss: 0.002802, mse: 0.019098]\n",
      "Epoch 108/100001\n",
      "[C1 valid: -0.826781, C2 fake: 0.244940], [G loss: 0.002661, mse: 0.018144]\n",
      "Epoch 109/100001\n",
      "[C1 valid: -0.838004, C2 fake: 0.238630], [G loss: 0.005234, mse: 0.019025]\n",
      "Epoch 110/100001\n",
      "[C1 valid: -0.824460, C2 fake: 0.244959], [G loss: 0.004070, mse: 0.019151]\n",
      "Epoch 111/100001\n",
      "[C1 valid: -0.836730, C2 fake: 0.234071], [G loss: 0.003201, mse: 0.017381]\n",
      "Epoch 112/100001\n",
      "[C1 valid: -0.846864, C2 fake: 0.237436], [G loss: 0.001397, mse: 0.017716]\n",
      "Epoch 113/100001\n",
      "[C1 valid: -0.851521, C2 fake: 0.240384], [G loss: 0.006107, mse: 0.019876]\n",
      "Epoch 114/100001\n",
      "[C1 valid: -0.802508, C2 fake: 0.245687], [G loss: 0.003199, mse: 0.020211]\n",
      "Epoch 115/100001\n",
      "[C1 valid: -0.847031, C2 fake: 0.236101], [G loss: 0.002019, mse: 0.018077]\n",
      "Epoch 116/100001\n",
      "[C1 valid: -0.848892, C2 fake: 0.235745], [G loss: 0.001921, mse: 0.017722]\n",
      "Epoch 117/100001\n",
      "[C1 valid: -0.854720, C2 fake: 0.225245], [G loss: 0.003421, mse: 0.017330]\n",
      "Epoch 118/100001\n",
      "[C1 valid: -0.854917, C2 fake: 0.228541], [G loss: 0.001389, mse: 0.016593]\n",
      "Epoch 119/100001\n",
      "[C1 valid: -0.859690, C2 fake: 0.224723], [G loss: 0.002937, mse: 0.017090]\n",
      "Epoch 120/100001\n",
      "[C1 valid: -0.865052, C2 fake: 0.216414], [G loss: 0.002006, mse: 0.017224]\n",
      "Epoch 121/100001\n",
      "[C1 valid: -0.867389, C2 fake: 0.220380], [G loss: 0.002306, mse: 0.016594]\n",
      "Epoch 122/100001\n",
      "[C1 valid: -0.840316, C2 fake: 0.250716], [G loss: -0.000170, mse: 0.014496]\n",
      "Epoch 123/100001\n",
      "[C1 valid: -0.814825, C2 fake: 0.236711], [G loss: 0.000458, mse: 0.015867]\n",
      "Epoch 124/100001\n",
      "[C1 valid: -0.875316, C2 fake: 0.222786], [G loss: 0.000452, mse: 0.015292]\n",
      "Epoch 125/100001\n",
      "[C1 valid: -0.876652, C2 fake: 0.213725], [G loss: 0.001102, mse: 0.015668]\n",
      "Epoch 126/100001\n",
      "[C1 valid: -0.880150, C2 fake: 0.209919], [G loss: 0.001742, mse: 0.015883]\n",
      "Epoch 127/100001\n",
      "[C1 valid: -0.883351, C2 fake: 0.205569], [G loss: 0.003076, mse: 0.015870]\n",
      "Epoch 128/100001\n",
      "[C1 valid: -0.872555, C2 fake: 0.201506], [G loss: 0.001595, mse: 0.016515]\n",
      "Epoch 129/100001\n",
      "[C1 valid: -0.869380, C2 fake: 0.210051], [G loss: 0.002220, mse: 0.015306]\n",
      "Epoch 130/100001\n",
      "[C1 valid: -0.882515, C2 fake: 0.206002], [G loss: 0.000379, mse: 0.015235]\n",
      "Epoch 131/100001\n",
      "[C1 valid: -0.867255, C2 fake: 0.223482], [G loss: 0.001667, mse: 0.013925]\n",
      "Epoch 132/100001\n",
      "[C1 valid: -0.885788, C2 fake: 0.199022], [G loss: 0.002195, mse: 0.015188]\n",
      "Epoch 133/100001\n",
      "[C1 valid: -0.888872, C2 fake: 0.203151], [G loss: 0.003486, mse: 0.015184]\n",
      "Epoch 134/100001\n",
      "[C1 valid: -0.885308, C2 fake: 0.200782], [G loss: 0.001180, mse: 0.014963]\n",
      "Epoch 135/100001\n",
      "[C1 valid: -0.892259, C2 fake: 0.196476], [G loss: 0.002977, mse: 0.014907]\n",
      "Epoch 136/100001\n",
      "[C1 valid: -0.897439, C2 fake: 0.194685], [G loss: 0.003556, mse: 0.015557]\n",
      "Epoch 137/100001\n",
      "[C1 valid: -0.887774, C2 fake: 0.195552], [G loss: 0.000861, mse: 0.014849]\n",
      "Epoch 138/100001\n",
      "[C1 valid: -0.880127, C2 fake: 0.207757], [G loss: 0.001671, mse: 0.015484]\n",
      "Epoch 139/100001\n",
      "[C1 valid: -0.892792, C2 fake: 0.188235], [G loss: 0.002113, mse: 0.015129]\n",
      "Epoch 140/100001\n",
      "[C1 valid: -0.893147, C2 fake: 0.188710], [G loss: 0.002598, mse: 0.014529]\n",
      "Epoch 141/100001\n",
      "[C1 valid: -0.891617, C2 fake: 0.185132], [G loss: 0.000999, mse: 0.013684]\n",
      "Epoch 142/100001\n",
      "[C1 valid: -0.896291, C2 fake: 0.188385], [G loss: -0.000614, mse: 0.013066]\n",
      "Epoch 143/100001\n",
      "[C1 valid: -0.901816, C2 fake: 0.185549], [G loss: 0.002663, mse: 0.014361]\n",
      "Epoch 144/100001\n",
      "[C1 valid: -0.902724, C2 fake: 0.182507], [G loss: 0.001889, mse: 0.015991]\n",
      "Epoch 145/100001\n",
      "[C1 valid: -0.877304, C2 fake: 0.199655], [G loss: -0.000304, mse: 0.013968]\n",
      "Epoch 146/100001\n",
      "[C1 valid: -0.902309, C2 fake: 0.192564], [G loss: 0.001490, mse: 0.013077]\n",
      "Epoch 147/100001\n",
      "[C1 valid: -0.903444, C2 fake: 0.185739], [G loss: 0.000416, mse: 0.013128]\n",
      "Epoch 148/100001\n",
      "[C1 valid: -0.908034, C2 fake: 0.177243], [G loss: 0.000040, mse: 0.013703]\n",
      "Epoch 149/100001\n",
      "[C1 valid: -0.909272, C2 fake: 0.176706], [G loss: 0.001587, mse: 0.013007]\n",
      "Epoch 150/100001\n",
      "[C1 valid: -0.914489, C2 fake: 0.174459], [G loss: -0.000288, mse: 0.011525]\n",
      "Epoch 151/100001\n",
      "[C1 valid: -0.909220, C2 fake: 0.166837], [G loss: 0.001402, mse: 0.012044]\n",
      "Epoch 152/100001\n",
      "[C1 valid: -0.910809, C2 fake: 0.187879], [G loss: -0.001254, mse: 0.013017]\n",
      "Epoch 153/100001\n",
      "[C1 valid: -0.909410, C2 fake: 0.174396], [G loss: 0.000726, mse: 0.013130]\n",
      "Epoch 154/100001\n",
      "[C1 valid: -0.909463, C2 fake: 0.190972], [G loss: 0.002160, mse: 0.012755]\n",
      "Epoch 155/100001\n",
      "[C1 valid: -0.839350, C2 fake: 0.201316], [G loss: 0.001174, mse: 0.012971]\n",
      "Epoch 156/100001\n",
      "[C1 valid: -0.918444, C2 fake: 0.169716], [G loss: 0.000772, mse: 0.013476]\n",
      "Epoch 157/100001\n",
      "[C1 valid: -0.917135, C2 fake: 0.170254], [G loss: -0.000055, mse: 0.011826]\n",
      "Epoch 158/100001\n",
      "[C1 valid: -0.919745, C2 fake: 0.160519], [G loss: 0.000339, mse: 0.010971]\n",
      "Epoch 159/100001\n",
      "[C1 valid: -0.917095, C2 fake: 0.171274], [G loss: 0.000071, mse: 0.011402]\n",
      "Epoch 160/100001\n",
      "[C1 valid: -0.919598, C2 fake: 0.168549], [G loss: -0.000268, mse: 0.011490]\n",
      "Epoch 161/100001\n",
      "[C1 valid: -0.918160, C2 fake: 0.163621], [G loss: -0.000610, mse: 0.011437]\n",
      "Epoch 162/100001\n",
      "[C1 valid: -0.918249, C2 fake: 0.157436], [G loss: 0.001342, mse: 0.011107]\n",
      "Epoch 163/100001\n",
      "[C1 valid: -0.922214, C2 fake: 0.163534], [G loss: 0.001723, mse: 0.011891]\n",
      "Epoch 164/100001\n",
      "[C1 valid: -0.908080, C2 fake: 0.179817], [G loss: -0.001742, mse: 0.011964]\n",
      "Epoch 165/100001\n",
      "[C1 valid: -0.898800, C2 fake: 0.166179], [G loss: -0.001024, mse: 0.012390]\n",
      "Epoch 166/100001\n",
      "[C1 valid: -0.919754, C2 fake: 0.157441], [G loss: -0.000175, mse: 0.012939]\n",
      "Epoch 167/100001\n",
      "[C1 valid: -0.927277, C2 fake: 0.162142], [G loss: 0.000936, mse: 0.013189]\n",
      "Epoch 168/100001\n",
      "[C1 valid: -0.927586, C2 fake: 0.153927], [G loss: 0.002911, mse: 0.014053]\n",
      "Epoch 169/100001\n",
      "[C1 valid: -0.925026, C2 fake: 0.158848], [G loss: 0.003123, mse: 0.014992]\n",
      "Epoch 170/100001\n",
      "[C1 valid: -0.927466, C2 fake: 0.153636], [G loss: 0.001721, mse: 0.012362]\n",
      "Epoch 171/100001\n",
      "[C1 valid: -0.929837, C2 fake: 0.151532], [G loss: 0.001720, mse: 0.011302]\n",
      "Epoch 172/100001\n",
      "[C1 valid: -0.928981, C2 fake: 0.159033], [G loss: -0.000463, mse: 0.010200]\n",
      "Epoch 173/100001\n",
      "[C1 valid: -0.910924, C2 fake: 0.156991], [G loss: 0.001012, mse: 0.010368]\n",
      "Epoch 174/100001\n",
      "[C1 valid: -0.891792, C2 fake: 0.231861], [G loss: 0.000391, mse: 0.011254]\n",
      "Epoch 175/100001\n",
      "[C1 valid: -0.923978, C2 fake: 0.165976], [G loss: 0.000071, mse: 0.011049]\n",
      "Epoch 176/100001\n",
      "[C1 valid: -0.927468, C2 fake: 0.161345], [G loss: 0.001763, mse: 0.011893]\n",
      "Epoch 177/100001\n",
      "[C1 valid: -0.935162, C2 fake: 0.155468], [G loss: 0.002398, mse: 0.012304]\n",
      "Epoch 178/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.931083, C2 fake: 0.152239], [G loss: 0.001538, mse: 0.012129]\n",
      "Epoch 179/100001\n",
      "[C1 valid: -0.935193, C2 fake: 0.151643], [G loss: 0.002076, mse: 0.012296]\n",
      "Epoch 180/100001\n",
      "[C1 valid: -0.935123, C2 fake: 0.154357], [G loss: 0.001343, mse: 0.010636]\n",
      "Epoch 181/100001\n",
      "[C1 valid: -0.933914, C2 fake: 0.141844], [G loss: 0.001555, mse: 0.010691]\n",
      "Epoch 182/100001\n",
      "[C1 valid: -0.941712, C2 fake: 0.143932], [G loss: 0.001215, mse: 0.010273]\n",
      "Epoch 183/100001\n",
      "[C1 valid: -0.937033, C2 fake: 0.151404], [G loss: 0.001521, mse: 0.010932]\n",
      "Epoch 184/100001\n",
      "[C1 valid: -0.806852, C2 fake: 0.237385], [G loss: 0.001718, mse: 0.010260]\n",
      "Epoch 185/100001\n",
      "[C1 valid: -0.922381, C2 fake: 0.155216], [G loss: 0.000173, mse: 0.009761]\n",
      "Epoch 186/100001\n",
      "[C1 valid: -0.933658, C2 fake: 0.158241], [G loss: 0.000745, mse: 0.009723]\n",
      "Epoch 187/100001\n",
      "[C1 valid: -0.936493, C2 fake: 0.153166], [G loss: 0.000211, mse: 0.009460]\n",
      "Epoch 188/100001\n",
      "[C1 valid: -0.934355, C2 fake: 0.151401], [G loss: 0.000898, mse: 0.009987]\n",
      "Epoch 189/100001\n",
      "[C1 valid: -0.933325, C2 fake: 0.142556], [G loss: 0.001237, mse: 0.009757]\n",
      "Epoch 190/100001\n",
      "[C1 valid: -0.939453, C2 fake: 0.141011], [G loss: 0.000866, mse: 0.009706]\n",
      "Epoch 191/100001\n",
      "[C1 valid: -0.943883, C2 fake: 0.139187], [G loss: 0.000952, mse: 0.009317]\n",
      "Epoch 192/100001\n",
      "[C1 valid: -0.940049, C2 fake: 0.142291], [G loss: -0.002556, mse: 0.008856]\n",
      "Epoch 193/100001\n",
      "[C1 valid: -0.933951, C2 fake: 0.144630], [G loss: -0.000214, mse: 0.009410]\n",
      "Epoch 194/100001\n",
      "[C1 valid: -0.940007, C2 fake: 0.132388], [G loss: 0.000986, mse: 0.010264]\n",
      "Epoch 195/100001\n",
      "[C1 valid: -0.944771, C2 fake: 0.135736], [G loss: 0.000830, mse: 0.009293]\n",
      "Epoch 196/100001\n",
      "[C1 valid: -0.940363, C2 fake: 0.132934], [G loss: 0.000997, mse: 0.009225]\n",
      "Epoch 197/100001\n",
      "[C1 valid: -0.935109, C2 fake: 0.139284], [G loss: 0.000665, mse: 0.009257]\n",
      "Epoch 198/100001\n",
      "[C1 valid: -0.946113, C2 fake: 0.133091], [G loss: 0.000409, mse: 0.009075]\n",
      "Epoch 199/100001\n",
      "[C1 valid: -0.946891, C2 fake: 0.129012], [G loss: 0.000942, mse: 0.009660]\n",
      "Epoch 200/100001\n",
      "[C1 valid: -0.946200, C2 fake: 0.133496], [G loss: 0.001278, mse: 0.010824]\n",
      "Epoch 201/100001\n",
      "[C1 valid: -0.923445, C2 fake: 0.143512], [G loss: 0.001121, mse: 0.010776]\n",
      "Epoch 202/100001\n",
      "[C1 valid: -0.903687, C2 fake: 0.203886], [G loss: 0.001028, mse: 0.011393]\n",
      "Epoch 203/100001\n",
      "[C1 valid: -0.942354, C2 fake: 0.135381], [G loss: 0.001396, mse: 0.010254]\n",
      "Epoch 204/100001\n",
      "[C1 valid: -0.943860, C2 fake: 0.136537], [G loss: 0.000783, mse: 0.009261]\n",
      "Epoch 205/100001\n",
      "[C1 valid: -0.943715, C2 fake: 0.134825], [G loss: 0.000801, mse: 0.008984]\n",
      "Epoch 206/100001\n",
      "[C1 valid: -0.945687, C2 fake: 0.133930], [G loss: 0.000710, mse: 0.008922]\n",
      "Epoch 207/100001\n",
      "[C1 valid: -0.950427, C2 fake: 0.124630], [G loss: 0.000532, mse: 0.008353]\n",
      "Epoch 208/100001\n",
      "[C1 valid: -0.947633, C2 fake: 0.125205], [G loss: 0.000770, mse: 0.008744]\n",
      "Epoch 209/100001\n",
      "[C1 valid: -0.951679, C2 fake: 0.124077], [G loss: 0.000859, mse: 0.008998]\n",
      "Epoch 210/100001\n",
      "[C1 valid: -0.947058, C2 fake: 0.119303], [G loss: 0.000770, mse: 0.008953]\n",
      "Epoch 211/100001\n",
      "[C1 valid: -0.951661, C2 fake: 0.118100], [G loss: 0.000370, mse: 0.008847]\n",
      "Epoch 212/100001\n",
      "[C1 valid: -0.954775, C2 fake: 0.119610], [G loss: 0.000574, mse: 0.009024]\n",
      "Epoch 213/100001\n",
      "[C1 valid: -0.950777, C2 fake: 0.118947], [G loss: 0.000549, mse: 0.008382]\n",
      "Epoch 214/100001\n",
      "[C1 valid: -0.951682, C2 fake: 0.113502], [G loss: 0.000417, mse: 0.008591]\n",
      "Epoch 215/100001\n",
      "[C1 valid: -0.950377, C2 fake: 0.117882], [G loss: 0.000203, mse: 0.008085]\n",
      "Epoch 216/100001\n",
      "[C1 valid: -0.944160, C2 fake: 0.148130], [G loss: -0.002686, mse: 0.008089]\n",
      "Epoch 217/100001\n",
      "[C1 valid: -0.914738, C2 fake: 0.146217], [G loss: -0.002745, mse: 0.007789]\n",
      "Epoch 218/100001\n",
      "[C1 valid: -0.950757, C2 fake: 0.124555], [G loss: -0.001042, mse: 0.007740]\n",
      "Epoch 219/100001\n",
      "[C1 valid: -0.952551, C2 fake: 0.121200], [G loss: -0.000481, mse: 0.007834]\n",
      "Epoch 220/100001\n",
      "[C1 valid: -0.953277, C2 fake: 0.124497], [G loss: -0.000203, mse: 0.007737]\n",
      "Epoch 221/100001\n",
      "[C1 valid: -0.949468, C2 fake: 0.127731], [G loss: -0.001450, mse: 0.007732]\n",
      "Epoch 222/100001\n",
      "[C1 valid: -0.954181, C2 fake: 0.115934], [G loss: 0.000130, mse: 0.008127]\n",
      "Epoch 223/100001\n",
      "[C1 valid: -0.954786, C2 fake: 0.112144], [G loss: 0.000001, mse: 0.008424]\n",
      "Epoch 224/100001\n",
      "[C1 valid: -0.954398, C2 fake: 0.114784], [G loss: -0.000814, mse: 0.008182]\n",
      "Epoch 225/100001\n",
      "[C1 valid: -0.950425, C2 fake: 0.118343], [G loss: 0.000322, mse: 0.008195]\n",
      "Epoch 226/100001\n",
      "[C1 valid: -0.951614, C2 fake: 0.149008], [G loss: 0.000291, mse: 0.008560]\n",
      "Epoch 227/100001\n",
      "[C1 valid: -0.877715, C2 fake: 0.187539], [G loss: -0.000854, mse: 0.008710]\n",
      "Epoch 228/100001\n",
      "[C1 valid: -0.946804, C2 fake: 0.125492], [G loss: -0.000246, mse: 0.008167]\n",
      "Epoch 229/100001\n",
      "[C1 valid: -0.956187, C2 fake: 0.122332], [G loss: 0.000230, mse: 0.007864]\n",
      "Epoch 230/100001\n",
      "[C1 valid: -0.954549, C2 fake: 0.119018], [G loss: -0.000094, mse: 0.007787]\n",
      "Epoch 231/100001\n",
      "[C1 valid: -0.955277, C2 fake: 0.114482], [G loss: 0.000652, mse: 0.008507]\n",
      "Epoch 232/100001\n",
      "[C1 valid: -0.960203, C2 fake: 0.113903], [G loss: 0.000646, mse: 0.008816]\n",
      "Epoch 233/100001\n",
      "[C1 valid: -0.958459, C2 fake: 0.118036], [G loss: 0.000858, mse: 0.009862]\n",
      "Epoch 234/100001\n",
      "[C1 valid: -0.958651, C2 fake: 0.113407], [G loss: 0.000932, mse: 0.009281]\n",
      "Epoch 235/100001\n",
      "[C1 valid: -0.951354, C2 fake: 0.111379], [G loss: 0.000742, mse: 0.009162]\n",
      "Epoch 236/100001\n",
      "[C1 valid: -0.959613, C2 fake: 0.115877], [G loss: -0.000187, mse: 0.008237]\n",
      "Epoch 237/100001\n",
      "[C1 valid: -0.953982, C2 fake: 0.124264], [G loss: 0.000637, mse: 0.008803]\n",
      "Epoch 238/100001\n",
      "[C1 valid: -0.949474, C2 fake: 0.111488], [G loss: -0.000646, mse: 0.008952]\n",
      "Epoch 239/100001\n",
      "[C1 valid: -0.954986, C2 fake: 0.109662], [G loss: 0.000004, mse: 0.007757]\n",
      "Epoch 240/100001\n",
      "[C1 valid: -0.964072, C2 fake: 0.106005], [G loss: -0.000211, mse: 0.007694]\n",
      "Epoch 241/100001\n",
      "[C1 valid: -0.955808, C2 fake: 0.105867], [G loss: -0.000510, mse: 0.007818]\n",
      "Epoch 242/100001\n",
      "[C1 valid: -0.956961, C2 fake: 0.125187], [G loss: -0.000214, mse: 0.007817]\n",
      "Epoch 243/100001\n",
      "[C1 valid: -0.862486, C2 fake: 0.184447], [G loss: -0.000772, mse: 0.007924]\n",
      "Epoch 244/100001\n",
      "[C1 valid: -0.953696, C2 fake: 0.128304], [G loss: -0.001094, mse: 0.007469]\n",
      "Epoch 245/100001\n",
      "[C1 valid: -0.954900, C2 fake: 0.120628], [G loss: -0.000470, mse: 0.007624]\n",
      "Epoch 246/100001\n",
      "[C1 valid: -0.961347, C2 fake: 0.115298], [G loss: -0.000325, mse: 0.007488]\n",
      "Epoch 247/100001\n",
      "[C1 valid: -0.956977, C2 fake: 0.112209], [G loss: -0.000171, mse: 0.007555]\n",
      "Epoch 248/100001\n",
      "[C1 valid: -0.950996, C2 fake: 0.107556], [G loss: -0.000127, mse: 0.007135]\n",
      "Epoch 249/100001\n",
      "[C1 valid: -0.955664, C2 fake: 0.109398], [G loss: -0.001170, mse: 0.007057]\n",
      "Epoch 250/100001\n",
      "[C1 valid: -0.961938, C2 fake: 0.109071], [G loss: -0.001202, mse: 0.007122]\n",
      "Epoch 251/100001\n",
      "[C1 valid: -0.960862, C2 fake: 0.108430], [G loss: -0.000645, mse: 0.006898]\n",
      "Epoch 252/100001\n",
      "[C1 valid: -0.958030, C2 fake: 0.122382], [G loss: -0.001369, mse: 0.007249]\n",
      "Epoch 253/100001\n",
      "[C1 valid: -0.956544, C2 fake: 0.111779], [G loss: 0.000205, mse: 0.007200]\n",
      "Epoch 254/100001\n",
      "[C1 valid: -0.963630, C2 fake: 0.108412], [G loss: -0.000033, mse: 0.007420]\n",
      "Epoch 255/100001\n",
      "[C1 valid: -0.961145, C2 fake: 0.102346], [G loss: 0.000162, mse: 0.007834]\n",
      "Epoch 256/100001\n",
      "[C1 valid: -0.964335, C2 fake: 0.101137], [G loss: 0.000393, mse: 0.008291]\n",
      "Epoch 257/100001\n",
      "[C1 valid: -0.962870, C2 fake: 0.097739], [G loss: 0.000598, mse: 0.008801]\n",
      "Epoch 258/100001\n",
      "[C1 valid: -0.966232, C2 fake: 0.096175], [G loss: 0.000240, mse: 0.008467]\n",
      "Epoch 259/100001\n",
      "[C1 valid: -0.964193, C2 fake: 0.102241], [G loss: -0.000179, mse: 0.007498]\n",
      "Epoch 260/100001\n",
      "[C1 valid: -0.950669, C2 fake: 0.098239], [G loss: -0.000874, mse: 0.006994]\n",
      "Epoch 261/100001\n",
      "[C1 valid: -0.965231, C2 fake: 0.092942], [G loss: -0.000491, mse: 0.006807]\n",
      "Epoch 262/100001\n",
      "[C1 valid: -0.966812, C2 fake: 0.095005], [G loss: -0.000417, mse: 0.006977]\n",
      "Epoch 263/100001\n",
      "[C1 valid: -0.965181, C2 fake: 0.094761], [G loss: -0.000127, mse: 0.007300]\n",
      "Epoch 264/100001\n",
      "[C1 valid: -0.923763, C2 fake: 0.303602], [G loss: -0.000111, mse: 0.007022]\n",
      "Epoch 265/100001\n",
      "[C1 valid: -0.939267, C2 fake: 0.232083], [G loss: -0.000088, mse: 0.007014]\n",
      "Epoch 266/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.949839, C2 fake: 0.198017], [G loss: 0.000098, mse: 0.006904]\n",
      "Epoch 267/100001\n",
      "[C1 valid: -0.948129, C2 fake: 0.173130], [G loss: -0.000217, mse: 0.006287]\n",
      "Epoch 268/100001\n",
      "[C1 valid: -0.959189, C2 fake: 0.144947], [G loss: 0.000203, mse: 0.007250]\n",
      "Epoch 269/100001\n",
      "[C1 valid: -0.959548, C2 fake: 0.131517], [G loss: -0.000474, mse: 0.006396]\n",
      "Epoch 270/100001\n",
      "[C1 valid: -0.957891, C2 fake: 0.125482], [G loss: -0.000404, mse: 0.006484]\n",
      "Epoch 271/100001\n",
      "[C1 valid: -0.963441, C2 fake: 0.117739], [G loss: 0.000009, mse: 0.006774]\n",
      "Epoch 272/100001\n",
      "[C1 valid: -0.954134, C2 fake: 0.115255], [G loss: -0.000151, mse: 0.006524]\n",
      "Epoch 273/100001\n",
      "[C1 valid: -0.963300, C2 fake: 0.113250], [G loss: -0.000283, mse: 0.006681]\n",
      "Epoch 274/100001\n",
      "[C1 valid: -0.960590, C2 fake: 0.108421], [G loss: 0.000033, mse: 0.006923]\n",
      "Epoch 275/100001\n",
      "[C1 valid: -0.959429, C2 fake: 0.109312], [G loss: 0.000286, mse: 0.006797]\n",
      "Epoch 276/100001\n",
      "[C1 valid: -0.964790, C2 fake: 0.106103], [G loss: 0.000174, mse: 0.006652]\n",
      "Epoch 277/100001\n",
      "[C1 valid: -0.954750, C2 fake: 0.104806], [G loss: 0.000124, mse: 0.006358]\n",
      "Epoch 278/100001\n",
      "[C1 valid: -0.960032, C2 fake: 0.104892], [G loss: -0.000070, mse: 0.006730]\n",
      "Epoch 279/100001\n",
      "[C1 valid: -0.963386, C2 fake: 0.103280], [G loss: 0.000022, mse: 0.006760]\n",
      "Epoch 280/100001\n",
      "[C1 valid: -0.962172, C2 fake: 0.110899], [G loss: 0.000425, mse: 0.007546]\n",
      "Epoch 281/100001\n",
      "[C1 valid: -0.961560, C2 fake: 0.108433], [G loss: 0.000444, mse: 0.007034]\n",
      "Epoch 282/100001\n",
      "[C1 valid: -0.959292, C2 fake: 0.096775], [G loss: 0.000643, mse: 0.007712]\n",
      "Epoch 283/100001\n",
      "[C1 valid: -0.959554, C2 fake: 0.096128], [G loss: 0.000417, mse: 0.007291]\n",
      "Epoch 284/100001\n",
      "[C1 valid: -0.960916, C2 fake: 0.087564], [G loss: 0.000348, mse: 0.007468]\n",
      "Epoch 285/100001\n",
      "[C1 valid: -0.961651, C2 fake: 0.092665], [G loss: -0.000116, mse: 0.006884]\n",
      "Epoch 286/100001\n",
      "[C1 valid: -0.964600, C2 fake: 0.090924], [G loss: -0.000810, mse: 0.006568]\n",
      "Epoch 287/100001\n",
      "[C1 valid: -0.951495, C2 fake: 0.140440], [G loss: -0.000351, mse: 0.006522]\n",
      "Epoch 288/100001\n",
      "[C1 valid: -0.946556, C2 fake: 0.119297], [G loss: -0.000058, mse: 0.006135]\n",
      "Epoch 289/100001\n",
      "[C1 valid: -0.958134, C2 fake: 0.098748], [G loss: -0.000097, mse: 0.006224]\n",
      "Epoch 290/100001\n",
      "[C1 valid: -0.957701, C2 fake: 0.097548], [G loss: 0.000080, mse: 0.006663]\n",
      "Epoch 291/100001\n",
      "[C1 valid: -0.960742, C2 fake: 0.091548], [G loss: 0.000028, mse: 0.006422]\n",
      "Epoch 292/100001\n",
      "[C1 valid: -0.964439, C2 fake: 0.095731], [G loss: -0.000145, mse: 0.006501]\n",
      "Epoch 293/100001\n",
      "[C1 valid: -0.959780, C2 fake: 0.090988], [G loss: -0.000209, mse: 0.006485]\n",
      "Epoch 294/100001\n",
      "[C1 valid: -0.966886, C2 fake: 0.086999], [G loss: -0.000316, mse: 0.006306]\n",
      "Epoch 295/100001\n",
      "[C1 valid: -0.966533, C2 fake: 0.088552], [G loss: -0.000274, mse: 0.006453]\n",
      "Epoch 296/100001\n",
      "[C1 valid: -0.966668, C2 fake: 0.108341], [G loss: -0.000350, mse: 0.006055]\n",
      "Epoch 297/100001\n",
      "[C1 valid: -0.950663, C2 fake: 0.123121], [G loss: -0.000264, mse: 0.005809]\n",
      "Epoch 298/100001\n",
      "[C1 valid: -0.952375, C2 fake: 0.099645], [G loss: -0.000402, mse: 0.005661]\n",
      "Epoch 299/100001\n",
      "[C1 valid: -0.961520, C2 fake: 0.097291], [G loss: -0.000457, mse: 0.006203]\n",
      "Epoch 300/100001\n",
      "[C1 valid: -0.965578, C2 fake: 0.095909], [G loss: -0.000558, mse: 0.005945]\n",
      "Epoch 301/100001\n",
      "[C1 valid: -0.966106, C2 fake: 0.089025], [G loss: -0.000260, mse: 0.006219]\n",
      "Epoch 302/100001\n",
      "[C1 valid: -0.964090, C2 fake: 0.090829], [G loss: -0.000680, mse: 0.005865]\n",
      "Epoch 303/100001\n",
      "[C1 valid: -0.969385, C2 fake: 0.084351], [G loss: -0.000732, mse: 0.005840]\n",
      "Epoch 304/100001\n",
      "[C1 valid: -0.968224, C2 fake: 0.084961], [G loss: -0.000696, mse: 0.005775]\n",
      "Epoch 305/100001\n",
      "[C1 valid: -0.917492, C2 fake: 0.205880], [G loss: -0.000218, mse: 0.006139]\n",
      "Epoch 306/100001\n",
      "[C1 valid: -0.942880, C2 fake: 0.137407], [G loss: -0.000463, mse: 0.006090]\n",
      "Epoch 307/100001\n",
      "[C1 valid: -0.965046, C2 fake: 0.107647], [G loss: -0.000158, mse: 0.006451]\n",
      "Epoch 308/100001\n",
      "[C1 valid: -0.966159, C2 fake: 0.100657], [G loss: -0.000062, mse: 0.006415]\n",
      "Epoch 309/100001\n",
      "[C1 valid: -0.968061, C2 fake: 0.094288], [G loss: -0.000060, mse: 0.006456]\n",
      "Epoch 310/100001\n",
      "[C1 valid: -0.965425, C2 fake: 0.090931], [G loss: 0.000165, mse: 0.006807]\n",
      "Epoch 311/100001\n",
      "[C1 valid: -0.963736, C2 fake: 0.089642], [G loss: 0.000842, mse: 0.007895]\n",
      "Epoch 312/100001\n",
      "[C1 valid: -0.967489, C2 fake: 0.087027], [G loss: 0.000501, mse: 0.007319]\n",
      "Epoch 313/100001\n",
      "[C1 valid: -0.962610, C2 fake: 0.078885], [G loss: 0.000435, mse: 0.007507]\n",
      "Epoch 314/100001\n",
      "[C1 valid: -0.971902, C2 fake: 0.079763], [G loss: -0.000047, mse: 0.006493]\n",
      "Epoch 315/100001\n",
      "[C1 valid: -0.969840, C2 fake: 0.077844], [G loss: -0.000037, mse: 0.006544]\n",
      "Epoch 316/100001\n",
      "[C1 valid: -0.963512, C2 fake: 0.083784], [G loss: -0.000093, mse: 0.006223]\n",
      "Epoch 317/100001\n",
      "[C1 valid: -0.969625, C2 fake: 0.078239], [G loss: -0.000343, mse: 0.005690]\n",
      "Epoch 318/100001\n",
      "[C1 valid: -0.969252, C2 fake: 0.074406], [G loss: -0.000219, mse: 0.005738]\n",
      "Epoch 319/100001\n",
      "[C1 valid: -0.971368, C2 fake: 0.074749], [G loss: -0.000419, mse: 0.005666]\n",
      "Epoch 320/100001\n",
      "[C1 valid: -0.970395, C2 fake: 0.077638], [G loss: -0.000329, mse: 0.005914]\n",
      "Epoch 321/100001\n",
      "[C1 valid: -0.969499, C2 fake: 0.075651], [G loss: -0.000123, mse: 0.006250]\n",
      "Epoch 322/100001\n",
      "[C1 valid: -0.968677, C2 fake: 0.080709], [G loss: -0.000045, mse: 0.006572]\n",
      "Epoch 323/100001\n",
      "[C1 valid: -0.967301, C2 fake: 0.073977], [G loss: -0.000053, mse: 0.006546]\n",
      "Epoch 324/100001\n",
      "[C1 valid: -0.963229, C2 fake: 0.077699], [G loss: -0.000446, mse: 0.006264]\n",
      "Epoch 325/100001\n",
      "[C1 valid: -0.966619, C2 fake: 0.078938], [G loss: -0.000155, mse: 0.006465]\n",
      "Epoch 326/100001\n",
      "[C1 valid: -0.968537, C2 fake: 0.077888], [G loss: -0.000856, mse: 0.005887]\n",
      "Epoch 327/100001\n",
      "[C1 valid: -0.917966, C2 fake: 0.208867], [G loss: -0.000334, mse: 0.005966]\n",
      "Epoch 328/100001\n",
      "[C1 valid: -0.964253, C2 fake: 0.121855], [G loss: -0.000119, mse: 0.006224]\n",
      "Epoch 329/100001\n",
      "[C1 valid: -0.969301, C2 fake: 0.107161], [G loss: -0.000307, mse: 0.005871]\n",
      "Epoch 330/100001\n",
      "[C1 valid: -0.968100, C2 fake: 0.105283], [G loss: -0.000575, mse: 0.005531]\n",
      "Epoch 331/100001\n",
      "[C1 valid: -0.968585, C2 fake: 0.101843], [G loss: -0.000343, mse: 0.005840]\n",
      "Epoch 332/100001\n",
      "[C1 valid: -0.968890, C2 fake: 0.095926], [G loss: -0.000480, mse: 0.005529]\n",
      "Epoch 333/100001\n",
      "[C1 valid: -0.967486, C2 fake: 0.096364], [G loss: -0.000418, mse: 0.005514]\n",
      "Epoch 334/100001\n",
      "[C1 valid: -0.971871, C2 fake: 0.093068], [G loss: -0.000380, mse: 0.005559]\n",
      "Epoch 335/100001\n",
      "[C1 valid: -0.965314, C2 fake: 0.089347], [G loss: -0.000381, mse: 0.005831]\n",
      "Epoch 336/100001\n",
      "[C1 valid: -0.967777, C2 fake: 0.083964], [G loss: -0.000437, mse: 0.005761]\n",
      "Epoch 337/100001\n",
      "[C1 valid: -0.972380, C2 fake: 0.073358], [G loss: -0.000428, mse: 0.005674]\n",
      "Epoch 338/100001\n",
      "[C1 valid: -0.969774, C2 fake: 0.077617], [G loss: -0.000652, mse: 0.005254]\n",
      "Epoch 339/100001\n",
      "[C1 valid: -0.971750, C2 fake: 0.074624], [G loss: -0.000456, mse: 0.005673]\n",
      "Epoch 340/100001\n",
      "[C1 valid: -0.969946, C2 fake: 0.084664], [G loss: -0.001078, mse: 0.005661]\n",
      "Epoch 341/100001\n",
      "[C1 valid: -0.958702, C2 fake: 0.088259], [G loss: -0.000036, mse: 0.005942]\n",
      "Epoch 342/100001\n",
      "[C1 valid: -0.964540, C2 fake: 0.079356], [G loss: -0.000138, mse: 0.005595]\n",
      "Epoch 343/100001\n",
      "[C1 valid: -0.971078, C2 fake: 0.073994], [G loss: -0.000415, mse: 0.005629]\n",
      "Epoch 344/100001\n",
      "[C1 valid: -0.971988, C2 fake: 0.073685], [G loss: -0.000433, mse: 0.005633]\n",
      "Epoch 345/100001\n",
      "[C1 valid: -0.971810, C2 fake: 0.071923], [G loss: -0.000775, mse: 0.005242]\n",
      "Epoch 346/100001\n",
      "[C1 valid: -0.973760, C2 fake: 0.075607], [G loss: -0.000978, mse: 0.005284]\n",
      "Epoch 347/100001\n",
      "[C1 valid: -0.968257, C2 fake: 0.071280], [G loss: -0.000613, mse: 0.005740]\n",
      "Epoch 348/100001\n",
      "[C1 valid: -0.973984, C2 fake: 0.066409], [G loss: -0.000904, mse: 0.005476]\n",
      "Epoch 349/100001\n",
      "[C1 valid: -0.971152, C2 fake: 0.066652], [G loss: -0.001127, mse: 0.005678]\n",
      "Epoch 350/100001\n",
      "[C1 valid: -0.973157, C2 fake: 0.072425], [G loss: -0.001266, mse: 0.005051]\n",
      "Epoch 351/100001\n",
      "[C1 valid: -0.975647, C2 fake: 0.073723], [G loss: -0.000953, mse: 0.005258]\n",
      "Epoch 352/100001\n",
      "[C1 valid: -0.969812, C2 fake: 0.071982], [G loss: -0.001140, mse: 0.005344]\n",
      "Epoch 353/100001\n",
      "[C1 valid: -0.972594, C2 fake: 0.066104], [G loss: -0.000905, mse: 0.005409]\n",
      "Epoch 354/100001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C1 valid: -0.970619, C2 fake: 0.070962], [G loss: -0.001047, mse: 0.005421]\n",
      "Epoch 355/100001\n",
      "[C1 valid: -0.972938, C2 fake: 0.072126], [G loss: -0.000501, mse: 0.006082]\n",
      "Epoch 356/100001\n",
      "[C1 valid: -0.975906, C2 fake: 0.074675], [G loss: -0.000476, mse: 0.006713]\n",
      "Epoch 357/100001\n",
      "[C1 valid: -0.958414, C2 fake: 0.077889], [G loss: -0.000522, mse: 0.006703]\n",
      "Epoch 358/100001\n",
      "[C1 valid: -0.972286, C2 fake: 0.074353], [G loss: -0.000973, mse: 0.005585]\n",
      "Epoch 359/100001\n",
      "[C1 valid: -0.975753, C2 fake: 0.071377], [G loss: -0.001433, mse: 0.005559]\n",
      "Epoch 360/100001\n",
      "[C1 valid: -0.972749, C2 fake: 0.071387], [G loss: -0.001553, mse: 0.005328]\n",
      "Epoch 361/100001\n",
      "[C1 valid: -0.977458, C2 fake: 0.065607], [G loss: -0.001059, mse: 0.005658]\n",
      "Epoch 362/100001\n",
      "[C1 valid: -0.968324, C2 fake: 0.069312], [G loss: -0.002795, mse: 0.005893]\n",
      "Epoch 363/100001\n",
      "[C1 valid: -0.960319, C2 fake: 0.113099], [G loss: -0.001783, mse: 0.005309]\n",
      "Epoch 364/100001\n",
      "[C1 valid: -0.950635, C2 fake: 0.160522], [G loss: -0.000904, mse: 0.005513]\n",
      "Epoch 365/100001\n",
      "[C1 valid: -0.968766, C2 fake: 0.084864], [G loss: -0.000573, mse: 0.005695]\n",
      "Epoch 366/100001\n",
      "[C1 valid: -0.973852, C2 fake: 0.087394], [G loss: -0.000753, mse: 0.005251]\n",
      "Epoch 367/100001\n",
      "[C1 valid: -0.968765, C2 fake: 0.083142], [G loss: -0.000587, mse: 0.005779]\n",
      "Epoch 368/100001\n",
      "[C1 valid: -0.971677, C2 fake: 0.076841], [G loss: -0.000759, mse: 0.005696]\n",
      "Epoch 369/100001\n",
      "[C1 valid: -0.966366, C2 fake: 0.069747], [G loss: -0.000743, mse: 0.005368]\n",
      "Epoch 370/100001\n",
      "[C1 valid: -0.973625, C2 fake: 0.068936], [G loss: -0.001124, mse: 0.004942]\n",
      "Epoch 371/100001\n",
      "[C1 valid: -0.970746, C2 fake: 0.066296], [G loss: -0.001010, mse: 0.004970]\n",
      "Epoch 372/100001\n",
      "[C1 valid: -0.974269, C2 fake: 0.066298], [G loss: -0.001100, mse: 0.005037]\n",
      "Epoch 373/100001\n",
      "[C1 valid: -0.974569, C2 fake: 0.062176], [G loss: -0.001716, mse: 0.005008]\n",
      "Epoch 374/100001\n",
      "[C1 valid: -0.971951, C2 fake: 0.084429], [G loss: -0.002080, mse: 0.005570]\n",
      "Epoch 375/100001\n",
      "[C1 valid: -0.970884, C2 fake: 0.086224], [G loss: -0.000702, mse: 0.005041]\n",
      "Epoch 376/100001\n",
      "[C1 valid: -0.969576, C2 fake: 0.073090], [G loss: -0.000939, mse: 0.004987]\n",
      "Epoch 377/100001\n",
      "[C1 valid: -0.971775, C2 fake: 0.066866], [G loss: -0.000735, mse: 0.005351]\n",
      "Epoch 378/100001\n",
      "[C1 valid: -0.975754, C2 fake: 0.064788], [G loss: -0.001064, mse: 0.005061]\n",
      "Epoch 379/100001\n",
      "[C1 valid: -0.974436, C2 fake: 0.068788], [G loss: -0.000759, mse: 0.005223]\n",
      "Epoch 380/100001\n",
      "[C1 valid: -0.976526, C2 fake: 0.065442], [G loss: -0.000980, mse: 0.004843]\n",
      "Epoch 381/100001\n",
      "[C1 valid: -0.975632, C2 fake: 0.062822], [G loss: -0.001110, mse: 0.004871]\n",
      "Epoch 382/100001\n",
      "[C1 valid: -0.973779, C2 fake: 0.061652], [G loss: -0.000951, mse: 0.005026]\n",
      "Epoch 383/100001\n",
      "[C1 valid: -0.975201, C2 fake: 0.061730], [G loss: -0.001054, mse: 0.005093]\n",
      "Epoch 384/100001\n",
      "[C1 valid: -0.978087, C2 fake: 0.064591], [G loss: -0.001378, mse: 0.004800]\n",
      "Epoch 385/100001\n",
      "[C1 valid: -0.975804, C2 fake: 0.062683], [G loss: -0.000934, mse: 0.005065]\n",
      "Epoch 386/100001\n",
      "[C1 valid: -0.975462, C2 fake: 0.087399], [G loss: -0.001293, mse: 0.005218]\n",
      "Epoch 387/100001\n",
      "[C1 valid: -0.953102, C2 fake: 0.132641], [G loss: -0.000687, mse: 0.005004]\n",
      "Epoch 388/100001\n",
      "[C1 valid: -0.973184, C2 fake: 0.087002], [G loss: -0.000657, mse: 0.005023]\n",
      "Epoch 389/100001\n",
      "[C1 valid: -0.969925, C2 fake: 0.088473], [G loss: -0.000846, mse: 0.004743]\n",
      "Epoch 390/100001\n",
      "[C1 valid: -0.972884, C2 fake: 0.082546], [G loss: -0.000826, mse: 0.004752]\n",
      "Epoch 391/100001\n",
      "[C1 valid: -0.972853, C2 fake: 0.081086], [G loss: -0.000714, mse: 0.004991]\n",
      "Epoch 392/100001\n",
      "[C1 valid: -0.976376, C2 fake: 0.078865], [G loss: -0.000744, mse: 0.004900]\n",
      "Epoch 393/100001\n",
      "[C1 valid: -0.974179, C2 fake: 0.070862], [G loss: -0.000759, mse: 0.004882]\n",
      "Epoch 394/100001\n",
      "[C1 valid: -0.976170, C2 fake: 0.068631], [G loss: -0.000888, mse: 0.004903]\n",
      "Epoch 395/100001\n",
      "[C1 valid: -0.973690, C2 fake: 0.065438], [G loss: -0.000944, mse: 0.004685]\n",
      "Epoch 396/100001\n",
      "[C1 valid: -0.976342, C2 fake: 0.063166], [G loss: -0.000814, mse: 0.005038]\n",
      "Epoch 397/100001\n",
      "[C1 valid: -0.969985, C2 fake: 0.066659], [G loss: -0.000773, mse: 0.004839]\n",
      "Epoch 398/100001\n",
      "[C1 valid: -0.974740, C2 fake: 0.069888], [G loss: -0.000624, mse: 0.004991]\n",
      "Epoch 399/100001\n",
      "[C1 valid: -0.974092, C2 fake: 0.060340], [G loss: -0.000277, mse: 0.005312]\n",
      "Epoch 400/100001\n",
      "[C1 valid: -0.972229, C2 fake: 0.060957], [G loss: -0.000345, mse: 0.005386]\n",
      "Epoch 401/100001\n",
      "[C1 valid: -0.977200, C2 fake: 0.063403], [G loss: -0.000466, mse: 0.005270]\n",
      "Epoch 402/100001\n",
      "[C1 valid: -0.975163, C2 fake: 0.058303], [G loss: -0.000691, mse: 0.004787]\n",
      "Epoch 403/100001\n",
      "[C1 valid: -0.977847, C2 fake: 0.059709], [G loss: -0.000946, mse: 0.004747]\n",
      "Epoch 404/100001\n",
      "[C1 valid: -0.978562, C2 fake: 0.055139], [G loss: -0.000879, mse: 0.004689]\n",
      "Epoch 405/100001\n",
      "[C1 valid: -0.977226, C2 fake: 0.062422], [G loss: -0.000516, mse: 0.005134]\n",
      "Epoch 406/100001\n",
      "[C1 valid: -0.977716, C2 fake: 0.058195], [G loss: -0.000680, mse: 0.004806]\n",
      "Epoch 407/100001\n",
      "[C1 valid: -0.976430, C2 fake: 0.069300], [G loss: -0.000689, mse: 0.004986]\n",
      "Epoch 408/100001\n",
      "[C1 valid: -0.974789, C2 fake: 0.056345], [G loss: -0.000746, mse: 0.004769]\n",
      "Epoch 409/100001\n",
      "[C1 valid: -0.977632, C2 fake: 0.060448], [G loss: -0.000899, mse: 0.004354]\n",
      "Epoch 410/100001\n",
      "[C1 valid: -0.976372, C2 fake: 0.055783], [G loss: -0.001223, mse: 0.004693]\n",
      "Epoch 411/100001\n",
      "[C1 valid: -0.977180, C2 fake: 0.062813], [G loss: -0.001112, mse: 0.004815]\n",
      "Epoch 412/100001\n",
      "[C1 valid: -0.955385, C2 fake: 0.082896], [G loss: -0.002861, mse: 0.005112]\n",
      "Epoch 413/100001\n",
      "[C1 valid: -0.968935, C2 fake: 0.062292], [G loss: -0.002755, mse: 0.004985]\n",
      "Epoch 414/100001\n",
      "[C1 valid: -0.972900, C2 fake: 0.065081], [G loss: -0.002724, mse: 0.004941]\n",
      "Epoch 415/100001\n",
      "[C1 valid: -0.975728, C2 fake: 0.067988], [G loss: -0.003286, mse: 0.004615]\n",
      "Epoch 416/100001\n",
      "[C1 valid: -0.972319, C2 fake: 0.059981], [G loss: -0.003038, mse: 0.004422]\n",
      "Epoch 417/100001\n",
      "[C1 valid: -0.975243, C2 fake: 0.056046], [G loss: -0.001965, mse: 0.004773]\n",
      "Epoch 418/100001\n",
      "[C1 valid: -0.973555, C2 fake: 0.055808], [G loss: -0.002245, mse: 0.004593]\n",
      "Epoch 419/100001\n",
      "[C1 valid: -0.975330, C2 fake: 0.056286], [G loss: -0.002493, mse: 0.004652]\n",
      "Epoch 420/100001\n",
      "[C1 valid: -0.977428, C2 fake: 0.056345], [G loss: -0.002306, mse: 0.004514]\n",
      "Epoch 421/100001\n"
     ]
    }
   ],
   "source": [
    "hist = aae.train(Z,BATCH_SIZE,train_dataset, epochs, scaler, scaled,X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict from the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the data values on the basis of the trained model.\n",
    "#sampling from the latent space without prediction\n",
    "\n",
    "latent_values = np.random.normal(loc=0, scale=1, size=([1000, Z]))\n",
    "predicted_values = aae.decoder.predict(latent_values)\n",
    "\n",
    "predicted_values2 = aae.decoder.predict(aae.encoder(X_train_scaled))\n",
    "\n",
    "\n",
    "if scaled == '-1-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "elif scaled =='0-1':\n",
    "    predicted_values = scaler.inverse_transform(predicted_values)\n",
    "    predicted_values2 = scaler.inverse_transform(predicted_values2)\n",
    "    \n",
    "\n",
    "if n_features==3:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"latent_space:\",Z)\n",
    "    print(\"BATCH_SIZE:\",BATCH_SIZE)\n",
    "    print(\"epochs:\",epochs)\n",
    "    \n",
    "\n",
    "    ab = plt.subplot(projection='3d')\n",
    "    ab.scatter(predicted_values[:,0],predicted_values[:,1],predicted_values[:,2])\n",
    "    ab.set_ylabel('Y')\n",
    "    ab.set_zlabel('Z')\n",
    "    ab.set_xlabel('X')\n",
    "    \n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(predicted_values[:,1],predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(predicted_values[:,0],predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,0]>=-0.8-0.05,predicted_values[:,0]<=-0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,0]>=0.0-0.05,predicted_values[:,0]<=0.0+0.05),predicted_values[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,0]>=0.8-0.05,predicted_values[:,0]<=0.8+0.05),predicted_values[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(predicted_values[:,1]>=0.2-0.05,predicted_values[:,1]<=0.2+0.05),predicted_values[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,predicted_values[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(predicted_values[:,1]>=0.5-0.05,predicted_values[:,1]<=0.5+0.05),predicted_values[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,predicted_values[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(predicted_values[:,1]>=0.8-0.05,predicted_values[:,1]<=0.8+0.05),predicted_values[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,predicted_values[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Predicted Values:\",predicted_values.shape)\n",
    "    print(\"Predicted Values:\",predicted_values2.shape)\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(predicted_values[:,0],predicted_values[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define these for desired prediction\n",
    "x_input = [-3,-1,1,3]\n",
    "n_points = 400\n",
    "y_min = -1\n",
    "y_max = 1\n",
    "\n",
    "\n",
    "# produces an input of fixed x coordinates with random y values\n",
    "predict1 = np.full((n_points//4, n_features), x_input[0])\n",
    "predict2 = np.full((n_points//4, n_features), x_input[1])\n",
    "predict3 = np.full((n_points//4, n_features), x_input[2])\n",
    "predict4 = np.full((n_points//4, n_features), x_input[3])\n",
    "predictthis = np.concatenate((predict1, predict2, predict3, predict4))\n",
    "predictthis = scaler.fit_transform(predictthis)\n",
    "input_test = predictthis.reshape(n_points, n_features).astype('float32')\n",
    "\n",
    "\n",
    "print(\"input_test :\",input_test.shape)\n",
    "plt.scatter(input_test[:,0],input_test[:,1] ,c='grey')\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_generated = aae.generator.predict(input_test)\n",
    "X_generated = aae.decoder.predict(aae.encoder(input_test))\n",
    "X_generated = scaler.inverse_transform(X_generated)\n",
    "print(\"X_generated :\",X_generated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scenario in (\"3d\", \"helix\"):\n",
    "    print(\"latent_space=\",latent_space)\n",
    "    print(\"Epochs=\",epochs)\n",
    "    print(\"BATCH_SIZE=\",BATCH_SIZE)\n",
    "    print(\"use_bias=\",use_bias)\n",
    "    \n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter(X_generated[:,0], X_generated[:,1], X_generated[:,2], label='Generated Data')\n",
    "\n",
    "\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_xlabel('X')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"X-Y 2D slices:\")\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-1.5,1.5)\n",
    "    axes[0].scatter(X_train[:,0],X_train[:,1])\n",
    "    axes[0].scatter(X_generated[:,0],X_generated[:,1])\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    plt.ylim(-1.5,1.5)\n",
    "    plt.xlim(-2,22)\n",
    "    axes[1].scatter(X_train[:,1],y_train)\n",
    "    axes[1].scatter(X_generated[:,1],X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.xlim(-1.5,1.5)\n",
    "    plt.ylim(-2,22)\n",
    "    axes[2].scatter(X_train[:,0],y_train)\n",
    "    axes[2].scatter(X_generated[:,0],X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    \n",
    "    ac=np.where(np.logical_and(X_train[:,0]>=-0.8-0.05,X_train[:,0]<=-0.8+0.05),X_train[:,1],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,0]>=-0.8-0.05,X_generated[:,0]<=-0.8+0.05),X_generated[:,1],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"Y(X=-0.8)\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,0]>=0.0-0.05,X_train[:,0]<=0.0+0.05),X_train[:,1],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,0]>=0.0-0.05,X_generated[:,0]<=0.0+0.05),X_generated[:,1],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"Y(X=0.0)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,0]>=0.8-0.05,X_train[:,0]<=0.8+0.05),X_train[:,1],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,0]>=0.8-0.05,X_generated[:,0]<=0.8+0.05),X_generated[:,1],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"Y(X=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5), sharey=False, sharex=False)\n",
    "    ac=np.where(np.logical_and(X_train[:,1]>=0.2-0.05,X_train[:,1]<=0.2+0.05),X_train[:,0],None)\n",
    "    ad=np.where(np.logical_and(X_generated[:,1]>=0.2-0.05,X_generated[:,1]<=0.2+0.05),X_generated[:,0],None)\n",
    "    axes[0].scatter(ac,y_train)\n",
    "    axes[0].scatter(ad,X_generated[:,2])\n",
    "    axes[0].set_xlabel(\"X(Y=0.2)\")\n",
    "    axes[0].set_ylabel(\"Z\")\n",
    "    \n",
    "    ae=np.where(np.logical_and(X_train[:,1]>=0.5-0.05,X_train[:,1]<=0.5+0.05),X_train[:,0],None)\n",
    "    af=np.where(np.logical_and(X_generated[:,1]>=0.5-0.05,X_generated[:,1]<=0.5+0.05),X_generated[:,0],None)\n",
    "    axes[1].scatter(ae,y_train)\n",
    "    axes[1].scatter(af,X_generated[:,2])\n",
    "    axes[1].set_xlabel(\"X(Y=0.5)\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "    \n",
    "    ag=np.where(np.logical_and(X_train[:,1]>=0.8-0.05,X_train[:,1]<=0.8+0.05),X_train[:,0],None)\n",
    "    ah=np.where(np.logical_and(X_generated[:,1]>=0.8-0.05,X_generated[:,1]<=0.8+0.05),X_generated[:,0],None)\n",
    "    axes[2].scatter(ag,y_train)\n",
    "    axes[2].scatter(ah,X_generated[:,2])\n",
    "    axes[2].set_xlabel(\"X(Y=0.8)\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Generated Data:\",X_generated.shape)\n",
    "    plt.scatter(X_train, y_train,label=\"Sample Data\")\n",
    "    plt.scatter(X_generated[:,0],X_generated[:,1])\n",
    "    #plt.scatter(predicted_values2[:,0],predicted_values2[:,1])\n",
    "    plt.ylabel('Y')\n",
    "    plt.xlabel('X')\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
